{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a pet names neural network with Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for the 2021 Dask Distributed Summit talk: [Using Dask and many GPUs to train a neural network with PyTorch](https://saturncloud.io/blog/dask-with-gpus/). The notebook runs experiments to test the value in using 1, 4, and 16 workers in a Dask cluster with PyTorch DDP to train a neural network that generates pet names. \n",
    "\n",
    "**If you are using the Free tier of Saturn Cloud Hosted you won't be able to use the full 16 workers.** Instead, you can test with a smaller number of workers (up to 3).\n",
    "\n",
    "PyTorch can potentially be sped up dramatically by having the training computations done on multiple GPUs across multiple workers. This relies on PyTorches [DistributedDataParallel (DDP)](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) module to take computing the values for each batch and spread them across multiple machines/processors. So each worker computes a part of the batch, and then they are all combined to determine the loss then optimize the nodes. If you kept a network training setup the exact same except tripled the number of GPUs with DDP, you would in practice be using a batch size that is 3x bigger than our original one. Be aware, not all networks benefit from having larger batch sizes, and using PyTorch across multiple workers adds the time it takes to pass the new values between each worker.\n",
    "\n",
    "This example generates new pet names by training an LSTM neural network on pet names from Seattle pet license data. The model takes a partially complete name and determines the probability of each possible next character in the name. Then to generate names a character is randomly sampled from that distribution and it's added to the name, then the process is repeated until a stop character is generated. Weights & Biases is used to track experimental results\n",
    "\n",
    "## Setting up model training\n",
    "\n",
    "Nothing in this section has anything to do with DDP, Dask, or Saturn. This merely downloads the already cleaned pet names data, creates functions to process it into a format to feed into an LSTM, and defines the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import pandas as pd  # noqa\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To log into Weights & Biases you'll need an [API key](https://wandb.ai/authorize). If you don't have a Weights & Biases account yet don't worry, they're free and you can store lots of experiments with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chunk:\n",
    "\n",
    "* downloads the pet names data,\n",
    "* creates a formatting function to turn the data into the proper format,\n",
    "* defines a data class for the data\n",
    "* defines a model class for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(\n",
    "    \"https://saturn-public-data.s3.us-east-2.amazonaws.com/examples/pytorch/seattle_pet_licenses_cleaned.json\"\n",
    ") as f:\n",
    "    pet_names = json.loads(f.read().decode(\"utf-8\"))\n",
    "\n",
    "# Our list of characters, where * represents blank and + represents stop\n",
    "characters = list(\"*+abcdefghijklmnopqrstuvwxyz-. \")\n",
    "str_len = 8\n",
    "\n",
    "\n",
    "def format_training_data(pet_names, device=None):\n",
    "    def get_substrings(in_str):\n",
    "        # add the stop character to the end of the name, then generate all the partial names\n",
    "        in_str = in_str + \"+\"\n",
    "        res = [in_str[0:j] for j in range(1, len(in_str) + 1)]\n",
    "        return res\n",
    "\n",
    "    pet_names_expanded = [get_substrings(name) for name in pet_names]\n",
    "    pet_names_expanded = [item for sublist in pet_names_expanded for item in sublist]\n",
    "    pet_names_characters = [list(name) for name in pet_names_expanded]\n",
    "    pet_names_padded = [name[-(str_len + 1) :] for name in pet_names_characters]\n",
    "    pet_names_padded = [\n",
    "        list((str_len + 1 - len(characters)) * \"*\") + characters for characters in pet_names_padded\n",
    "    ]\n",
    "    pet_names_numeric = [[characters.index(char) for char in name] for name in pet_names_padded]\n",
    "\n",
    "    # the final x and y data to use for training the model. Note that the x data needs to be one-hot encoded\n",
    "    if device is None:\n",
    "        y = torch.tensor([name[1:] for name in pet_names_numeric])\n",
    "        x = torch.tensor([name[:-1] for name in pet_names_numeric])\n",
    "    else:\n",
    "        y = torch.tensor([name[1:] for name in pet_names_numeric], device=device)\n",
    "        x = torch.tensor([name[:-1] for name in pet_names_numeric], device=device)\n",
    "    x = torch.nn.functional.one_hot(x, num_classes=len(characters)).float()\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, pet_names, device=None):\n",
    "        self.x, self.y = format_training_data(pet_names, device)\n",
    "        self.permute()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.permutation[idx]\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def permute(self):\n",
    "        self.permutation = torch.randperm(len(self.x))\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=len(characters),\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=4,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, len(characters))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, state = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with Dask and Saturn\n",
    "\n",
    "Next we train the model in parallel over multiple workers using Dask and Saturn. Before running the code, check that you've [started the Dask cluster](https://www.saturncloud.io/docs/getting-started/create_cluster_ui/) in your Saturn Cloud Project.\n",
    "\n",
    "First, we need to import several modules for Dask and Saturn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from dask_pytorch_ddp import dispatch, results\n",
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "from distributed.worker import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the `train()` function that will be run on each of the workers. This has much of the same training code you would see in any PyTorch training loop, with a few key differences. The data is distributed with the DistributedSampler--now each worker will only have a fraction of the data so that together all of the workers combined see each data point exactly once in an epoch. The model is also wrapped in a `DDP()` function call so that they can communicate with each other. The `logger` is used to show intermediate results in the Dask logs for each worker, and the results handler `rh` is used to write intermediate values back to the machine running Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(experiment):\n",
    "    num_epochs = experiment['num_epochs']\n",
    "    n_workers = experiment['n_workers']\n",
    "    base_lr = experiment['base_lr']\n",
    "    batch_size = experiment['batch_size']\n",
    "\n",
    "    worker_rank = int(dist.get_rank())\n",
    "    device = torch.device(0)\n",
    "\n",
    "    logger.info(f\"Worker {worker_rank} - beginning\")\n",
    "\n",
    "    dataset = OurDataset(pet_names, device=device)\n",
    "    # the distributed sampler makes it so the samples are distributed across the different workers\n",
    "    sampler = DistributedSampler(dataset)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    worker_rank = int(dist.get_rank())\n",
    "\n",
    "    # the model has to both be passed to the GPU device, then has to be wrapped in DDP so it can communicate with the other workers\n",
    "    model = Model()\n",
    "    model = model.to(device)\n",
    "    model = DDP(model)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=base_lr)\n",
    "\n",
    "    wandb_config = {\n",
    "        'num_epochs': num_epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'base_lr': base_lr,\n",
    "        'num_workers': n_workers\n",
    "    }\n",
    "\n",
    "    if worker_rank == 0:\n",
    "        run = wandb.init(config=wandb_config, project='pytorch-petnames', reinit=True)\n",
    "        run.watch(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # the logger here logs to the dask log of each work, for easy debugging\n",
    "        logger.info(\n",
    "            f\"Worker {worker_rank} - {datetime.datetime.now().isoformat()} - Beginning epoch {epoch}\"\n",
    "        )\n",
    "\n",
    "        # this ensures the data is reshuffled each epoch\n",
    "        sampler.set_epoch(epoch)\n",
    "        dataset.permute()\n",
    "\n",
    "        # nothing in the code for each batch is now any different than base PyTorch\n",
    "        for i, (batch_x, batch_y) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_y_pred = model(batch_x)\n",
    "\n",
    "            loss = criterion(batch_y_pred.transpose(1, 2), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            logger.info(\n",
    "                f\"Worker {worker_rank} - {datetime.datetime.now().isoformat()} - epoch {epoch} - batch {i} - batch complete - loss {loss.item()}\"\n",
    "            )\n",
    "\n",
    "        # the first rh call saves a json file with the loss from the worker at the end of the epoch\n",
    "        rh.submit_result(\n",
    "            f\"logs/data_{worker_rank}_{epoch}.json\",\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"time\": datetime.datetime.now().isoformat(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"worker\": worker_rank,\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if worker_rank == 0:\n",
    "            run.log({\n",
    "                    'loss': loss.item(),\n",
    "                    'epoch': epoch\n",
    "                    })\n",
    "\n",
    "        # this saves the model. We only need to do it for one worker (so we picked worker 0)\n",
    "        if worker_rank == 0:\n",
    "            rh.submit_result(\"model.pkl\", pickle.dumps(model.state_dict()))\n",
    "    if worker_rank == 0:\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the experiment we set up lists of each of the different parameters, then combine them into one larger list of experiments that contains the cross products of all of them. **If you are on the Saturn Cloud free tier you'll need to change this from testing a Dask cluster with up to 16 machines to up to only 3 machines.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = [1, 4, 16]  # If you are using the free tier this should be changed to [1, 2, 3]\n",
    "base_lr = [0.001, 0.004, 0.016]\n",
    "batch_size = [16384, 4096, 1024]\n",
    "num_epochs = [15]\n",
    "\n",
    "experiments = [{'n_workers': nw, 'base_lr': blr, 'batch_size': bs, 'num_epochs': ne}\n",
    "               for nw in n_workers for blr in base_lr for bs in batch_size for ne in num_epochs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chunk spins up a Dask cluster and results handler object. If this code has trouble running you may need to [restart the Dask cluster](https://www.saturncloud.io/docs/getting-started/create_cluster_ui/) from the Saturn GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "key = uuid.uuid4().hex\n",
    "rh = results.DaskResultsHandler(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code for each experiment scales the Dask cluster to the right number of workers, starts the training job on all the workers, then uses the results handler to listen for results. The `process_results` function will hold the Jupyter notebook until the training job is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    client.restart()\n",
    "    cluster.scale(experiment['n_workers'])\n",
    "    client.wait_for_workers(experiment['n_workers'])\n",
    "    futures = dispatch.run(client, train, experiment=experiment)\n",
    "    rh.process_results(\"/home/jovyan/project/training/\", futures, raise_errors=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we close the dask workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Names\n",
    "\n",
    "To generate names, we have a function that takes the model and runs it over an over on a string generating each new character until a stop character is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(model, characters, str_len):\n",
    "    in_progress_name = []\n",
    "    next_letter = \"\"\n",
    "    while not next_letter == \"+\" and len(in_progress_name) < 30:\n",
    "        # prep the data to run in the model again\n",
    "        in_progress_name_padded = in_progress_name[-str_len:]\n",
    "        in_progress_name_padded = (\n",
    "            list((str_len - len(in_progress_name_padded)) * \"*\") + in_progress_name_padded\n",
    "        )\n",
    "        in_progress_name_numeric = [characters.index(char) for char in in_progress_name_padded]\n",
    "        in_progress_name_tensor = torch.tensor(in_progress_name_numeric)\n",
    "        in_progress_name_tensor = torch.nn.functional.one_hot(\n",
    "            in_progress_name_tensor, num_classes=len(characters)\n",
    "        ).float()\n",
    "        in_progress_name_tensor = torch.unsqueeze(in_progress_name_tensor, 0)\n",
    "\n",
    "        # get the probabilities of each possible next character by running the model\n",
    "        with torch.no_grad():\n",
    "            next_letter_probabilities = model(in_progress_name_tensor)\n",
    "\n",
    "        next_letter_probabilities = next_letter_probabilities[0, -1, :]\n",
    "        next_letter_probabilities = (\n",
    "            torch.nn.functional.softmax(next_letter_probabilities, dim=0).detach().cpu().numpy()\n",
    "        )\n",
    "        next_letter_probabilities = next_letter_probabilities[1:]\n",
    "        next_letter_probabilities = [\n",
    "            p / sum(next_letter_probabilities) for p in next_letter_probabilities\n",
    "        ]\n",
    "\n",
    "        # determine what the actual letter is\n",
    "        next_letter = characters[\n",
    "            np.random.choice(len(characters) - 1, p=next_letter_probabilities) + 1\n",
    "        ]\n",
    "        if next_letter != \"+\":\n",
    "            # if the next character isn't stop add the latest generated character to the name and continue\n",
    "            in_progress_name.append(next_letter)\n",
    "    # turn the list of characters into a single string\n",
    "    pet_name = \"\".join(in_progress_name).title()\n",
    "    return pet_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the function we first need to load the model data from the training folder. That saved model state will be inserted into a parallel cuda model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and the trained parameters\n",
    "model_state = pickle.load(open(\"/home/jovyan/project/training/model.pkl\", \"rb\"))\n",
    "model = torch.nn.DataParallel(Model()).cuda()\n",
    "model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets generate 50 names! Also let's remove any names that would have shown up in the training data since those are less fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 50 names then filter out existing ones\n",
    "generated_names = [generate_name(model, characters, str_len) for i in range(0, 50)]\n",
    "generated_names = [name for name in generated_names if name not in pet_names]\n",
    "print(generated_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the code above you should see a list of names like:\n",
    "\n",
    "```python\n",
    "['Moicu', 'Caspa', 'Penke', 'Lare', 'Otlnys', 'Zexto', 'Toba', 'Siralto',\n",
    "'Luny', 'Lit', 'Bonhe', 'Mashs', 'Riys Wargen', 'Roli', 'Sape', 'Anhyyhe',\n",
    "'Lorla', 'Boupir', 'Zicka', 'Muktse', 'Musko', 'Mosdin', 'Yapfe', 'Snevi',\n",
    "'Zedy', 'Cedi', 'Wivagok Rayten', 'Luzia', 'Teclyn', 'Pibty', 'Cheynet', \n",
    "'Lazyh', 'Ragopes', 'Bitt', 'Bemmen', 'Duuxy', 'Graggie', 'Rari', 'Kisi',\n",
    "'Lvanxoeber', 'Bonu','Masnen', 'Isphofke', 'Myai', 'Shur', 'Lani', 'Ructli',\n",
    "'Folsy', 'Icthobewlels', 'Kuet Roter']\n",
    "```\n",
    "\n",
    "We've now successfully trained a PyTorch neural network on a distributed set of computers with Dask, and then used it to do NLP inference! Note that depending on the size of your data, your network architecture, and other parameters particular to your situation, training over a distributed set of computers may or may not be faster than training on a single GPU. You can also go to the [Weights & Biases Projects Page](https://wandb.ai/home) to see detailed results of the experiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
