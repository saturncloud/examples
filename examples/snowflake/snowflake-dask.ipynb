{
 "cells": [
  {
   "source": [
    "# Connecting Saturn Cloud to Snowflake\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to connect to a Snowflake database and do large data manipulations that would require distributed computing using dask. We will use the NYC Taxi dataset hosted in a Saturn Cloud Snowflake database.\n",
    "\n",
    "First, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we connect to the Snowflake database using the Snowflake connector module. Here we are loading the credentials as environmental variables using the Saturn Cloud credential manager, however you could load them in other ways too. Be sure not to save them as plaintext in unsafe places!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_info = {\n",
    "    \"account\": os.environ[\"EXAMPLE_SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"EXAMPLE_SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"EXAMPLE_SNOWFLAKE_PASSWORD\"],\n",
    "    \"database\": os.environ[\"TAXI_DATABASE\"],\n",
    "}\n",
    "\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run queries directly on a Snowflake database and load them into a Pandas DataFrame. In the following chunk we get the data on which days had any taxi rides in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_query = \"\"\"\n",
    "SELECT\n",
    "    DISTINCT(DATE(pickup_datetime)) as date\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    pickup_datetime BETWEEN '2019-01-01' and '2019-01-31'\n",
    "\"\"\"\n",
    "dates = pd.read_sql(dates_query, conn)[\"DATE\"].tolist()\n",
    "\n",
    "print(dates[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Querying Snowflake and running complex computations with Dask\n",
    "\n",
    "Sometimes Snowflake queries return so much data that the result wouldn't fit on a single machine. In these cases Dask can be used to store and manipulate data in a distributed way.\n",
    "\n",
    "To use Dask with snowflake, first we import the required modules and connect to the Dask cluster on Saturn Cloud. _For this code to run you need to start the Dask cluster from the project page of Saturn Cloud._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-02-25 21:45:46] INFO - dask-saturn | Cluster is ready\n",
      "[2021-02-25 21:45:46] INFO - dask-saturn | Registering default plugins\n",
      "[2021-02-25 21:45:46] INFO - dask-saturn | {'tcp://10.0.13.86:36207': {'status': 'repeat'}, 'tcp://10.0.5.136:45745': {'status': 'repeat'}, 'tcp://10.0.6.177:39553': {'status': 'repeat'}}\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that will query a small part of the data. Here we query information about a single day of taxi rides. We will run this query for each day separately and using all the Dask workers to run the queries and store the results. The `@dask.delayed` indicates that this function will be run over the Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_from_snowflake(day):\n",
    "    with snowflake.connector.connect(**conn_info) as conn:\n",
    "        query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM taxi_yellow\n",
    "        WHERE\n",
    "            date(pickup_datetime) = '{day}'\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query.format(day=day), conn)\n",
    "        # some days have no values for congestion_surcharge, this line ensures\n",
    "        # that the missing data is properly filled\n",
    "        df.CONGESTION_SURCHARGE = df.CONGESTION_SURCHARGE.astype(\"float64\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of all the results from running this query in a distributed way over all the days. As you can see from the `delayed_obs[:5]` call, these aren't Pandas dataframes that are returned, they are Dask objects. The queries haven't actually be run yet, since Dask is lazy they won't be run until they are needed. The list of delayed observations can be turned into a single Dask Dataframe using `.from_delayed()`. A Dask DataFrame performs just liked a Pandas DataFrame, they can use similar function calls and share a similar syntax, however the Dask DataFrame is actually a collection of Pandas dataframes all distributed across a Dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_snowflake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-5786b497d586>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdelayed_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_from_snowflake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mday\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdelayed_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-5786b497d586>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdelayed_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_from_snowflake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mday\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdelayed_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_snowflake' is not defined"
     ]
    }
   ],
   "source": [
    "delayed_obs = [load_from_snowflake(day) for day in dates]\n",
    "delayed_obs[:5]\n",
    "\n",
    "dask_data = dd.from_delayed(delayed_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the contents of the DataFrame by using the same `.head()` call on the Dask DataFrame as we would on the Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CSV_FILENAME</th>\n",
       "      <th>VENDORID</th>\n",
       "      <th>PICKUP_DATETIME</th>\n",
       "      <th>DROPOFF_DATETIME</th>\n",
       "      <th>PASSENGER_COUNT</th>\n",
       "      <th>PICKUP_LONGITUDE</th>\n",
       "      <th>PICKUP_LATITUDE</th>\n",
       "      <th>TRIP_DISTANCE</th>\n",
       "      <th>RATECODEID</th>\n",
       "      <th>STORE_AND_FWD_FLAG</th>\n",
       "      <th>...</th>\n",
       "      <th>DROPOFF_TAXIZONE_ID</th>\n",
       "      <th>PAYMENT_TYPE</th>\n",
       "      <th>FARE_AMOUNT</th>\n",
       "      <th>EXTRA</th>\n",
       "      <th>MTA_TAX</th>\n",
       "      <th>TIP_AMOUNT</th>\n",
       "      <th>TOLLS_AMOUNT</th>\n",
       "      <th>IMPROVEMENT_SURCHARGE</th>\n",
       "      <th>TOTAL_AMOUNT</th>\n",
       "      <th>CONGESTION_SURCHARGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trip data/yellow_tripdata_2019-01.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-21 00:01:16</td>\n",
       "      <td>2019-01-21 00:05:15</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>229</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trip data/yellow_tripdata_2019-01.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-21 00:03:49</td>\n",
       "      <td>2019-01-21 00:05:07</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trip data/yellow_tripdata_2019-01.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-21 00:00:30</td>\n",
       "      <td>2019-01-21 00:24:07</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5.13</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>21.30</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trip data/yellow_tripdata_2019-01.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-21 00:00:29</td>\n",
       "      <td>2019-01-21 00:05:11</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10.71</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trip data/yellow_tripdata_2019-01.csv</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-21 00:01:08</td>\n",
       "      <td>2019-01-21 00:05:28</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.09</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>7.98</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            CSV_FILENAME  VENDORID     PICKUP_DATETIME  \\\n",
       "0  trip data/yellow_tripdata_2019-01.csv         2 2019-01-21 00:01:16   \n",
       "1  trip data/yellow_tripdata_2019-01.csv         2 2019-01-21 00:03:49   \n",
       "2  trip data/yellow_tripdata_2019-01.csv         2 2019-01-21 00:00:30   \n",
       "3  trip data/yellow_tripdata_2019-01.csv         2 2019-01-21 00:00:29   \n",
       "4  trip data/yellow_tripdata_2019-01.csv         2 2019-01-21 00:01:08   \n",
       "\n",
       "     DROPOFF_DATETIME  PASSENGER_COUNT PICKUP_LONGITUDE PICKUP_LATITUDE  \\\n",
       "0 2019-01-21 00:05:15                3             None            None   \n",
       "1 2019-01-21 00:05:07                1             None            None   \n",
       "2 2019-01-21 00:24:07                1             None            None   \n",
       "3 2019-01-21 00:05:11                1             None            None   \n",
       "4 2019-01-21 00:05:28                1             None            None   \n",
       "\n",
       "   TRIP_DISTANCE  RATECODEID STORE_AND_FWD_FLAG  ... DROPOFF_TAXIZONE_ID  \\\n",
       "0           0.82           1                  N  ...                 229   \n",
       "1           0.21           1                  N  ...                 239   \n",
       "2           5.13           1                  N  ...                  37   \n",
       "3           1.36           1                  N  ...                 137   \n",
       "4           1.09           1                  N  ...                 263   \n",
       "\n",
       "  PAYMENT_TYPE  FARE_AMOUNT  EXTRA  MTA_TAX  TIP_AMOUNT  TOLLS_AMOUNT  \\\n",
       "0            1          5.5    0.5      0.5        1.36           0.0   \n",
       "1            2          3.0    0.5      0.5        0.00           0.0   \n",
       "2            2         20.0    0.5      0.5        0.00           0.0   \n",
       "3            1          6.0    0.5      0.5        1.46           0.0   \n",
       "4            1          5.5    0.5      0.5        1.18           0.0   \n",
       "\n",
       "   IMPROVEMENT_SURCHARGE  TOTAL_AMOUNT  CONGESTION_SURCHARGE  \n",
       "0                    0.3          8.16                   NaN  \n",
       "1                    0.3          4.30                   NaN  \n",
       "2                    0.3         21.30                   NaN  \n",
       "3                    0.3         10.71                   NaN  \n",
       "4                    0.3          7.98                   NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually cause the lazy computations to run, such as when finding the sum of a column in Pandas, for Dask we need to end with `.computed()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2430011.1799999997"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_data['TOLLS_AMOUNT'].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "As you can see, by using Dask we are able to store data across multiple machines and run distributed calculations, all while using the same syntax as Pandas. Depending on the size and type of data you are working with it may make more sense to either use Pandas directly or Dask instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}