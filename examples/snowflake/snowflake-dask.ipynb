{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Snowflake into Saturn Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our users have varied needs for loading data, but Snowflake is a very popular choice for data storage. This tutorial will give you foundational information to load data from Snowflake into Saturn Cloud quickly and easily.\n",
    "\n",
    "If you have questions about making a connection between Saturn Cloud and Snowflake, visit our [documentation about data connectivity](https://saturncloud.io/docs/getting-started/connect_data/).\n",
    "\n",
    "This notebook shows how to connect to a Snowflake database and do large data manipulations that would require distributed computing using Dask. We will use the NYC Taxi dataset hosted in a Saturn Cloud Snowflake database.\n",
    "\n",
    "First, we import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Don't worry if you see a warning about an incompatible version of pyarrow installed. This is because `snowflake.connector` relies on a older version of pyarrow for certain methods. We won't use those methods here so it's not a problem!\n",
    "\n",
    "Next, we connect to the Snowflake database using the Snowflake connector module. Here we are loading the credentials as environment variables using the Saturn Cloud credential manager, however you could [load them in other ways too](https://www.saturncloud.io/docs/getting-started/credentials/). Be sure not to save them as plaintext in unsafe places!\n",
    "\n",
    "## Connection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_info = {\n",
    "    \"account\": os.environ[\"EXAMPLE_SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"EXAMPLE_SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"EXAMPLE_SNOWFLAKE_PASSWORD\"],\n",
    "    \"database\": os.environ[\"TAXI_DATABASE\"],\n",
    "}\n",
    "\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snowflake Connector (pandas)\n",
    "\n",
    "We can run queries directly on a Snowflake database and load them into a pandas DataFrame. In the following cell we run a query to determine which days had a least one taxi ride in January 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_query = \"\"\"\n",
    "SELECT\n",
    "    DISTINCT(DATE(pickup_datetime)) as date\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    pickup_datetime BETWEEN '2019-01-01' and '2019-01-31'\n",
    "\"\"\"\n",
    "dates = pd.read_sql(dates_query, conn)[\"DATE\"].tolist()\n",
    "\n",
    "print(dates[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Snowflake Connector with Dask\n",
    "\n",
    "When data sizes exceed what can fit into a single pandas DataFrame, we can read larger datasets into Dask DataFrames.\n",
    "\n",
    "To use Dask with Snowflake, first we import the required modules and connect to the Dask cluster on Saturn Cloud. For this code to run you need to [start the Dask cluster from the project page of Saturn Cloud](https://saturncloud.io/docs/examples/dask/create_cluster_ui/).\n",
    "\n",
    "### Start the Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function that will query a small part of the data. Here we query information about a single day of taxi rides. We will run this query for each day separately and using all the Dask workers to run the queries and store the results. The `@dask.delayed` indicates that this function will be run over the Dask cluster. (For more information about delayed functions, check out [the Dask documentation](https://docs.dask.org/en/latest/).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def load_from_snowflake(day):\n",
    "    with snowflake.connector.connect(**conn_info) as conn:\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM taxi_yellow\n",
    "        WHERE\n",
    "            date(pickup_datetime) = '{day}'\n",
    "        \"\"\"\n",
    "        df = pd.read_sql(query, conn)\n",
    "        # some days have no values for congestion_surcharge, this line ensures\n",
    "        # that the missing data is properly filled\n",
    "        df.CONGESTION_SURCHARGE = df.CONGESTION_SURCHARGE.astype(\"float64\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list of all the results from running this query in a distributed way over all the days. As you can see from the `delayed_obs[:5]` call, these aren't pandas dataframes that are returned, they are Dask objects. The queries haven't actually be run yet, since Dask is lazy they won't be run until they are needed.\n",
    "\n",
    "The list of delayed observations can be turned into a single Dask Dataframe using `.from_delayed()`. A Dask DataFrame performs just liked a pandas DataFrame, they can use similar function calls and share a similar syntax, however the Dask DataFrame is actually a collection of pandas dataframes all distributed across a Dask cluster.\n",
    "\n",
    "## Pull Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_obs = [load_from_snowflake(day) for day in dates]\n",
    "delayed_obs[:5]\n",
    "\n",
    "dask_data = dd.from_delayed(delayed_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the contents of the DataFrame by using the same `.head()` call on the Dask DataFrame as we would on the pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually cause the lazy computations to run, such as when finding the sum of a column in pandas, for Dask we need to end with `.computed()` or `.persist()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_data[\"TOLLS_AMOUNT\"].sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "As you can see, by using Dask we are able to store data across multiple machines and run distributed calculations, all while using the same syntax as pandas. Depending on the size and type of data you are working with it may make more sense to either use pandas directly or Dask instead!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
