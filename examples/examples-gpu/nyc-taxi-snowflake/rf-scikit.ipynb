{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classification (single-node, CPU)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../_img/scikit-learn.png\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../_img/snowflake.png\" width=\"450\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a machine learning training workflow using the famous [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). That dataset contains information on taxi trips in New York City.\n",
    "\n",
    "In this exercise, you'll use `pandas` to load a subset of the data from Snowflake and `scikit-learn` to answer this classification question:\n",
    "\n",
    "> based on characteristics that can be known at the beginning of a trip, will this trip result in a high tip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This notebook has some cells that can take 3-10 minutes to run. Consider opening [this Dask + RAPIDS notebook](./rf-dask-rapids.ipynb) and running that while you're waiting for cells in this notebook to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Resource Usage\n",
    "\n",
    "As you work through this tutorial, you may want to monitor the code's utilization of the available CPU and memory, to get a better understanding of how expensive different operations are.\n",
    "\n",
    "<details><summary>(click here to learn how to monitor resource utilization)</summary>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Monitoring CPU and Main Memory**\n",
    "\n",
    "To monitor CPU utilization and the amount of free main memory, you can use `htop`.\n",
    "\n",
    "Open a new terminal and run `htop`. That will keep an auto-updating dashboard up that shows the CPU utilization and memory usage.\n",
    "\n",
    "**Monitoring GPU and GPU memory**\n",
    "    \n",
    "Open a new terminal and run the following command.\n",
    "\n",
    "```shell\n",
    "watch -n 5 nvidia-smi\n",
    "```\n",
    "\n",
    "<br>\n",
    " \n",
    "This command will update the output in the terminal every 5 seconds. It shows some information like:\n",
    "\n",
    "* current CUDA version\n",
    "* NVIDIA driver version\n",
    "* internal temperature\n",
    "* current utilization of GPU memory\n",
    "* list of processes (if any) currently running on the GPU, and how much GPU memory they're consuming\n",
    "\n",
    "You should see that `scikit-learn` never uses the available GPU.    \n",
    "\n",
    "<br>\n",
    "\n",
    "Whichever option you choose, leave these terminals with the monitoring process running while you work, so you can see how the code below uses the available resources.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Connect to Snowflake\n",
    "\n",
    "This example uses data stored in a Snowflake data warehouse that is managed by the team at Saturn Cloud. We've set up a read-only user for use in these examples. If you would like to access data stored in your own Snowflake account, you should set up [Credentials](https://www.saturncloud.io/docs/concepts/credentials/) for your account, user, and password then set the other connection information accordingly. For more details on Snowflake connection information, see [\"Connecting to Snowflake\"](https://docs.snowflake.com/en/user-guide/python-connector-example.html#connecting-to-snowflake) in the `snowflake-connector-python` docs.\n",
    "\n",
    "Note that in order to update environment variables your Jupyter server will need to be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "\n",
    "conn_info = {\n",
    "    \"account\": os.environ[\"EXAMPLE_SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"EXAMPLE_SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"EXAMPLE_SNOWFLAKE_PASSWORD\"],\n",
    "    \"database\": os.environ[\"TAXI_DATABASE\"],\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `taxi_yellow` table has NYC taxi data from 2017-2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"select * from taxi_yellow limit 5\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Load data\n",
    "\n",
    "This example is designed to run quickly with small resources. So let's just load a single month of taxi data for training.\n",
    "\n",
    "This example uses Snowflake to handle the hard work of creating new features, then creates a `pandas` data frame with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    pickup_taxizone_id,\n",
    "    dropoff_taxizone_id,\n",
    "    passenger_count,\n",
    "    DIV0(tip_amount, fare_amount) > 0.2 AS high_tip,\n",
    "    DAYOFWEEKISO(pickup_datetime) - 1 AS pickup_weekday,\n",
    "    WEEKOFYEAR(pickup_datetime) AS pickup_weekofyear,\n",
    "    HOUR(pickup_datetime) AS pickup_hour,\n",
    "    (pickup_weekday * 24) + pickup_hour AS pickup_week_hour,\n",
    "    MINUTE(pickup_datetime) AS pickup_minute\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    DATE_TRUNC('MONTH', pickup_datetime) = '{day}'\n",
    "\"\"\"\n",
    "\n",
    "taxi = pd.read_sql(query.format(day=\"2019-01-01\"), conn)\n",
    "taxi.columns = taxi.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "numeric_feat = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_week_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "categorical_feat = [\n",
    "    \"pickup_taxizone_id\",\n",
    "    \"dropoff_taxizone_id\",\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = \"high_tip\"\n",
    "\n",
    "query_result = conn.cursor().execute(query.format(day=\"2019-01-01\"))\n",
    "columns = [x[0] for x in query_result.description]\n",
    "taxi_train = pd.DataFrame(query_result.fetchall(), columns=columns)\n",
    "taxi_train.columns = taxi_train.columns.str.lower()\n",
    "taxi_train = taxi_train[features + [y_col]].astype(float).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes the size of this dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num rows: {len(taxi_train)}, Size: {taxi_train.memory_usage(deep=True).sum() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the structure of the data with `pandas` commands:\n",
    "\n",
    "`.head()` = view the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dtypes` = list all the columns and the type of data in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`taxi_train` is a `pandas` dataframe that will be passed in to a machine learning model. Since this is a binary classification task, before proceeding we should examine the distributions of 1s and 0s in the target. This can be done with the `.value_counts()` method.\n",
    "\n",
    "<details><summary>(click here to learn why data scientists do this)</summary>\n",
    "\n",
    "**Target Distribution**\n",
    "    \n",
    "In binary classification tasks, we ask machine learning models to learn the difference between records that produced a 0 for some target and those that produce a 1. In this example:\n",
    "    \n",
    "* `high_tip = 1`: \"this trip resulted in a high tip\"\n",
    "* `high_tip = 0`: \"this trip did not result in a high tip\"\n",
    "    \n",
    "The \"distribution\" of this target just means \"what % of the trips ended in a high tip?\". This is important to understand because we might have to change our modelling approach if the distribution was uneven. For example, if 99% of trips did NOT end in a high tip, a model that just always guessed \"not a high tip\" would achieve an accuracy of 99%.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.high_tip.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train a Model\n",
    "\n",
    "Now that the data have been prepped, it's time to build a model!\n",
    "\n",
    "For this task, we'll use the `RandomForestClassifier` from `scikit-learn`. If you've never used a random forest or need a refresher, consult [\"Forests of randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#forest) in the `sciki-learn` documentation.\n",
    "\n",
    "The code below initializes a random forest classifier with the following parameter values.\n",
    "\n",
    "* `n_estimators=100` = create a 100-tree forest\n",
    "* `max_depth=10` = stop growing a tree once it contains a leaf node that is 10 levels below the root\n",
    "* `random_state=42` = ensure that every run produces the same results. The choice of `42` is irrelevant and this could be any integer.\n",
    "* `n_jobs=-1` = use all available cores on this machine to train models. Note that `scikit-learn` does not use the GPU, and this only refers to CPU cores.\n",
    "\n",
    "All other parameters use the defaults from `RandomForestClassifier`.\n",
    "\n",
    "<details><summary>(click here to learn why data scientists do this)</summary>\n",
    "\n",
    "**Setting max_depth**\n",
    "    \n",
    "Tree-based models split the training data into smaller and smaller groups, to try to group together records with similar values of the target. A tree can be thought of as a collection of rules like `pickup_hour greater than 11` and `pickup_minute less than 31.0`. As you add more rules, those groups (called \"leaf nodes\") get smaller. In an extreme example, a model could create a tree with enough rules to place each record in the training data into its own group. That would probably take a lot of rules, and would be referred to as a \"deep\" tree.\n",
    "    \n",
    "Deep trees are problematic because their descriptions of the world are too specific to be useful on new data. Imagine training a classification model to predict whether or not visitors to a theme park will ride a particular rollercoaster. You could measure the time down to the millisecond that every guest's ticket is scanned at the entrance, and a model might learn a rule like *\"if the guest has been to the park before and if the guest is older than 40 and younger than 41, and if the guest is staying at Hotel A and if the guest enters the park after 1:00:17.456 and if the guest enters the park earlier than 1:00:17.995, they will ride the rollercoaster\"*. This is very very unlikely to ever match any future visitors, and if it does it's unlikely that this prediction will be very good unless you have some reason to believe that a visitor arriving at 1:00:18 instead of 1:00:17 really changes the probability that they'll ride that rollercoaster.\n",
    "    \n",
    "To prevent this situation (called \"overfitting\"), most tree-based machine learning algorithms accept parameters that control how deep the trees can get. `max_depth` is common, and says \"don't create a rule more complex than this\". In the example above, that rule has a depth of 7.\n",
    "    \n",
    "1. visiting the park\n",
    "2. has been to the park before?\n",
    "3. older than 40?\n",
    "4. younger than 41?\n",
    "5. staying at Hotel A?\n",
    "6. entered the park after 1:00:17.456?\n",
    "7. entered the park before 1:00:17.995?\n",
    "    \n",
    "Setting `max_depth = 5` would have prevented those weirdly-specific timing rules from ever being generated.\n",
    "    \n",
    "Choosing good values for this parameter is part art, part science, and is outside the scope of this tutorial.\n",
    "    \n",
    "**Setting n_jobs=-1**\n",
    "\n",
    "In the random forest algorithm, each tree in the forest is completely independent of the others. This means that you can cut the training time by 50% by training two trees at the same time, 75% by training 4 trees at a time, and so on. Your computer's ability to parallelize work like this is determined by the number of available CPU cores. Setting `n_jobs=-1` says \"use all available CPU cores to train this model\", which will result in the fastest possible training time.\n",
    "\n",
    "The value `-1` is preferred to directly hard-coding a number, because it means that if you run this code on a different-sized machine, it will take advantage of all of that machine's resources without requiring you to remember to change the code.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classifier created, fit it to some data! The code below uses `%%time` to print out a timing, so you can see how long it takes to train. This can be used to compare `scikit-learn` to methods explored in other notebooks, or to test how changing some parameters to `RandomForestClassifier` changes the runtime for training.\n",
    "\n",
    "**NOTE:** This will take a few minutes to run. Consider opening [this Dask + RAPIDS notebook](./rf-rapids-dask.ipynb) and running that while you're waiting for this model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = rfc.fit(taxi_train[features], taxi_train[y_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Save model\n",
    "\n",
    "Once you've trained a model, save it in a file to use later for scoring or for comparison with other models.\n",
    "\n",
    "There are several ways to do this, but `cloudpickle` is likely to give you the best experience. It handles some common drawbacks of the built-in `pickle` library.\n",
    "\n",
    "`cloudpickle` can be used to write a Python object to bytes, and to create a Python object from that binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "with open(f\"{MODEL_PATH}/random_forest_scikit.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(rfc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Calculate metrics on test set\n",
    "\n",
    "Machine learning training tries to create a model which can produce useful results on new data that it didn't see during training. To test how well we've accomplished that in this example, read in another month of taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = pd.read_sql(query.format(day=\"2019-02-01\"), conn)\n",
    "taxi_test.columns = taxi_test.columns.str.lower()\n",
    "taxi_test = taxi_test.astype(float).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` comes with many functions for calculating metrics that describe how well a model's predictions match the actual values. For a complete list, see [\"Metrics and scoring\"](https://scikit-learn.org/stable/modules/model_evaluation.html) in the `sciki-learn` docs.\n",
    "\n",
    "This tutorial uses the `roc_auc_score` to evaluate the model. This metric measures the area under the [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. Values closer to 1.0 are desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "preds = rfc.predict_proba(taxi_test[features])[:, 1]\n",
    "roc_auc_score(taxi_test[y_col], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to train a model for a binary classification task, using `scikit-learn`, based on data in Snowflake. Training took 9 minutes, for a dataset that was only 0.55 GB in memory.\n",
    "\n",
    "You can train the same model much more quickly by taking advantage of the available GPU on this machine. Try [this Dask + RAPIDS notebook](./rf-rapids-dask.ipynb) next to see how this is done.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}