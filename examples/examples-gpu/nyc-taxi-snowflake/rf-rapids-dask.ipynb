{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classification (multi-node, multi-GPU)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../_img/dask-horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../_img/rapids.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../_img/snowflake.png\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a machine learning training workflow using the famous [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). That dataset contains information on taxi trips in New York City.\n",
    "\n",
    "In this exercise, you'll use `dask-cudf` to load a subset of the data and `cuml` to answer this classification question:\n",
    "\n",
    "> based on characteristics that can be known at the beginning of a trip, will this trip result in a high tip?\n",
    "\n",
    "## Use RAPIDS libraries\n",
    "\n",
    "RAPIDS is a collection of libraries which enable you to take advantage of NVIDIA GPUs to accelerate machine learning workflows. This exercise uses the following RAPIDS packages:\n",
    "    \n",
    "* [`dask-cudf`](https://docs.rapids.ai/api/cudf/stable/dask-cudf.html): distributed `cudf` dataframes using Dask\n",
    "* [`cuml`](https://github.com/rapidsai/cuml): machine learning training and evaluation, similar to `scikit-learn`\n",
    "\n",
    "These libraries are already available in the `saturncloud/saturn-gpu` images by default. For more information on RAPIDS, see [\"Getting Started\"](https://rapids.ai/start.html) in the RAPIDS docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Initialize A Dask GPU Cluster\n",
    "\n",
    "This tutorial uses multiple machines to show how to apply more computing resources to machine learning training. This is done with Dask. Saturn Cloud offers managed Dask clusters, which can be provisioned and modified programmatically.\n",
    "\n",
    "The code below creates a Dask cluster using [`dask-saturn`](https://github.com/saturncloud/dask-saturn), the official Dask client for Saturn Cloud. It creates a cluster with the following specs:\n",
    "\n",
    "* `n_workers=3` --> 3 machines in the cluster\n",
    "* `scheduler_size='medium'` --> the Dask scheduler will have 4GB of RAM and 2 CPU cores\n",
    "* `worker_size='g4dnxlarge'` --> each worker machine will have 4 CPU cores, 16GB of RAM, 16GB of GPU RAM, and 1 GPU\n",
    "\n",
    "To see a list of possible sizes, run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'medium': 'Medium - 2 cores - 4 GB RAM',\n",
       " 'large': 'Large - 2 cores - 16 GB RAM',\n",
       " 'xlarge': 'XLarge - 4 cores - 32 GB RAM',\n",
       " '2xlarge': '2XLarge - 8 cores - 64 GB RAM',\n",
       " '4xlarge': '4XLarge - 16 cores - 128 GB RAM',\n",
       " '8xlarge': '8XLarge - 32 cores - 256 GB RAM',\n",
       " '12xlarge': '12XLarge - 48 cores - 384 GB RAM',\n",
       " '16xlarge': '16XLarge - 64 cores - 512 GB RAM',\n",
       " 'g4dnxlarge': 'T4-XLarge - 4 cores - 16 GB RAM - 1 GPU',\n",
       " 'g4dn4xlarge': 'T4-4XLarge - 16 cores - 64 GB RAM - 1 GPU',\n",
       " 'g4dn8xlarge': 'T4-8XLarge - 32 cores - 128 GB RAM - 1 GPU',\n",
       " 'p32xlarge': 'V100-2XLarge - 8 cores - 61 GB RAM - 1 GPU',\n",
       " 'p38xlarge': 'V100-8XLarge - 32 cores - 244 GB RAM - 4 GPU',\n",
       " 'p316xlarge': 'V100-16XLarge - 64 cores - 488 GB RAM - 8 GPU'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask_saturn\n",
    "\n",
    "dask_saturn.describe_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask-saturn` code below creates two important objects: a cluster and a client.\n",
    "\n",
    "* `cluster`: knows about and manages the scheduler and workers\n",
    "    - can be used to create, resize, reconfigure, or destroy those resources\n",
    "    - knows how to communicate with the scheduler, and where to find logs and diagnostic dashboards\n",
    "* `client`: tells the cluster to do things\n",
    "    - can send work to the cluster\n",
    "    - can restart all the worker processes\n",
    "    - can send data to the cluster or pull data back from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-01-21 15:44:52] INFO - dask-saturn | Starting cluster. Status: pending\n",
      "[2021-01-21 15:45:01] INFO - dask-saturn | Starting cluster. Status: pending\n",
      "[2021-01-21 15:45:13] INFO - dask-saturn | Cluster is ready\n",
      "[2021-01-21 15:45:13] INFO - dask-saturn | Registering default plugins\n",
      "[2021-01-21 15:45:13] INFO - dask-saturn | {'tcp://10.0.10.129:43827': {'status': 'OK'}, 'tcp://10.0.21.103:40637': {'status': 'OK'}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ece1868022d463b9b78a06aa28805d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SaturnCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers, \n",
    "    scheduler_size=\"medium\", \n",
    "    worker_size=\"g4dnxlarge\",\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster in the Saturn UI if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'workers': {}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.scheduler_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://d-aaron-examples-gpu-41446557ab86456d8e5e62b6e21aa4ab.main-namespace:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='https://d-aaron-examples-gpu-41446557ab86456d8e5e62b6e21aa4ab.community.saturnenterprise.io' target='_blank'>https://d-aaron-examples-gpu-41446557ab86456d8e5e62b6e21aa4ab.community.saturnenterprise.io</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>31.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.0.10.71:8786' processes=2 threads=8, memory=31.00 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Resource Usage\n",
    "\n",
    "This tutorial aims to teach you how to take advantage of multiple GPUs for data science workflows. To prove to yourself that Dask RAPIDS are utilizing the GPUs, it's important to understand how to monitor that utilization while your code is running. If you already know how to do that, skip to the next section.\n",
    "\n",
    "Print the `cluster` object in a notebook renders a widget that shows the number of workers, available CPU and memory, and a dashboard link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ece1868022d463b9b78a06aa28805d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SaturnCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click that dashboard link to view some diagnostic information about the Dask cluster. This can be used to view the current resource utilization of workers in the cluster and lots of information about what they're currently working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple other dashboard pages worth viewing for GPU memory and utilization that are not listed on the navbar. Run the code below to view them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<b>GPU Dashboard links</b>\n",
       "<ul>\n",
       "<li><a href=\"https://d-aaron-examples-gpu-41446557ab86456d8e5e62b6e21aa4ab.community.saturnenterprise.io/individual-gpu-memory\" target=\"_blank\">GPU memory</a></li>\n",
       "<li><a href=\"https://d-aaron-examples-gpu-41446557ab86456d8e5e62b6e21aa4ab.community.saturnenterprise.io/individual-gpu-utilization\" target=\"_blank\">GPU utilization</a></li>\n",
       "</ul>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "gpu_links = f\"\"\"\n",
    "<b>GPU Dashboard links</b>\n",
    "<ul>\n",
    "<li><a href=\"{client.dashboard_link}/individual-gpu-memory\" target=\"_blank\">GPU memory</a></li>\n",
    "<li><a href=\"{client.dashboard_link}/individual-gpu-utilization\" target=\"_blank\">GPU utilization</a></li>\n",
    "</ul>\n",
    "\"\"\"\n",
    "display(HTML(gpu_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Connect to Snowflake\n",
    "\n",
    "This example uses data stored in a Snowflake data warehouse that is managed by the team at Saturn Cloud. We've set up a read-only user for use in these examples. If you would like to access data stored in your own Snowflake account, you should set up [Credentials](https://www.saturncloud.io/docs/concepts/credentials/) for your account, user, and password then set the other connection information accordingly. For more details on Snowflake connection information, see [\"Connecting to Snowflake\"](https://docs.snowflake.com/en/user-guide/python-connector-example.html#connecting-to-snowflake) in the `snowflake-connector-python` docs.\n",
    "\n",
    "Note that in order to update environment variables your Jupyter server will need to be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import dask_cudf as cudd\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "conn_info = {\n",
    "    \"account\": os.environ['EXAMPLE_SNOWFLAKE_ACCOUNT'],\n",
    "    \"user\": os.environ['EXAMPLE_SNOWFLAKE_USER'],\n",
    "    \"password\": os.environ['EXAMPLE_SNOWFLAKE_PASSWORD'],\n",
    "    \"warehouse\": 'saturn_warehouse',\n",
    "    'database': 'saturn_nyc_taxi',\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Load data\n",
    "\n",
    "This example is designed to run quickly with small resources. So let's just load a single month of taxi data for training.\n",
    "\n",
    "The code below loads the data into a `dask-cudf` data frame. Your code here in this notebook can interact with this data structure as if it was just a regular `cudf` data frame, but it is actually a collection of smaller `cudf` data frames spread across the workers in the Dask cluster.\n",
    "\n",
    "A `cudf` data frame is similar to a `pandas` data frame, but it lives in GPU memory and most operations on it are done on the GPU.\n",
    "\n",
    "This example uses Snowflake to handle the hard work of creating new features, then creates a `cudf` data frame with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    pickup_taxizone_id,\n",
    "    dropoff_taxizone_id,\n",
    "    passenger_count,\n",
    "    DIV0(tip_amount, fare_amount) > 0.2 AS high_tip,\n",
    "    DAYOFWEEKISO(pickup_datetime) - 1 AS pickup_weekday,\n",
    "    WEEKOFYEAR(pickup_datetime) AS pickup_weekofyear,\n",
    "    HOUR(pickup_datetime) AS pickup_hour,\n",
    "    (pickup_weekday * 24) + pickup_hour AS pickup_week_hour,\n",
    "    MINUTE(pickup_datetime) AS pickup_minute\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = %s\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def load(conn_info, query, day):\n",
    "    with snowflake.connector.connect(**conn_info) as conn:\n",
    "        taxi = conn.cursor().execute(query, day).fetch_pandas_all()\n",
    "        taxi.columns = taxi.columns.str.lower()\n",
    "        taxi = cudf.from_pandas(taxi)\n",
    "        return taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(start, end):\n",
    "    date_query = \"\"\"\n",
    "    SELECT\n",
    "        DISTINCT(DATE(pickup_datetime)) as date\n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        pickup_datetime BETWEEN %s and %s\n",
    "    \"\"\"\n",
    "    dates_df = conn.cursor().execute(date_query, (start, end))\n",
    "    dates_df = conn.cursor().execute(date_query, (\"2019-01-01\", \"2019-01-31\"))\n",
    "    return dates_df.fetch_pandas_all()['DATE'].tolist()\n",
    "\n",
    "\n",
    "dates = get_dates(\"2019-01-01\", \"2019-01-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = cudd.from_delayed([load(conn_info, query, day) for day in dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_taxizone_id</th>\n",
       "      <th>dropoff_taxizone_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>high_tip</th>\n",
       "      <th>pickup_weekday</th>\n",
       "      <th>pickup_weekofyear</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_week_hour</th>\n",
       "      <th>pickup_minute</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=31</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int16</td>\n",
       "      <td>int16</td>\n",
       "      <td>int16</td>\n",
       "      <td>bool</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>int8</td>\n",
       "      <td>int16</td>\n",
       "      <td>int8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: from-delayed, 62 tasks</div>"
      ],
      "text/plain": [
       "<dask_cudf.DataFrame | 62 tasks | 31 npartitions>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask performs computations in a [lazy manner](https://tutorial.dask.org/01x_lazy.html), so we persist the dataframe to perform data loading and feature processing and load into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekofyear\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_week_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "categorical_feat = [\n",
    "    \"pickup_taxizone_id\",\n",
    "    \"dropoff_taxizone_id\",\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = \"high_tip\"\n",
    "\n",
    "taxi_train = taxi[features + [y_col]]\n",
    "taxi_train[features] = taxi_train[features].astype(\"float32\").fillna(-1)\n",
    "taxi_train[y_col] = taxi_train[y_col].astype(\"int32\").fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 ms, sys: 1.71 ms, total: 159 ms\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "taxi_train = taxi_train.persist()\n",
    "_ = wait(taxi_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes the size of this dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rows: 7667648, Size: 276.035328 MB\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Num rows: {len(taxi_train)}, Size: {taxi_train.memory_usage(deep=True).sum().compute() / 1e6} MB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the structure of the data with `cudf` commands:\n",
    "\n",
    "`.head()` = view the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_weekday</th>\n",
       "      <th>pickup_weekofyear</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_week_hour</th>\n",
       "      <th>pickup_minute</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_taxizone_id</th>\n",
       "      <th>dropoff_taxizone_id</th>\n",
       "      <th>high_tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_weekday  pickup_weekofyear  pickup_hour  pickup_week_hour  \\\n",
       "0             5.0                4.0         19.0             139.0   \n",
       "1             5.0                4.0         23.0             143.0   \n",
       "2             5.0                4.0         23.0             143.0   \n",
       "3             5.0                4.0         23.0             143.0   \n",
       "4             5.0                4.0         23.0             143.0   \n",
       "\n",
       "   pickup_minute  passenger_count  pickup_taxizone_id  dropoff_taxizone_id  \\\n",
       "0           30.0              1.0               255.0                263.0   \n",
       "1           58.0              6.0               164.0                230.0   \n",
       "2           56.0              1.0               249.0                 33.0   \n",
       "3           59.0              2.0               264.0                264.0   \n",
       "4           59.0              1.0               141.0                151.0   \n",
       "\n",
       "   high_tip  \n",
       "0         0  \n",
       "1         1  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dtypes` = list all the columns and the type of data in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_taxizone_id     int16\n",
       "dropoff_taxizone_id    int16\n",
       "passenger_count        int16\n",
       "high_tip                bool\n",
       "pickup_weekday          int8\n",
       "pickup_weekofyear       int8\n",
       "pickup_hour             int8\n",
       "pickup_week_hour       int16\n",
       "pickup_minute           int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we say that a `dask-cudf` dataframe is a *distributed* data frame, that means that it comprises multiple smaller `cudf` data frames. Run the following to see how many of these pieces (called \"partitions\") there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`taxi_train` is a `dask_cudf` dataframe that will be passed in to a machine learning model. Since this is a binary classification task, before proceeding we should examine the distributions of 1s and 0s in the target. This can be done with by combining `.groupby()` and `.count()`\n",
    "\n",
    "<details><summary>(click here to learn why data scientists do this)</summary>\n",
    "\n",
    "**Target Distribution**\n",
    "    \n",
    "In binary classification tasks, we ask machine learning models to learn the difference between records that produced a 0 for some target and those that produce a 1. In this example:\n",
    "    \n",
    "* `high_tip = 1`: \"this trip resulted in a high tip\"\n",
    "* `high_tip = 0`: \"this trip did not result in a high tip\"\n",
    "    \n",
    "The \"distribution\" of this target just means \"what % of the trips ended in a high tip?\". This is important to understand because we might have to change our modelling approach if the distribution was uneven. For example, if 99% of trips did NOT end in a high tip, a model that just always guessed \"not a high tip\" would achieve an accuracy of 99%.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high_tip\n",
       "1    4001846\n",
       "0    3665802\n",
       "Name: high_tip, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_train.groupby(\"high_tip\")[\"high_tip\"].count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train a Model\n",
    "\n",
    "Now that the data have been prepped, it's time to build a model!\n",
    "\n",
    "For this task, we'll use the `RandomForestClassifier` from `cuml`. If you've never used a random forest or need a refresher, consult [\"Forests of randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#forest) in the `sciki-learn` documentation.\n",
    "\n",
    "The code below initializes a random forest classifier with the following parameter values.\n",
    "\n",
    "* `n_estimators=100` = create a 100-tree forest\n",
    "* `max_depth=10` = stop growing a tree once it contains a leaf node that is 10 levels below the root\n",
    "\n",
    "All other parameters use the defaults from `RandomForestClassifier`.\n",
    "\n",
    "<details><summary>(click here to learn why data scientists do this)</summary>\n",
    "\n",
    "**Setting max_depth**\n",
    "\n",
    "Tree-based models split the training data into smaller and smaller groups, to try to group together records with similar values of the target. A tree can be thought of as a collection of rules like `pickup_hour greater than 11` and `pickup_minute less than 31.0`. As you add more rules, those groups (called \"leaf nodes\") get smaller. In an extreme example, a model could create a tree with enough rules to place each record in the training data into its own group. That would probably take a lot of rules, and would be referred to as a \"deep\" tree.\n",
    "\n",
    "Deep trees are problematic because their descriptions of the world are too specific to be useful on new data. Imagine training a classification model to predict whether or not visitors to a theme park will ride a particular rollercoaster. You could measure the time down to the millisecond that every guest's ticket is scanned at the entrance, and a model might learn a rule like *\"if the guest has been to the park before and if the guest is older than 40 and younger than 41, and if the guest is staying at Hotel A and if the guest enters the park after 1:00:17.456 and if the guest enters the park earlier than 1:00:17.995, they will ride the rollercoaster\"*. This is very very unlikely to ever match any future visitors, and if it does it's unlikely that this prediction will be very good unless you have some reason to believe that a visitor arriving at 1:00:18 instead of 1:00:17 really changes the probability that they'll ride that rollercoaster.\n",
    "\n",
    "To prevent this situation (called \"overfitting\"), most tree-based machine learning algorithms accept parameters that control how deep the trees can get. `max_depth` is common, and says \"don't create a rule more complex than this\". In the example above, that rule has a depth of 7.\n",
    "\n",
    "1. visiting the park\n",
    "2. has been to the park before?\n",
    "3. older than 40?\n",
    "4. younger than 41?\n",
    "5. staying at Hotel A?\n",
    "6. entered the park after 1:00:17.456?\n",
    "7. entered the park before 1:00:17.995?\n",
    "\n",
    "Setting `max_depth = 5` would have prevented those weirdly-specific timing rules from ever being generated.\n",
    "\n",
    "Choosing good values for this parameter is part art, part science, and is outside the scope of this tutorial.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classifier created, fit it to some data! The code below uses `%%time` to print out a timing, so you can see how long it takes to train. This can be used to compare `cuml` to methods explored in other notebooks, or to test how changing some parameters to `RandomForestClassifier` changes the runtime for training.\n",
    "\n",
    "You might notice that the code below looks identical to the code from the non-Dask, single node example covered in the [single-node RAPIDS example](./rf-rapids.ipynb). This is a really nice feature of `cuml`. `cuml` models can accept training data in `dask_cudf` or `cudf` format. Pretty cool, right?\n",
    "\n",
    "As soon as you run the cell below, switch over to the tab where you started up the Dask diagnostic dashboard(s). You should see some work being done! If it finishes too quickly, change `n_estimators` in the cell above to a larger value, like 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 236 ms, sys: 157 ms, total: 393 ms\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = rfc.fit(taxi_train[features], taxi_train[y_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "\n",
    "The distributed model objects from `cuml` cannot be pickled directly. Some of the model objects come with methods for generating a single-node equivalent (see [\"Pickling cuML Models\"](https://docs.rapids.ai/api/cuml/stable/pickling_cuml_models.html#Distributed-Model-Pickling)), but unfortunately `cuml.dask.ensemble.RandomForestClassifier` is not one of those.\n",
    "\n",
    "Your best option for storing this model is to use a private method, `._concat_treelite_models()`. This produces a regular `cuml.ensemble.randomforestclassifier.RandomForestClassifier`, which you can use for scoring on local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = rfc._concat_treelite_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use this model to score on local data, like in a `pandas` data frame or `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1\n",
       "0   0.05  0.95\n",
       "1   0.24  0.76\n",
       "2   0.15  0.85\n",
       "3   0.08  0.92\n",
       "4   0.13  0.87\n",
       "..   ...   ...\n",
       "95  0.32  0.68\n",
       "96  0.15  0.85\n",
       "97  0.20  0.80\n",
       "98  0.15  0.85\n",
       "99  0.24  0.76\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.predict_proba(taxi_train.head(100)[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can also be saved for later use. There are several ways to do this, but `cloudpickle` is likely to give you the best experience. It handles some common drawbacks of the built-in `pickle` library.\n",
    "\n",
    "`cloudpickle` can be used to write a Python object to bytes, and to create a Python object from that binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "with open(f\"{MODEL_PATH}/random_forest_dask_rapids.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(local_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Calculate metrics on test set\n",
    "\n",
    "Machine learning training tries to create a model which can produce useful results on new data that it didn't see during training. To test how well we've accomplished that in this example, read in another month of taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = get_dates(\"2019-02-01\", \"2019-02-28\")\n",
    "taxi_test = cudd.from_delayed([load(conn_info, query, day) for day in test_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = taxi_test[features + [y_col]]\n",
    "taxi_test[features] = taxi_test[features].astype(\"float32\").fillna(-1)\n",
    "taxi_test[y_col] = taxi_test[y_col].astype(\"int32\").fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = taxi_test.persist()\n",
    "_ = wait(taxi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cuml` comes with many functions for calculating metrics that describe how well a model's predictions match the actual values. For a complete list, see [the cuml API docs](https://docs.rapids.ai/api/cuml/stable/api.html#metrics-regression-classification-and-distance) or run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAIRWISE_DISTANCE_METRICS',\n",
       " 'accuracy',\n",
       " 'accuracy_score',\n",
       " 'adjusted_rand_score',\n",
       " 'cluster',\n",
       " 'completeness_score',\n",
       " 'confusion_matrix',\n",
       " 'entropy',\n",
       " 'homogeneity_score',\n",
       " 'log_loss',\n",
       " 'mean_absolute_error',\n",
       " 'mean_squared_error',\n",
       " 'mean_squared_log_error',\n",
       " 'mutual_info_score',\n",
       " 'pairwise_distances',\n",
       " 'precision_recall_curve',\n",
       " 'r2_score',\n",
       " 'regression',\n",
       " 'roc_auc_score',\n",
       " 'trustworthiness',\n",
       " 'utils']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cuml.metrics\n",
    "\n",
    "[m for m in dir(cuml.metrics) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial uses the `roc_auc_score` to evaluate the model. This metric measures the area under the [receiver operating characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. Values closer to 1.0 are desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import roc_auc_score\n",
    "\n",
    "preds = rfc.predict_proba(taxi_test[features])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of this writing, `cuml.metrics.roc_auc_score` does not support Dask collections as inputs. The code below uses `.compute()` to create numpy arrays instead. This isn't ideal because it means that the amount of data you can score on is limited by the size of the instance you're running this code on. Hopefully support for Dask collections will be added in a future verison of `cuml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5473746657371521"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(taxi_test[y_col].compute(), preds.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to train a model for a binary classification task, using `cuml` and Dask. Training took around 3 seconds for a dataset that was 0.31 GB in memory, a huge improvement on the almost 9 minutes it took to [train the same model using `scikit-learn`](./01-rf-scikit.ipynb)!\n",
    "\n",
    "You've reached the end of this set of exercises on using Snowflake + RAPIDS + Dask to accomplish distributed machine learning training using GPUs.\n",
    "\n",
    "If you want to try working with larger datasets, change the code in this notebook to use more data, or explore how to programmatically resize your Dask cluster by following the tips in [\"Managing Dask Resources for Machine Learning Training\"](https://www.saturncloud.io/docs/tips-and-tricks/training-on-dask).\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
