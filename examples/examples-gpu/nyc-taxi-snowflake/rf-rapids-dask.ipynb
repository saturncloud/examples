{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classification\n",
    "\n",
    "## Dask + RAPIDS GPU cluster\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://rapids.ai/assets/images/RAPIDS-logo-purple.svg\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_PATH = 'models'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "numeric_feat = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "categorical_feat = [\n",
    "    'pickup_taxizone_id', \n",
    "    'dropoff_taxizone_id',\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = 'high_tip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dask GPU cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "import time\n",
    "from dask import persist\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers,\n",
    "    scheduler_size='medium',\n",
    "    worker_size='g4dnxlarge'\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dashboard (link ^) and watch it when you execute some commands, you'll see which tasks are running across the cluster. There are a couple other dashboard pages worth viewing for GPU memory and utilization that are not listed on the navbar, so we grab direct links for those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "gpu_links = f'''\n",
    "<b>GPU Dashboard links</b>\n",
    "<ul>\n",
    "<li><a href=\"{client.dashboard_link}/individual-gpu-memory\" target=\"_blank\">GPU memory</a></li>\n",
    "<li><a href=\"{client.dashboard_link}/individual-gpu-utilization\" target=\"_blank\">GPU utilization</a></li>\n",
    "</ul>\n",
    "'''\n",
    "display(HTML(gpu_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster from the \"Dask\" page in Saturn if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering\n",
    "\n",
    "Load a full month for this exercise. Note we are loading the data with Dask+RAPIDS now (`dask_cudf.read_csv` vs. `pd.read_csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import cudf\n",
    "import dask_cudf as cudd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "SNOWFLAKE_ACCOUNT = os.environ['SNOWFLAKE_ACCOUNT']\n",
    "SNOWFLAKE_USER = os.environ['SNOWFLAKE_USER']\n",
    "SNOWFLAKE_PASSWORD = os.environ['SNOWFLAKE_PASSWORD']\n",
    "\n",
    "SNOWFLAKE_WAREHOUSE = os.environ['SNOWFLAKE_WAREHOUSE']\n",
    "TAXI_DATABASE = os.environ['TAXI_DATABASE']\n",
    "TAXI_SCHEMA = os.environ['TAXI_SCHEMA']\n",
    "\n",
    "conn_info = {\n",
    "    'account': SNOWFLAKE_ACCOUNT,\n",
    "    'user': SNOWFLAKE_USER,\n",
    "    'password': SNOWFLAKE_PASSWORD,\n",
    "    'warehouse': SNOWFLAKE_WAREHOUSE,\n",
    "    'database': TAXI_DATABASE,\n",
    "    'schema': TAXI_SCHEMA,\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    pickup_taxizone_id,\n",
    "    dropoff_taxizone_id,\n",
    "    passenger_count,\n",
    "    DIV0(tip_amount, fare_amount) > 0.2 AS high_tip,\n",
    "    DAYOFWEEKISO(pickup_datetime) - 1 AS pickup_weekday,\n",
    "    WEEKOFYEAR(pickup_datetime) AS pickup_weekofyear,\n",
    "    HOUR(pickup_datetime) AS pickup_hour,\n",
    "    (pickup_weekday * 24) + pickup_hour AS pickup_week_hour,\n",
    "    MINUTE(pickup_datetime) AS pickup_minute\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = %s\n",
    "\"\"\"\n",
    "\n",
    "@delayed\n",
    "def load(conn_info, query, day):\n",
    "    with snowflake.connector.connect(**conn_info) as conn:\n",
    "        taxi = conn.cursor().execute(query, '2019-01-01')\n",
    "        # using fetchall() because rapids requires a different pyarrow version than snowflake-connector-python\n",
    "        columns = [x[0] for x in taxi.description]\n",
    "        taxi = pd.DataFrame(taxi.fetchall(), columns=columns)\n",
    "        taxi.columns = taxi.columns.str.lower()\n",
    "        taxi = cudf.from_pandas(taxi)\n",
    "        return taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(start, end):\n",
    "    date_query = \"\"\"\n",
    "    SELECT\n",
    "        DISTINCT(DATE(pickup_datetime)) as date \n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        pickup_datetime BETWEEN %s and %s\n",
    "    \"\"\"\n",
    "    dates_df = conn.cursor().execute(date_query, (start, end))\n",
    "    columns = [x[0] for x in dates_df.description]\n",
    "    dates_df = pd.DataFrame(dates_df.fetchall(), columns=columns)\n",
    "    return dates_df['DATE'].tolist()\n",
    "\n",
    "dates = get_dates('2019-01-01', '2019-01-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = cudd.from_delayed([load(conn_info, query, day) for day in dates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask performs computations in a [lazy manner](https://tutorial.dask.org/01x_lazy.html), so we persist the dataframe to perform data loading and feature processing and load into GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train = taxi[features + [y_col]]\n",
    "taxi_train[features] = taxi_train[features].astype(\"float32\").fillna(-1)\n",
    "taxi_train[y_col] = taxi_train[y_col].astype(\"int32\").fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train = taxi_train.persist()\n",
    "_ = wait(taxi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num rows: {len(taxi_train)}, Size: {taxi_train.memory_usage(deep=True).sum().compute() / 1e6} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.groupby('high_tip')['high_tip'].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = rfc.fit(taxi_train[features], taxi_train[y_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not yet supported with cuml.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics on test set\n",
    "\n",
    "Use a different month for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = get_dates('2019-02-01', '2019-02-28')\n",
    "taxi_test = cudd.from_delayed([load(conn_info, query, day) for day in test_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = taxi_test[features + [y_col]]\n",
    "taxi_test[features] = taxi_test[features].astype(\"float32\").fillna(-1)\n",
    "taxi_test[y_col] = taxi_test[y_col].astype(\"int32\").fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = taxi_test.persist()\n",
    "_ = wait(taxi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Convert to single-GPU DataFrame using `compute()` because the Dask+RAPIDS implementation doesn't yet have `roc_auc_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.metrics import roc_auc_score\n",
    "\n",
    "preds = rfc.predict_proba(taxi_test[features])[1]\n",
    "roc_auc_score(taxi_test[y_col].compute(), preds.compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
