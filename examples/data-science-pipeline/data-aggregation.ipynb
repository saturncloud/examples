{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the data aggregation code to prepare data files for [the dashboard](./dashboard.ipynb). You can run this notebook to see how Dask is used with a Saturn cluster for data processing, but the files generated here will not be used by any of the examples. The dashboard uses pre-aggregated files from Saturn's public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "import hvplot.dask  # noqa\n",
    "import hvplot.pandas  # noqa\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Initialize A Dask Cluster\n",
    "\n",
    "This tutorial uses multiple machines to show how to apply more computing resources to machine learning training. This is done with Dask. Saturn Cloud offers managed Dask clusters, which can be provisioned and modified programmatically.\n",
    "\n",
    "The code below creates a Dask cluster using [`dask-saturn`](https://github.com/saturncloud/dask-saturn), the official Dask client for Saturn Cloud. It creates a cluster with the following specs:\n",
    "\n",
    "* `n_workers=3` --> 3 machines in the cluster\n",
    "* `scheduler_size='medium'` --> the Dask scheduler will have 4GB of RAM and 2 CPU cores\n",
    "* `worker_size='large'` --> each worker machine will have 2 CPU cores and 16GB of RAM\n",
    "\n",
    "To see a list of possible sizes, run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_saturn\n",
    "\n",
    "dask_saturn.describe_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask-saturn` code below creates two important objects: a cluster and a client.\n",
    "\n",
    "* `cluster`: knows about and manages the scheduler and workers\n",
    "    - can be used to create, resize, reconfigure, or destroy those resources\n",
    "    - knows how to communicate with the scheduler, and where to find logs and diagnostic dashboards\n",
    "* `client`: tells the cluster to do things\n",
    "    - can send work to the cluster\n",
    "    - can restart all the worker processes\n",
    "    - can send data to the cluster or pull data back from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(n_workers=n_workers, scheduler_size=\"medium\", worker_size=\"large\")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster in the Saturn UI if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Setup a function to load files with Dask. Cleanup some column names and parse data types correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = [\n",
    "    \"VendorID\",\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"RatecodeID\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "    \"payment_type\",\n",
    "    \"fare_amount\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"improvement_surcharge\",\n",
    "    \"total_amount\",\n",
    "]\n",
    "\n",
    "\n",
    "def read_taxi_csv(files):\n",
    "    ddf = dd.read_csv(\n",
    "        files,\n",
    "        assume_missing=True,\n",
    "        parse_dates=[1, 2],\n",
    "        usecols=usecols,\n",
    "        storage_options={\"anon\": True},\n",
    "    )\n",
    "    # grab the columns we need and rename\n",
    "    ddf = ddf[\n",
    "        [\n",
    "            \"tpep_pickup_datetime\",\n",
    "            \"tpep_dropoff_datetime\",\n",
    "            \"PULocationID\",\n",
    "            \"DOLocationID\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"payment_type\",\n",
    "            \"tip_amount\",\n",
    "            \"fare_amount\",\n",
    "        ]\n",
    "    ]\n",
    "    ddf.columns = [\n",
    "        \"pickup_datetime\",\n",
    "        \"dropoff_datetime\",\n",
    "        \"pickup_taxizone_id\",\n",
    "        \"dropoff_taxizone_id\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"payment_type\",\n",
    "        \"tip_amount\",\n",
    "        \"fare_amount\",\n",
    "    ]\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a listing of files from the public S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    f\"s3://{x}\"\n",
    "    for x in fs.glob(\"s3://nyc-tlc/trip data/yellow_tripdata_201*.csv\")\n",
    "    if \"2017\" in x or \"2018\" in x or \"2019\" in x\n",
    "]\n",
    "len(files), files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = read_taxi_csv(files[:5])  # only load first 5 months of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We are loading a small sample for this exercise, but if you want to use the full data and replicate the aggregated data hosted on Saturn's bucket, you will need to use a larger cluster. Here is a sample cluster configuration you can use, but you can play around with sizes and see how performance changes!\n",
    "\n",
    "```python\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=10, \n",
    "    scheduler_size='xlarge',\n",
    "    worker_size='8xlarge', \n",
    "    nthreads=32,\n",
    ")\n",
    "```\n",
    "\n",
    "You will have to run `cluster.reset(...)` if the cluster has already been configured. Run the following to see what sizes are available:\n",
    "\n",
    "```python\n",
    "from dask_saturn.core import describe_sizes\n",
    "describe_sizes()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all 3 years of data\n",
    "# ddf = read_taxi_csv(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated files for Dashboard\n",
    "\n",
    "Create several CSV file to use for visualization in the dashboard. Note that each of these perform some Dask dataframe operations, then call `compute()` to pull down a pandas dataframe, and then write that dataframe to a CSV file.\n",
    "\n",
    "## Augment data\n",
    "\n",
    "We'll distill some features out of the datetime component of the data. This is similar to the feature engineering that is done in other places in this demo, but we'll only create the features that'll be most useful in the visuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"pickup_hour\"] = ddf.pickup_datetime.dt.hour\n",
    "ddf[\"dropoff_hour\"] = ddf.dropoff_datetime.dt.hour\n",
    "ddf[\"pickup_weekday\"] = ddf.pickup_datetime.dt.weekday\n",
    "ddf[\"dropoff_weekday\"] = ddf.dropoff_datetime.dt.weekday\n",
    "ddf[\"percent_tip\"] = (ddf[\"tip_amount\"] / ddf[\"fare_amount\"]).replace(\n",
    "    [np.inf, -np.inf], np.nan\n",
    ") * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take out the extreme high values since they disrupt the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"percent_tip\"] = ddf[\"percent_tip\"].apply(lambda x: np.nan if x > 1000 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all of the above cells execute pretty much instantly. This is because of Dask's [lazy evaluation](https://tutorial.dask.org/01x_lazy.html). Calling `persist()` below tells Dask to run all the operations and keep the results in memory for faster computation. This cell takes some time to run because Dask needs to first parse all the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = ddf.persist()\n",
    "_ = wait(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries datasets\n",
    "\n",
    "We'll resample to an hourly timestep so that we don't have to pass around so much data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_ddf = ddf[[\"pickup_datetime\", \"percent_tip\"]].set_index(\"pickup_datetime\").dropna()\n",
    "tips = tip_ddf.resample(\"1H\").mean().compute()\n",
    "\n",
    "tips.to_csv(f\"{DATA_PATH}/pickup_average_percent_tip_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_ddf = ddf[[\"pickup_datetime\", \"fare_amount\"]].set_index(\"pickup_datetime\").dropna()\n",
    "fare = fare_ddf.resample(\"1H\").mean().compute()\n",
    "\n",
    "fare.to_csv(f\"{DATA_PATH}/pickup_average_fare_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate datasets\n",
    "\n",
    "Since our data is rather large and will mostly be viewed in grouped aggregates, we can do some aggregation now and save it off for use in plots later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in [\"pickup\", \"dropoff\"]:\n",
    "    data = (\n",
    "        ddf.groupby(\n",
    "            [\n",
    "                f\"{value}_taxizone_id\",\n",
    "                f\"{value}_hour\",\n",
    "                f\"{value}_weekday\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            {\n",
    "                \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "                \"trip_distance\": [\"mean\"],\n",
    "                \"percent_tip\": [\"mean\"],\n",
    "            }\n",
    "        )\n",
    "        .compute()\n",
    "    )\n",
    "    data.columns = data.columns.to_flat_index()\n",
    "    data = data.rename(\n",
    "        {\n",
    "            (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "            (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "            (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "            (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "            (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "        },\n",
    "        axis=1,\n",
    "    ).reset_index(level=[1, 2])\n",
    "    data.to_csv(f\"{DATA_PATH}/{value}_grouped_by_zone_and_time.csv\")\n",
    "\n",
    "grouped_zone_and_time = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in [\"pickup\", \"dropoff\"]:\n",
    "    data = (\n",
    "        ddf.groupby(\n",
    "            [\n",
    "                f\"{value}_taxizone_id\",\n",
    "            ]\n",
    "        )\n",
    "        .agg(\n",
    "            {\n",
    "                \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "                \"trip_distance\": [\"mean\"],\n",
    "                \"percent_tip\": [\"mean\"],\n",
    "            }\n",
    "        )\n",
    "        .compute()\n",
    "    )\n",
    "    data.columns = data.columns.to_flat_index()\n",
    "    data = data.rename(\n",
    "        {\n",
    "            (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "            (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "            (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "            (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "            (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    data.to_csv(f\"{DATA_PATH}/{value}_grouped_by_zone.csv\")\n",
    "\n",
    "grouped_zone = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"pickup\"\n",
    "data = (\n",
    "    ddf.groupby([f\"{value}_hour\", f\"{value}_weekday\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "            \"trip_distance\": [\"mean\"],\n",
    "            \"percent_tip\": [\"mean\"],\n",
    "        }\n",
    "    )\n",
    "    .compute()\n",
    ")\n",
    "data.columns = data.columns.to_flat_index()\n",
    "data = data.rename(\n",
    "    {\n",
    "        (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "        (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "        (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "        (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "        (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "data.to_csv(f\"{DATA_PATH}/{value}_grouped_by_time.csv\")\n",
    "grouped_time = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get shape files for dashboard\n",
    "\n",
    "The shape files are stored in a zip on the public S3. Here we pull it down, unzip it, then place the files on our S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with fs.open(\"s3://nyc-tlc/misc/taxi_zones.zip\") as f:\n",
    "    with zipfile.ZipFile(f) as zip_ref:\n",
    "        zip_ref.extractall(f\"{DATA_PATH}/taxi_zones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "To make use of the new datasets we can visualize all the data at once using a grouped heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zone_and_time.hvplot.heatmap(\n",
    "    x=\"dropoff_weekday\",\n",
    "    y=\"dropoff_hour\",\n",
    "    C=\"average_percent_tip\",\n",
    "    groupby=\"dropoff_taxizone_id\",\n",
    "    responsive=True,\n",
    "    min_height=600,\n",
    "    cmap=\"viridis\",\n",
    "    clim=(0, 20),\n",
    "    colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset that is only grouped by zone can be paired with other information such as geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "zones = gpd.read_file(f\"{DATA_PATH}/taxi_zones/taxi_zones.shp\").to_crs(\"epsg:4326\")\n",
    "joined = zones.join(grouped_zone, on=\"LocationID\")\n",
    "\n",
    "joined.hvplot(\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    c=\"average_fare\",\n",
    "    geo=True,\n",
    "    tiles=\"CartoLight\",\n",
    "    cmap=\"fire\",\n",
    "    alpha=0.5,\n",
    "    hover_cols=[\"zone\", \"borough\"],\n",
    "    title=\"Average fare by dropoff location\",\n",
    "    height=600,\n",
    "    width=800,\n",
    "    clim=(0, 100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to use Dask to extract, transform, and load data. You also learned how to use [`hvplot`](https://hvplot.holoviz.org/index.html) to create data visualizations on top of that data.\n",
    "\n",
    "Next, try [this dask-ml notebook](./hyperparameter-dask.ipynb) to see how to use this Dask DataFrame to accelerate common machine learning tasks like feature engineering and hyperparameter tuning.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
