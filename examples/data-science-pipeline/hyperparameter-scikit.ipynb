{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning (single-node with scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/scikit-learn.png\" width=\"300\">\n",
    "\n",
    "This notebook describes a machine learning training workflow using the famous [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). That dataset contains information on taxi trips in New York City.\n",
    "\n",
    "In this exercise, you'll load data into a `pandas` data frame and use `scikit-learn` to answer this question\n",
    "\n",
    "> based on characteristics that can be known at the beginning of a trip, what tip will this trip earn (as a % of the total fare)?\n",
    "\n",
    "**NOTE:** This notebook has some cells that can take 3-10 minutes to run. Consider opening [this dask-ml notebook](./hyperparameter-dask.ipynb) and running that while you're waiting for cells in this notebook to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Load data\n",
    "\n",
    "This example is designed to run quickly with small, relatively inexpensive resources. So let's just load a single month of taxi data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "taxi = pd.read_csv(\n",
    "    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-01.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    ").sample(frac=0.01, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below computes the size of this dataset in memory. One month is about 7.6 million rows and 1.5 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num rows: {len(taxi)}, Size: {taxi.memory_usage(deep=True).sum() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the structure of the data with `pandas` commands:\n",
    "\n",
    "`.head()` = view the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.dtypes` = list all the columns and the type of data in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Prep for Training\n",
    "\n",
    "Before training a model, we need to transform this dataset into a format that's better-suited to the research question. The function below does that with `pandas` operations.\n",
    "\n",
    "<details><summary>(click here to learn why data scientists do this)</summary>\n",
    "\n",
    "**Compute the Target**\n",
    "\n",
    "The raw data don't contain a column that cleanly describes the tip as a percentage of the total fare. So we need to add one!\n",
    "\n",
    "**Add Features**\n",
    "\n",
    "Giving a machine learning model a richer description of each training observation improves its ability to describe the relationship between those observations' characteristics and the target. These characteristics are called \"features\".\n",
    "\n",
    "For example, instead of giving a model a raw timestamp, it can be valuable to provide multiple derived characteristics like hour of the day and day of the week. It's plausible, for example, that weekend rides might have a different distribution of tips because they tend to be for leisure, where weekday rides might be mostly people travelling for work.\n",
    "\n",
    "**Remove Unused Features**\n",
    "\n",
    "If the goal is to produce a model that could predict the tip for a ride, then characteristics that can only be known AFTER the tip have to be excluded. For example, you can't know the dropoff time or the type of payment until a ride has concluded.\n",
    "\n",
    "Such features should be dropped before training.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekofyear\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_week_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "categorical_feat = [\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "target_col = \"tip_fraction\"\n",
    "\n",
    "\n",
    "def prep_df(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare a raw taxi dataframe for training.\n",
    "        * computes the target ('tip_fraction')\n",
    "        * adds features\n",
    "        * removes unused features\n",
    "    \"\"\"\n",
    "    df = df[df.fare_amount > 0]  # avoid divide-by-zero\n",
    "    df[target_col] = df.tip_amount / df.fare_amount\n",
    "\n",
    "    df[\"pickup_weekday\"] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df[\"pickup_weekofyear\"] = df.tpep_pickup_datetime.dt.isocalendar().week\n",
    "    df[\"pickup_hour\"] = df.tpep_pickup_datetime.dt.hour\n",
    "    df[\"pickup_week_hour\"] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df[\"pickup_minute\"] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [target_col]].astype(float).fillna(-1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to get a new data frame, `taxi_train`, that can be used directly for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"tip_fraction\"\n",
    "taxi_train = prep_df(taxi, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`taxi_train` is a `pandas` dataframe that will be passed in to a machine learning model. Before going further, check the first few rows of the dataset to make sure that the features look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe has been processed, check its size in memory again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Num rows: {len(taxi_train)}, Size: {round(taxi_train.memory_usage(deep=True).sum() / 1e9, 2)} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, removing unused columns dropped the size of the training data to 0.55 GB, about one third the size of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Run grid search\n",
    "\n",
    "Now that you've loaded and preprocessed your training data, use the code below to find the best set of hyperparameters for your mode. This is done with `scikit-learn`'s `GridSearchCV`.\n",
    "\n",
    "Setting `n_jobs=-1` tells scikit-learn to use all available cores on this machine to train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"preprocess\",\n",
    "            ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", StandardScaler(), numeric_feat),\n",
    "                    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False), categorical_feat),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", ElasticNet(normalize=False, max_iter=100)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"clf__l1_ratio\": np.arange(0, 1.1, 0.1),\n",
    "    \"clf__alpha\": [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, params, cv=3, n_jobs=-1, verbose=1, scoring=\"neg_mean_squared_error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the grid search set up, it's time to train some models!\n",
    "\n",
    "**NOTE:** This will take a few minutes to run. Consider opening [this dask-ml notebook](./hyperparameter-dask.ipynb) and running that while you're waiting for this model to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(taxi_train[features], taxi_train[target_col])\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Save model\n",
    "\n",
    "`GridSearchCV` automatically fits the best parameters to the full data and stores in `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "with open(f\"{MODEL_PATH}/elastic_net_scikit.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Calculate metrics on test set\n",
    "\n",
    "Machine learning training tries to create a model which can produce useful results on new data that it didn't see during training. To test how well we've accomplished that in this example, read in another month of taxi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = pd.read_csv(\n",
    "    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-02.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    ").sample(frac=0.01, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating predictions on this new dataset, it has to be transformed in exactly the way that the original training data were prepared. Thankfully you've already wrapped that transformation logic in a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = prep_df(taxi_test, target_col=target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` comes with many functions for calculating metrics that describe how well a model's predictions match the actual values. For a complete list, see [\"Metrics and scoring\"](https://scikit-learn.org/stable/modules/model_evaluation.html) in the `sciki-learn` docs.\n",
    "\n",
    "This tutorial uses the `mean_squared_error` to evaluate the model. This metric penalizes large errors more than small errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = grid_search.predict(taxi_test[features])\n",
    "mean_squared_error(taxi_test[target_col], preds, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to use `pandas` for feature engineering and `scikit-learn` for hyperparameter optimization.\n",
    "\n",
    "Next, try [this dask-ml + Dask notebook](./xgboost-dask.ipynb) to learn how to use Dask to train larger models and / or reduce training time.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
