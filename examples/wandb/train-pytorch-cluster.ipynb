{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Weights & Biases\n",
    "### Train Image Classifier with PyTorch on GPU Dask Cluster\n",
    "\n",
    "In this project, we use the Stanford Dogs dataset, and starting with a pre-trained version of Resnet50, we will use transfer learning to make it perform better at dog image identification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:16.959792Z",
     "iopub.status.busy": "2021-05-05T17:13:16.959514Z",
     "iopub.status.idle": "2021-05-05T17:13:17.906794Z",
     "shell.execute_reply": "2021-05-05T17:13:17.906178Z",
     "shell.execute_reply.started": "2021-05-05T17:13:16.959720Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import json\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from dask_pytorch_ddp import data, dispatch\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Weights & Biases\n",
    "\n",
    "Import the Weights & Biases library, and confirm that you are logged in. \n",
    "\n",
    ">The Start Script in this project uses your Weights & Biases token to log in. Make sure that this is correctly saved in the Credentials section, and named `WANDB_LOGIN`, if you have any trouble. This is important because all the workers in your cluster need to have this token. This credential needs to be set up before the cluster is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:17.907928Z",
     "iopub.status.busy": "2021-05-05T17:13:17.907755Z",
     "iopub.status.idle": "2021-05-05T17:13:18.542268Z",
     "shell.execute_reply": "2021-05-05T17:13:18.541509Z",
     "shell.execute_reply.started": "2021-05-05T17:13:17.907907Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Specific Elements\n",
    "\n",
    "Because this task uses a Dask cluster, we need to load a few extra libraries, and ensure our cluster is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:18.544523Z",
     "iopub.status.busy": "2021-05-05T17:13:18.544277Z",
     "iopub.status.idle": "2021-05-05T17:13:18.567371Z",
     "shell.execute_reply": "2021-05-05T17:13:18.566835Z",
     "shell.execute_reply.started": "2021-05-05T17:13:18.544491Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:18.568730Z",
     "iopub.status.busy": "2021-05-05T17:13:18.568488Z",
     "iopub.status.idle": "2021-05-05T17:13:18.892098Z",
     "shell.execute_reply": "2021-05-05T17:13:18.891500Z",
     "shell.execute_reply.started": "2021-05-05T17:13:18.568698Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(2)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Formatting \n",
    "These utilities ensure the training data labels correspond to the pretrained model's label expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:18.893601Z",
     "iopub.status.busy": "2021-05-05T17:13:18.893343Z",
     "iopub.status.idle": "2021-05-05T17:13:19.224088Z",
     "shell.execute_reply": "2021-05-05T17:13:19.223471Z",
     "shell.execute_reply.started": "2021-05-05T17:13:18.893566Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import s3fs\n",
    "\n",
    "##### Load label dataset\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "with s3.open(\"s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt\") as f:\n",
    "    imagenetclasses = [line.strip() for line in f.readlines()]\n",
    "\n",
    "##### Format labels to match pretrained Resnet\n",
    "def replace_label(dataset_label, model_labels):\n",
    "    label_string = re.search(\"n[0-9]+-([^/]+)\", dataset_label).group(1)\n",
    "\n",
    "    for i in model_labels:\n",
    "        i = str(i).replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "        model_label_str = re.search(\"\"\"b[\"'][0-9]+: [\"']([^\\/]+)[\"'],[\"']\"\"\", str(i))\n",
    "        model_label_idx = re.search(\"\"\"b[\"']([0-9]+):\"\"\", str(i)).group(1)\n",
    "\n",
    "        if re.search(str(label_string).replace(\"_\", \" \"), str(model_label_str).replace(\"_\", \" \")):\n",
    "            return i, model_label_idx\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model Specifications\n",
    "\n",
    "Here you can assign your model hyperparameters, as well as identifying where the training data is housed on S3. All these parameters, as well as some extra elements like Notes and Tags, are tracked by Weights & Biases for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:19.225295Z",
     "iopub.status.busy": "2021-05-05T17:13:19.225044Z",
     "iopub.status.idle": "2021-05-05T17:13:19.229241Z",
     "shell.execute_reply": "2021-05-05T17:13:19.228539Z",
     "shell.execute_reply.started": "2021-05-05T17:13:19.225262Z"
    }
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_epochs\": 6,\n",
    "    \"batch_size\": 64,\n",
    "    \"base_lr\": 0.0003,\n",
    "    \"downsample_to\": 0.5,  # Value represents percent of training data you want to use\n",
    "    \"bucket\": \"saturn-public-data\",\n",
    "    \"prefix\": \"dogs/Images\",\n",
    "    \"pretrained_classes\": imagenetclasses,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:19.230548Z",
     "iopub.status.busy": "2021-05-05T17:13:19.230308Z",
     "iopub.status.idle": "2021-05-05T17:13:19.234509Z",
     "shell.execute_reply": "2021-05-05T17:13:19.233890Z",
     "shell.execute_reply.started": "2021-05-05T17:13:19.230517Z"
    }
   },
   "outputs": [],
   "source": [
    "wbargs = {\n",
    "    **model_params,\n",
    "    \"classes\": 120,\n",
    "    \"Notes\": \"baseline\",\n",
    "    \"Tags\": [\"downsample\", \"cluster\", \"gpu\", \"6wk\", \"subsample\"],\n",
    "    \"Group\": \"DDP\",\n",
    "    \"dataset\": \"StanfordDogs\",\n",
    "    \"architecture\": \"ResNet\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "This function encompasses the training task. \n",
    "* Load model and wrap it in PyTorch's Distributed Data Parallel function\n",
    "* Initialize Weights & Biases run\n",
    "* Set up DataLoader to iterate over training data\n",
    "* Perform training tasks\n",
    "* Write model performance data to Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:19.236343Z",
     "iopub.status.busy": "2021-05-05T17:13:19.236098Z",
     "iopub.status.idle": "2021-05-05T17:13:19.252291Z",
     "shell.execute_reply": "2021-05-05T17:13:19.251603Z",
     "shell.execute_reply.started": "2021-05-05T17:13:19.236312Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_train_cluster(\n",
    "    bucket, prefix, batch_size, downsample_to, n_epochs, base_lr, pretrained_classes\n",
    "):\n",
    "    #     os.environ[\"DASK_DISTRIBUTED__WORKER__DAEMON\"] = \"False\"\n",
    "    os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "\n",
    "    worker_rank = int(dist.get_rank())\n",
    "\n",
    "    # --------- Format params --------- #\n",
    "    device = torch.device(\"cuda\")\n",
    "    net = models.resnet50(pretrained=True)  # True means we start with the imagenet version\n",
    "    model = net.to(device)\n",
    "    model = DDP(model)\n",
    "\n",
    "    # --------- Start wandb --------- #\n",
    "    if worker_rank == 0:\n",
    "        wandb.init(config=wbargs, entity=\"wandb\", project=\"wandb_saturn_demo\")\n",
    "        wandb.watch(model)\n",
    "\n",
    "    # --------- Set up eval --------- #\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=base_lr, eps=1e-06)\n",
    "\n",
    "    # --------- Retrieve data for training --------- #\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize(256), transforms.CenterCrop(250), transforms.ToTensor()]\n",
    "    )\n",
    "\n",
    "    # Because we want to load our images directly and lazily from S3,\n",
    "    # we use a custom Dataset class called S3ImageFolder.\n",
    "    whole_dataset = data.S3ImageFolder(bucket, prefix, transform=transform, anon=True)\n",
    "\n",
    "    # Format target labels\n",
    "    new_class_to_idx = {\n",
    "        x: int(replace_label(x, pretrained_classes)[1]) for x in whole_dataset.classes\n",
    "    }\n",
    "    whole_dataset.class_to_idx = new_class_to_idx\n",
    "\n",
    "    # ------ Create dataloader ------- #\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        whole_dataset,\n",
    "        sampler=RandomSampler(\n",
    "            whole_dataset,\n",
    "            replacement=True,\n",
    "            num_samples=math.floor(len(whole_dataset) * downsample_to),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # Using the OneCycleLR learning rate schedule\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=base_lr, steps_per_epoch=len(train_loader), epochs=n_epochs\n",
    "    )\n",
    "\n",
    "    # ------ Prepare wandb Table for predictions ------- #\n",
    "    if worker_rank == 0:\n",
    "        columns = [\"image\", \"label\", \"prediction\", \"score\"]\n",
    "        preds_table = wandb.Table(columns=columns)\n",
    "\n",
    "    # --------- Start Training ------- #\n",
    "    for epoch in range(n_epochs):\n",
    "        count = 0\n",
    "        model.train()\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            dt = datetime.datetime.now().isoformat()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Run model iteration\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Format results\n",
    "            pred_idx, preds = torch.max(outputs, 1)\n",
    "            perct = [\n",
    "                torch.nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)\n",
    "            ]\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = (preds == labels).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # ✍️ Log your metrics to wandb ✍️\n",
    "            if worker_rank == 0:\n",
    "                logs = {\n",
    "                    \"train/train_loss\": loss.item(),\n",
    "                    \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                    \"train/correct\": correct,\n",
    "                    \"train/epoch\": epoch + count / len(train_loader),\n",
    "                    \"train/count\": count,\n",
    "                }\n",
    "\n",
    "                # ✍️  Occasionally some images to ensure the image data looks correct ✍️\n",
    "                if count % 25 == 0:\n",
    "                    logs[\"examples/example_images\"] = wandb.Image(\n",
    "                        inputs[:5], caption=f\"Step: {count}\"\n",
    "                    )\n",
    "\n",
    "                # ✍️ Log some predictions to wandb during final epoch for analysis✍️\n",
    "                if epoch == max(range(n_epochs)) and count % 4 == 0:\n",
    "                    for i in range(len(labels)):\n",
    "                        preds_table.add_data(wandb.Image(inputs[i]), labels[i], preds[i], perct[i])\n",
    "\n",
    "                # ✍️  Log metrics to wandb ✍️\n",
    "                wandb.log(logs)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    # ✍️  Upload your predictions table for analysis ✍️\n",
    "    if worker_rank == 0:\n",
    "        predictions_artifact = wandb.Artifact(\n",
    "            \"train_predictions_\" + str(wandb.run.id), type=\"train_predictions\"\n",
    "        )\n",
    "        predictions_artifact.add(preds_table, \"train_predictions\")\n",
    "        wandb.run.log_artifact(predictions_artifact)\n",
    "\n",
    "        # ✍️ Close your wandb run ✍️\n",
    "        wandb.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model\n",
    "\n",
    "To run the model, we use the `dask-pytorch-ddp` function `dispatch.run()`. This takes our client, our training function, and our dictionary of model parameters. You can monitor the model run on all workers using the Dask dashboard, or monitor the performance of Worker 0 on Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-05T17:13:19.926271Z",
     "iopub.status.busy": "2021-05-05T17:13:19.926030Z"
    }
   },
   "outputs": [],
   "source": [
    "client.restart()  # Clears memory on cluster- optional but recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "futures = dispatch.run(client, simple_train_cluster, **model_params)\n",
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one or more worker jobs errors, this will describe the issue\n",
    "futures[0].result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can view the Weights & Biases dashboard to see the performance of the model and system resources utilization in real time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
