{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3291081f-8907-42ea-b36f-e31e2e5b4e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-25T00:16:37.275305Z",
     "iopub.status.busy": "2022-01-25T00:16:37.274696Z",
     "iopub.status.idle": "2022-01-25T00:16:39.843629Z",
     "shell.execute_reply": "2022-01-25T00:16:39.842758Z",
     "shell.execute_reply.started": "2022-01-25T00:16:37.275226Z"
    }
   },
   "source": [
    "# Multi GPU Training in TensorFlow\n",
    "\n",
    "![TensorFlow Logo](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/tensorflow.png \"doc-image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2388dd59-540d-4032-801c-59e34eb1a83f",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This example is an extension of the example of using [Single GPU Tensorflow](./01-tensorflow-single-gpu.ipynb) to train a Resnet50 architecture on Birds dataset to identify different species of birds, only here we will be using multiple GPUs.  We will parallelize the learning by using TensorFlow Mirrored Strategy. The model will split the data in each batch (sometimes called “global batch”) across the GPUs (making “worker batches”). Each GPU has a copy of the model, called a “replica”, and while they learn on different parts of each batch, they will combine the learned gradients at the end of the step. They stay synchronized this way, and the result at the end of training is one model that has learned on all the data.\n",
    "\n",
    "This dataset constitutes 40,000+ birds and has been taken from [kaggle](https://www.kaggle.com/gpiosenka/100-bird-species).\n",
    "\n",
    "We recommend to spin up a bigger EC2 instance now, with multiple GPUs, so we can distribute the training work across these and have them working simultaneously, all training the same model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e847b-0887-4159-8a06-6fcaef016c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87f3bb-3b2f-497f-bdb5-fe1977949030",
   "metadata": {},
   "source": [
    "The dataset originally had 285 classes. We have taken subset of this data which has 61 classes . The data is stored in AWS S3.The first time you run this job, you'll need to download the training and test data in the code chunk above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679b4bb-2f79-4327-bb18-d447c9fc4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "_ = s3.get(\n",
    "    rpath=\"s3://saturn-public-data/100-bird-species/100-bird-species/*/*/*.jpg\",\n",
    "    lpath=\"dataset/birds/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc316c-afca-498c-a1e0-a6859a7fbe59",
   "metadata": {},
   "source": [
    "Our datasat has already neatly separated out training, test, and validation samples. In code below we are constructing Keras data object for training and validation set using keras.preprocessing.image_dataset_from_directory . We have chosen Adam optimizer and have set learning rate to 0.02. We are training our classifier with ResNet50 architecture, which has 48 Convolution layers along with 1 MaxPool and 1 Average Pool layer. It’s learning from each image, updating its model gradients, and gradually improving its performance as it goes. The model is being compiled,trained and saved at path 'model/keras_multi/'. This is actually incredibly similar to our single GPU approach. We’re just applying the Mirrored Strategy scope around our model definition and compilation stage, so that TensorFlow knows this model is to be trained on multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16caad50-4916-4d98-999e-e5bfb8a7cedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_multigpu(n_epochs, classes, base_lr, batchsize,scale_batch = False, scale_lr = False):\n",
    "    \n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('Number of devices: %d' % strategy.num_replicas_in_sync)\n",
    "\n",
    "    \n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = tf.keras.applications.ResNet50(\n",
    "            include_top=True,\n",
    "            weights=None,\n",
    "            classes=classes)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=base_lr)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # Data\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        'dataset/birds/train',\n",
    "        image_size=(224,224),\n",
    "        batch_size=batchsize\n",
    "    ).prefetch(2).cache().shuffle(1000)\n",
    "    \n",
    "    # printing sample birds images\n",
    "    for birds, labels in train_ds.take(1):\n",
    "        plt.figure(figsize=(18, 18))\n",
    "        for i in range(9):\n",
    "            plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(birds[i].numpy().astype(\"uint8\"))\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        'dataset/birds/valid',\n",
    "        image_size=(224,224),\n",
    "        batch_size=batchsize\n",
    "    ).prefetch(2)\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    model.fit(\n",
    "        train_ds, \n",
    "        epochs=n_epochs, \n",
    "        validation_data=valid_ds,\n",
    "    )\n",
    "\n",
    "    end = time.time()-start\n",
    "    print(\"model training time\", end)\n",
    "    \n",
    "    tf.keras.models.save_model(model, 'model/keras_multi/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c9a2d-b590-44da-bc39-f2a137143556",
   "metadata": {},
   "source": [
    "In code below we are setting up necessary parameters . We are only running a few epochs, to save time, but once you've got this working you'll have all the information you need to build and run bigger Tensorflow models on Saturn Cloud. Make sure that you take full advantage of multi GPUs processing power by increasing batch size so that GPUs can be kept busy (but without overrunning our RAM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39284f50-2c73-4714-8cc3-1323d30454f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = {'n_epochs': 3, \n",
    "                'base_lr': .02,\n",
    "               'batchsize': 64,\n",
    "                   'classes':61,\n",
    "               'scale_batch': True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03d4e1-532b-4ae9-a7fc-9ec7666b29c4",
   "metadata": {},
   "source": [
    "The code below runs the model training process, and saves your trained model object to the Jupyter instance memory. A folder called `model` will be created and populated for you. Also on running the training function you can see below some beautiful bird's of various species!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce956fe9-5f17-44a5-afac-cf8d4e18a376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tester_plain = train_multigpu( **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69643b3-ccce-401a-bf5d-ebcefc07089a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
