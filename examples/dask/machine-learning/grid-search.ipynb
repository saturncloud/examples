{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63589aa1-0c45-4af5-a7f8-f309942a72eb",
   "metadata": {},
   "source": [
    "# Grid Search with Scikit-learn and Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5963136-4ecf-4bb6-b2a5-ffecb431f5c3",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a crucial, and often painful, part of building machine learning models. Squeezing out each bit of performance from your model may mean the difference of millions of dollars in ad revenue or life-and-death for patients in healthcare models. Even if your model takes one minute to train, you can end up waiting hours for a grid search to complete (think a 10Ã—10 grid, cross-validation, etc.). Each time you wait for a search to finish breaks an iteration cycle and increases the time it takes to produce value with your model.\n",
    "\n",
    "This article assumes that you already have a working pipeline with single-node Python packages such as pandas, NumPy, and scikit-learn. This guide will help you take this code and speed it up! There are two different scenarios that can arise when doing hyperparameter searching:\n",
    "1. The training data is small, and the parameter space is small - You do not need Dask, use scikit-learn.\n",
    "2. The training data is small, and the parameter space is **large** - Train the data with pandas/NumPy/scikit-learn, and use [joblib and Dask](https://ml.dask.org/joblib.html) for distributed parameter testing.\n",
    "3. The training data is **large** - Use the [Dask-ML package](https://ml.dask.org/), with classes that look and feel like scikit-learn, to spread operations across your Dask cluster.\n",
    "\n",
    "We'll illustrate the two Dask methods of scaling grid search using the famous [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This dataset contains information on taxi trips in New York City.\n",
    "\n",
    "First, start the Dask cluster associated with your Saturn Cloud resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a18bd2-3efc-43f6-8e16-01ef79148914",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T23:01:59.929088Z",
     "iopub.status.busy": "2022-01-14T23:01:59.928832Z",
     "iopub.status.idle": "2022-01-14T23:02:00.688952Z",
     "shell.execute_reply": "2022-01-14T23:02:00.688425Z",
     "shell.execute_reply.started": "2022-01-14T23:01:59.929008Z"
    }
   },
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(SaturnCluster())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03971e",
   "metadata": {},
   "source": [
    "After running the above command, it's recommend that you check on the Saturn Cloud resource page that the Dask cluster as fully online before continuing. Alternatively, you can use the command `client.wait_for_workers(3)` to halt the notebook execution until all three of the workers are ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e8ad4-e1bb-4318-b70c-81fe9ec458d8",
   "metadata": {},
   "source": [
    "## Joblib for small data and large parameter spaces\n",
    "In this case, the training data and pipeline code remains in pandas/NumPy/scikit-learn. Scikit-learn has algorithms that support parallel execution via the `n_jobs` parameter, and `GridSearchCV` is one of them. By default, this parallelizes across all cores on a single machine using the [Joblib](https://joblib.readthedocs.io/en/latest/) library. Dask provides a Joblib backend that hooks into these scikit-learn algorithms to parallelize work across a Dask cluster. This enables us to pull in Dask just for the grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e072615-04d8-4a57-9324-56aa891cdca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T23:02:00.690108Z",
     "iopub.status.busy": "2022-01-14T23:02:00.689900Z",
     "iopub.status.idle": "2022-01-14T23:02:01.060370Z",
     "shell.execute_reply": "2022-01-14T23:02:01.059842Z",
     "shell.execute_reply.started": "2022-01-14T23:02:00.690080Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c68ee0-93ce-4d80-a1be-decfa589f5f7",
   "metadata": {},
   "source": [
    "The data is loaded into a pandas DataFrame from S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d1cdd-03a8-498e-90d3-a7e1bddf2f11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T23:02:01.062338Z",
     "iopub.status.busy": "2022-01-14T23:02:01.062056Z",
     "iopub.status.idle": "2022-01-14T23:02:03.949597Z",
     "shell.execute_reply": "2022-01-14T23:02:03.948926Z",
     "shell.execute_reply.started": "2022-01-14T23:02:01.062308Z"
    }
   },
   "outputs": [],
   "source": [
    "taxi = pd.read_csv(\n",
    "    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-05.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a2774-5d4e-4522-bf6c-6f6ac67eb8b6",
   "metadata": {},
   "source": [
    "The next chunk of code defines the features and cleans the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0b5cb-6659-4cb9-8451-1b372d2f1612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T23:02:03.957131Z",
     "iopub.status.busy": "2022-01-14T23:02:03.956916Z",
     "iopub.status.idle": "2022-01-14T23:02:04.241980Z",
     "shell.execute_reply": "2022-01-14T23:02:04.241309Z",
     "shell.execute_reply.started": "2022-01-14T23:02:03.957102Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_features = [\n",
    "    \"tpep_pickup_datetime\",\n",
    "    \"passenger_count\",\n",
    "    \"tip_amount\",\n",
    "    \"fare_amount\",\n",
    "]\n",
    "features = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekofyear\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_week_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "label = \"tip_fraction\"\n",
    "\n",
    "\n",
    "def prep_df(taxi_df):\n",
    "    \"\"\"\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    \"\"\"\n",
    "    df = taxi_df[taxi_df.fare_amount > 0][raw_features].copy()  # avoid divide-by-zero\n",
    "    df[label] = df.tip_amount / df.fare_amount\n",
    "\n",
    "    df[\"pickup_weekday\"] = df.tpep_pickup_datetime.dt.isocalendar().day\n",
    "    df[\"pickup_weekofyear\"] = df.tpep_pickup_datetime.dt.isocalendar().week\n",
    "    df[\"pickup_hour\"] = df.tpep_pickup_datetime.dt.hour\n",
    "    df[\"pickup_week_hour\"] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df[\"pickup_minute\"] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [label]].astype(float).fillna(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "taxi_feat = prep_df(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84701bf-5d14-479b-a133-6721934138a2",
   "metadata": {},
   "source": [
    "The pipeline needs to be define for how to do the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31261317-2f49-485a-aeef-8e92dd72b0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T23:02:04.243799Z",
     "iopub.status.busy": "2022-01-14T23:02:04.243195Z",
     "iopub.status.idle": "2022-01-14T23:02:04.249180Z",
     "shell.execute_reply": "2022-01-14T23:02:04.248634Z",
     "shell.execute_reply.started": "2022-01-14T23:02:04.243766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"clf__l1_ratio\": np.arange(0, 1.1, 0.1),\n",
    "    \"clf__alpha\": [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    params,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a3466-c904-49fe-a4ae-4c5865007205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T17:59:55.149778Z",
     "iopub.status.busy": "2022-01-14T17:59:55.149479Z",
     "iopub.status.idle": "2022-01-14T17:59:55.157014Z",
     "shell.execute_reply": "2022-01-14T17:59:55.155687Z",
     "shell.execute_reply.started": "2022-01-14T17:59:55.149747Z"
    }
   },
   "source": [
    "To execute the grid search in Dask we need to run inside a context manager for a Joblib backend. Besides that, we call the `grid_search.fit()` method the same way as you would when using scikit-learn in a non-distributed environment. When you run this cell, watch the Dask Dashboard to see the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97799286-3c31-4223-ae87-b6d866a5a487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-14T23:02:04.250841Z",
     "iopub.status.busy": "2022-01-14T23:02:04.250447Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    _ = grid_search.fit(\n",
    "        taxi_feat[features],\n",
    "        taxi_feat[label],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce2940-f4c5-4aee-8f61-272cb63d16fa",
   "metadata": {},
   "source": [
    "Note that using the Dask Joblib backend requires sending the DataFrame through the scheduler to all the workers, so make sure your scheduler has enough RAM to hold your dataset.\n",
    "\n",
    "\n",
    "## Dask-ML for large data and/or large parameter spaces\n",
    "\n",
    "This version accelerates the grid search by using Dask DataFrames and Dask-ML's `GridSearchCV` class. [Dask-ML](https://ml.dask.org/) is a package in the Dask ecosystem that has its own parallel implementations of machine learning algorithms, written in a familiar scikit-learn-like API. This includes `GridSearchCV` and other hyperparameter search options. To use it, we load our data into a Dask DataFrame and use Dask MLâ€™s preprocessing and model selection classes.\n",
    "\n",
    "We begin by importing the required libraries. When setting up the training grid notice that we are still using scikit-learn's `ElasticNet` class in this case, but now we are using `dask_ml` versions of some libraries. This will use Dask to do the pre-processing and grid search work, but use scikit-learn for the model fitting. This means that within a given Dask worker, the processed training dataset will be pulled down to a pandas DataFrame. In most cases, this is probably okay because the data will be small after processing. If the data is still too large, you can use one of Dask-ML's estimators, such as [LinearRegression](https://ml.dask.org/glm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e360a-818e-4073-a246-d35e6f2b03ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b937e1",
   "metadata": {},
   "source": [
    "This time the data is read into a Dask DataFrame instead of a pandas one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68a32b-f7c6-449c-a69e-756f40977f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "taxi_dd = dd.read_csv(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2020-05.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    storage_options={\"anon\": True},\n",
    "    assume_missing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca3c4f-d977-4354-a038-7632cb8d8251",
   "metadata": {},
   "source": [
    "The data is cleaned similar to before only now it's a Dask DataFrame being cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce1f98-60c9-492e-8587-baf9170b98f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_df(taxi_df):\n",
    "    \"\"\"\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    \"\"\"\n",
    "    df = taxi_df[taxi_df.fare_amount > 0][raw_features].copy()  # avoid divide-by-zero\n",
    "    df[label] = df.tip_amount / df.fare_amount\n",
    "\n",
    "    df[\"pickup_weekday\"] = df.tpep_pickup_datetime.dt.isocalendar().day\n",
    "    df[\"pickup_weekofyear\"] = df.tpep_pickup_datetime.dt.isocalendar().week\n",
    "    df[\"pickup_hour\"] = df.tpep_pickup_datetime.dt.hour\n",
    "    df[\"pickup_week_hour\"] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df[\"pickup_minute\"] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [label]].astype(float).fillna(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "taxi_feat_dd = prep_df(taxi_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d1b7af-7c5e-43f6-a3f1-3f192fbffe34",
   "metadata": {},
   "source": [
    "Again the pipeline is created in a similar manner as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e7c4a-74bb-4201-92c8-cc5ce5a7af04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", ElasticNet(normalize=False, max_iter=100, l1_ratio=0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"clf__l1_ratio\": np.arange(0, 1.1, 0.1),\n",
    "    \"clf__alpha\": [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    params,\n",
    "    cv=3,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb30fdf-b17c-4311-883b-13c51776d73a",
   "metadata": {},
   "source": [
    "Now we can run the grid search using the `grid_search` object defined above. It works the same way as scikit-learnâ€™s `GridSearchCV` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c56c7-1c68-470d-b239-de982f67d010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = grid_search.fit(\n",
    "    taxi_feat_dd[features],\n",
    "    taxi_feat_dd[label],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65565571-9773-4ac8-ba1c-d7b7f45e9fa8",
   "metadata": {},
   "source": [
    "These are a couple of ways to speed up grid search for machine learning with Dask. Check out our [other examples](https://saturncloud.io/docs/examples/python/) for more ways to use Python on Saturn Cloud!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
