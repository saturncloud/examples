{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c42dbf-eea7-4a7e-b685-2c458cf9c9f5",
   "metadata": {},
   "source": [
    "# Machine Learning with Dask : LightGBM, XGBoost, scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1969c-40ae-4a86-bf44-a481053096b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-13T18:11:02.889899Z",
     "iopub.status.busy": "2022-01-13T18:11:02.888500Z",
     "iopub.status.idle": "2022-01-13T18:11:02.899947Z",
     "shell.execute_reply": "2022-01-13T18:11:02.898875Z",
     "shell.execute_reply.started": "2022-01-13T18:11:02.889814Z"
    }
   },
   "source": [
    "Dask integrates very nicely with existing machine learning libraries like LightGBM, XGBoost. Distributing tasks across multiple cores and multiple machines, helps in scaling training. But before we look into training of machine learning models using the power of Dask, let us load and prepare our data. First, start the Dask cluster associated with your Saturn Cloud resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169f16f-c6a3-4b02-89b8-b0ee5775af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03993178-634f-478a-98f4-a604ad04eb0d",
   "metadata": {},
   "source": [
    "Now load your data into a Dask Dataframe. Here we are loading data for csv file located at s3 storage. Using read_csv from Dask takes the same form as using that function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7bb5d6-6177-41a2-99e3-6e882ad93aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"s3://saturn-public-data/examples/Dask/revised_house\", storage_options={\"anon\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b4df13-47d0-485a-b6bf-9b33bfe45d79",
   "metadata": {},
   "source": [
    "We will now assign predictors to X and target feature to y. Dask has a specific module called `dask_ml` that replicates the features of scikit-learn accelerated with parallelization. We will use that feature and split our data to train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a4c77-3356-4702-a10f-770c084dcdd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "y = df[\"SalePrice\"]\n",
    "X = df[[\"YearBuilt\", \"BedroomAbvGr\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=True, random_state=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb20165-53e5-4477-b66b-a8aaac104bbd",
   "metadata": {},
   "source": [
    "## LightGBM Training with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacda41a-c440-4a9f-90c5-2f0ecc68f4d3",
   "metadata": {},
   "source": [
    "LightGBM is a popular algorithm for supervised learning with tabular data. It has been used in many winning solutions in data science competitions. In LightGBM, like in other gradient-boosted decision tree algorithms, trees are built one node at a time. When building a tree, a set of possible “split points” (tuples of (feature, threshold)) is generated, and then each split point is evaluated. The one that leads to the best reduction in loss is chosen and added to the tree. Nodes are added to the tree this way until tree-specific stopping conditions are met. \n",
    "\n",
    "LightGBM’s distributed training comes in two flavors: “data parallel” and “feature parallel”. For full details, see [“Optimization in Parallel Learning” (LightGBM docs)](https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-parallel-learning).\n",
    "\n",
    "The `DaskLGBMRegressor` class from `lightgbm` accepts any parameters that can be passed to `lightgbm.LGBRegressor`. Let’s use the default parameters for this example. `lightgbm.dask` model objects also come with a `predict()` method that can be used to create predictions on a Dask Array or Dask DataFrame.\n",
    "\n",
    "Run the code below to create a validation set and test how well the model we trained in previous steps performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519198bb-0792-4946-89ad-3d1dec2c3024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "dask_model = lgb.DaskLGBMRegressor()\n",
    "dask_model.fit(X_train, y_train)\n",
    "preds = dask_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da271a9-7c9a-42bd-8129-ade10b4d8703",
   "metadata": {},
   "source": [
    "## XGBoost Training with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe517db-3f61-408f-aeb7-b81d5e4cb18c",
   "metadata": {},
   "source": [
    "XGBoost is a popular algorithm for supervised learning with tabular data. It has been used in many winning solutions in data science competitions, and in [real-world solutions at large enterprises like Capital One.](https://www.capitalone.com/tech/machine-learning/how-to-control-your-xgboost-model/)\n",
    "\n",
    "The XGBoost Python package allows for efficient single-machine training using multithreading. However, the amount of training data you can use is limited by the size of that one machine. To solve this problem, XGBoost supports distributed training using several different interfaces. Let us see how distributed training works for XGBoost using Dask.\n",
    "\n",
    "We will use the test and training set we had created before. `XGBoost` allows you to train on Dask collections like Dask DataFrames and Dask Arrays. This is really powerful because it means that you never have to have a single machine that’s big enough for all of your training data. For more details on this see the [XGBoost docs](https://xgboost.readthedocs.io/en/stable/tutorials/dask.html).\n",
    "\n",
    "Training data for `xgboost.dask` needs to be prepared in a special object called `DaskDMatrix`. This is like the XGBoost DMatrix that you might be familiar with, but is backed by Dask’s distributed collections (Dask DataFrame and Dask Array). The `train()` function from `xgboost` accepts any parameters that can be passed to `xgboost.train()`, with one exception: `nthread`. `xgboost.dask.predict()` can be used to create predictions on a Dask collection using an XGBoost model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c680663-c1c0-44cb-9f11-37bf23bb180e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.dask.DaskDMatrix(client=client, data=X_train, label=y_train)\n",
    "result = xgb.dask.train(\n",
    "    client=client,\n",
    "    params={\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 5,\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=50,\n",
    ")\n",
    "y_pred = xgb.dask.predict(client, result, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafe3ba-0385-4c6b-b273-e58b0dd26c28",
   "metadata": {},
   "source": [
    "## Train a Model with scikit-learn and Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73788b6e-7583-4e2a-a064-ec9fd2fdef83",
   "metadata": {},
   "source": [
    "Many data scientists use scikit-learn as the framework for running machine learning tasks. Conveniently, Dask is intentionally easy to integrate with scikit-learn and has strong API similarities in the dask-ml library. In this example, we’ll show you how to create a machine learning pipeline that has all the convenience of scikit-learn but adds the speed and performance of Dask. For more information about dask-ml, visit the [official docs.](https://ml.dask.org/)\n",
    "\n",
    "We’ll train a linear model to predict sale price of houses. We define a Pipeline to encompass both feature scaling and model training. This will be useful in cases like performing a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646e7be-a367-4e3d-8a0a-f920d6560f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "\n",
    "lr = Pipeline(\n",
    "    steps=[\n",
    "        (\"scale\", StandardScaler()),\n",
    "        (\"clf\", LinearRegression(penalty=\"l2\", max_iter=100)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaf1bce-e44d-42c2-a07d-8c03d6fbc500",
   "metadata": {},
   "source": [
    "Now we are ready to train our model. Before we train, we’ll convert our testing and training sets from dask.dataframe objects to dask.array objects. We’ll also take this chance to precompute the chunksize of our arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747675e-6087-4d61-83b5-562d642b3abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_arr = X_train.to_dask_array(lengths=True)\n",
    "y_train_arr = y_train.to_dask_array(lengths=True)\n",
    "X_test_arr = X_test.to_dask_array(lengths=True)\n",
    "y_test_arr = y_test.to_dask_array(lengths=True)\n",
    "\n",
    "lr_fitted = lr.fit(\n",
    "    X_train_arr,\n",
    "    y_train_arr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7967f3-e177-4886-b2b3-ab3cc69bf114",
   "metadata": {},
   "source": [
    "Evaluate the model against the test set using MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0038e-6970-46c4-8bf8-12999aa4f6f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_ml.metrics import mean_squared_error\n",
    "\n",
    "lr_preds = lr_fitted.predict(X_test_arr)\n",
    "mean_squared_error(y_test_arr, lr_preds, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1ad21-0896-48ee-9eb0-21ca3ed74176",
   "metadata": {},
   "source": [
    "If you want your models to perform even faster, check our example on [using RAPIDS on a GPU Cluster](https://saturncloud.io/docs/examples/python/rapids/qs-02-rapids-gpu-cluster/), where we have utilized Dask to orchestrate the model training over multiple worker machines, each with NVIDIA GPU to accelerate machine learning workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
