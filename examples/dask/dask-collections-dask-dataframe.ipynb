{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a417106-e439-4896-9d77-e7ca5bdad58b",
   "metadata": {},
   "source": [
    "# Dask DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748af31-6bd6-4272-bfb8-957a31b534d2",
   "metadata": {},
   "source": [
    "If you come across problems where you find that data is too large to fit into pandas DataFrame or computations in pandas are slow, you can transition to Dask DataFrames. Dask DataFrames mimic pandas DataFrames allow you to distribute data across a Dask cluster.\n",
    "\n",
    "Dask DataFrame can be thought of as multiple pandas DataFames spread over multiple Dask workers. In the diagram below you can see that we have one Dask DataFrame made up of 3 pandas DataFrames, which resides across multiple machines. Pandas being single threaded needs to fit all the data in a single machine, so if the size of the data is more than the size of your machine you may come across an 'out of memory' error. Dask DataFrames on the other hand distribut the data, hence you can use much more data and run commands on it concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a814d67-d69a-4e36-8146-95eb0709ceff",
   "metadata": {},
   "source": [
    "![dask dataframe](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/dask-dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786df27-2dad-45aa-ac7c-f902176821e1",
   "metadata": {},
   "source": [
    "## Creating a Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f91d8-402f-4474-aa60-f58894ba2edf",
   "metadata": {},
   "source": [
    "Dask DataFrames have a similar set of commands to pandas DataFrames, so creating and using them are fairly similar.\n",
    "\n",
    "First, start the Dask cluster associated with your Saturn Cloud resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa8764d-af5b-4f79-b5e3-3af31e712755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(SaturnCluster())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef384b0-5476-4888-bc13-128fc0d08e39",
   "metadata": {},
   "source": [
    "After running the above command, it's recommend that you check on the Saturn Cloud resource page that the Dask cluster as fully online before continuing. Alternatively, you can use the command `client.wait_for_workers(3)` to halt the notebook execution until all three of the workers are ready.\n",
    "\n",
    "## Create Dask Dataframe from File\n",
    "\n",
    "In code below, the data file is hosted on a public S3 bucket, so we can read the CSVs directly from there. Using `read_csv` from Dask takes the same form as using that function from pandas. You can also read other file formats like Parquet file, HDF files, JSON files etc. Note that Dask loads the data lazily--it won't read in the full dataset until it is used by later operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b4885d-edbc-4976-abee-67cab60c10c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"s3://saturn-public-data/examples/Dask/revised_house\", storage_options={\"anon\": True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b959fdef-049e-4a06-bb71-8cbbaaae346e",
   "metadata": {},
   "source": [
    "## Create Dask Dataframe from a pandas DataFrame\n",
    "\n",
    "You can create a Dask DataFrame from an existing pandas DataFrame using `from_pandas`. In the code below `npartitions` states how many partitions of the index we want to create. You can also use `chunksize` parameter instead which tells the number of rows per index partition to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6366311-b36d-412a-bc9d-6c6909f9999f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [{\"x\": 1, \"y\": 2, \"z\": 3}, {\"x\": 4, \"y\": 5, \"z\": 6}]\n",
    "\n",
    "# Creates DataFrame.\n",
    "df = pd.DataFrame(data)\n",
    "df1 = dd.from_pandas(df, npartitions=1)\n",
    "df1.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64c5db-36e6-4d9a-915d-77decb011b1f",
   "metadata": {},
   "source": [
    "## Create a Dask DataFrame from a Dask Array\n",
    "\n",
    "You can convert a dask array to dask dataframe using `from_dask_array` method. In code below parameter `column` lists column names for DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9208752c-ca35-4386-bc21-8b823117d098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "df = dd.from_dask_array(da.zeros((9, 3), chunks=(3, 3)), columns=[\"x\", \"y\", \"z\"])\n",
    "df.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494a52f-fd55-4a4d-95b4-aaed2692c0da",
   "metadata": {},
   "source": [
    "## Example of using Dask DataFrames\n",
    "\n",
    "The code below shows how to do group and summary operations with Dask DataFrames. Here we have a formula one laptime dataset taken from [kaggle](https://www.kaggle.com/rohanrao/formula-1-world-championship-1950-2020?select=lap_times.csv). You can see in code below that we have used group by and mean function on a Dask DataFrame the same way as we do with pandas except in the end we have added `compute()`. This is because dask is lazy and won't compute the operation until told to do so. When you use `read_csv`, Dask DataFrame is not going to read the entire data set. It is just going to read the column names and data types. Only when you do call compute function will Dask read all the data and perform computation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78bd25d-59db-4d4d-b77e-341685485041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = dd.read_csv(\n",
    "    \"s3://saturn-public-data/examples/Dask/f1_laptime.csv\", storage_options={\"anon\": True}\n",
    ")\n",
    "f1.groupby(\"driverId\").milliseconds.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f601f5-faba-43ad-aa78-e44270585f95",
   "metadata": {},
   "source": [
    "## Best Practices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91197fa-73d3-4e27-99bf-30fdf12cdcc8",
   "metadata": {},
   "source": [
    "1. Be thoughtful about when you use the `compute` command--a powerful part of Dask is that it can be used to avoid unnecessary computations until they're needed.\n",
    "2. Go simple whenever possible. Use Dask dataset when your data is large, but once you have filtered your data and reached a point where the data can be handled by pandas, use pandas.\n",
    "3.Choose your partitions wisely. Your data in partition should be small enough to fit in memory but big enough to avoid large overheads during operations\n",
    "\n",
    "See the Saturn Cloud blog on [differences in Dask and pandas](https://saturncloud.io/blog/dask-is-not-pandas/) for more detailed tips on this subject."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
