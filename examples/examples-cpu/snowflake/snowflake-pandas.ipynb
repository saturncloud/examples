{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake + Pandas\n",
    "\n",
    "How to load data from a Snowflake table or query into a pandas dataframe\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../_img/dask-horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Snowflake\n",
    "\n",
    "See [README](README.md) for more details on how to set up the credentials environment variables for SNOWFLAKE_ACCOUNT, SNOWFLAKE_USER, and SNOWFLAKE_PASSWORD.\n",
    "\n",
    "The other variables can be set on your Jupyter server or overwritten below based on the Snowflake warehouse and schema you used when running `load-data.sql`. Note that in order to update environment variables your Jupyter server will need to be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SNOWFLAKE_ACCOUNT = os.environ[\"SNOWFLAKE_ACCOUNT\"]\n",
    "SNOWFLAKE_USER = os.environ[\"SNOWFLAKE_USER\"]\n",
    "SNOWFLAKE_PASSWORD = os.environ[\"SNOWFLAKE_PASSWORD\"]\n",
    "\n",
    "SNOWFLAKE_WAREHOUSE = os.environ[\"SNOWFLAKE_WAREHOUSE\"]\n",
    "TAXI_DATABASE = os.environ[\"TAXI_DATABASE\"]\n",
    "TAXI_SCHEMA = os.environ[\"TAXI_SCHEMA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "conn_info = {\n",
    "    \"account\": SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": SNOWFLAKE_USER,\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"warehouse\": SNOWFLAKE_WAREHOUSE,\n",
    "    \"database\": TAXI_DATABASE,\n",
    "    \"schema\": TAXI_SCHEMA,\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run query\n",
    "\n",
    "The [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector-pandas.html) has `fetch_pandas_all()` and `fetch_pandas_batches()` methods that utilize [Arrow](https://arrow.apache.org/) for fast data exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    date_trunc('DAY', pickup_datetime) = '2020-01-01'\n",
    "\"\"\"\n",
    "cur = conn.cursor().execute(query)\n",
    "df = cur.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), df.memory_usage().sum() / 1e6  # memory size in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fetch_pandas_batches()` is useful if you can perform operations if the full result doesn't fit in memory, but there are operations you can perform to individual batches. It returns a `generator` that you can loop over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor().execute(query)\n",
    "batches = cur.fetch_pandas_batches()\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    print(len(batch), batch.memory_usage().sum() / 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data and/or computation are just too big for pandas on a single node, that's when you move to Dask! Check out the [`snowflake-dask.ipynb`](snowflake-dask.ipynb) notebook for the Dask implementation of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to use Snowflake and `snowflake-connector-python` to execute SQL queries over large datasets. You also learned how to read those query results into a `pandas` data frame.\n",
    "\n",
    "If you want to work with large query results or want to do post-processing that would benefit from a lot more parallelism than you can achieve on a single machine, you might want to read this result into a Dask DataFrame instead. Try [this dask + snowflake notebook](./snowflake-dask.ipynb) to learn how to efficiently read Snowflake query results into Dask collections.\n",
    "\n",
    "If you want to see how to combine the lessons from this notebook with common machine learning tasks like feature engineering and hyperparameter tuning, try [this scikit-learn notebook](../nyc-taxi-snowflake/hyperparameter-scikit.ipynb).\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
