{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowflake + Pandas\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../_img/snowflake.png\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial describes how to connect to Snowflake, load data into a Snowflake table, and work with that data in `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Connect to Snowflake\n",
    "\n",
    "This example uses data stored in a Snowflake data warehouse that is managed by the team at Saturn Cloud. We've set up a read-only user for use in these examples. If you would like to access data stored in your own Snowflake account, see the [README](./README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "\n",
    "conn_info = {\n",
    "    \"account\": os.environ[\"EXAMPLE_SNOWFLAKE_ACCOUNT\"],\n",
    "    \"user\": os.environ[\"EXAMPLE_SNOWFLAKE_USER\"],\n",
    "    \"password\": os.environ[\"EXAMPLE_SNOWFLAKE_PASSWORD\"],\n",
    "    \"database\": os.environ[\"TAXI_DATABASE\"],\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Run query\n",
    "\n",
    "The [Snowflake Connector for Python](https://docs.snowflake.com/en/user-guide/python-connector-pandas.html) has `fetch_pandas_all()` and `fetch_pandas_batches()` methods that utilize [Arrow](https://arrow.apache.org/) for fast data exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM taxi_yellow\n",
    "WHERE\n",
    "    date_trunc('DAY', pickup_datetime) = '2020-01-01'\n",
    "\"\"\"\n",
    "cur = conn.cursor().execute(query)\n",
    "df = cur.fetch_pandas_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), df.memory_usage().sum() / 1e6  # memory size in MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fetch_pandas_batches()` is useful if you can perform operations if the full result doesn't fit in memory, but there are operations you can perform to individual batches. It returns a `generator` that you can loop over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor().execute(query)\n",
    "batches = cur.fetch_pandas_batches()\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in batches:\n",
    "    print(len(batch), batch.memory_usage().sum() / 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data and/or computation are just too big for pandas on a single node, that's when you move to Dask! Check out the [`snowflake-dask.ipynb`](snowflake-dask.ipynb) notebook for the Dask implementation of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to use Snowflake and `snowflake-connector-python` to execute SQL queries over large datasets. You also learned how to read those query results into a `pandas` data frame.\n",
    "\n",
    "If you want to work with large query results or want to do post-processing that would benefit from a lot more parallelism than you can achieve on a single machine, you might want to read this result into a Dask DataFrame instead. Try [this dask + snowflake notebook](./snowflake-dask.ipynb) to learn how to efficiently read Snowflake query results into Dask collections.\n",
    "\n",
    "If you want to see how to combine the lessons from this notebook with common machine learning tasks like feature engineering and hyperparameter tuning, try [this scikit-learn notebook](../nyc-taxi-snowflake/hyperparameter-scikit.ipynb).\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
