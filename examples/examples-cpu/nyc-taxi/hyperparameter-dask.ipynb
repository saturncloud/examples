{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning (multi-node with Dask)\n",
    "\n",
    "<img src=\"../_img/dask-horizontal.svg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a machine learning training workflow using the famous [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). That dataset contains information on taxi trips in New York City.\n",
    "\n",
    "In this exercise, you'll load data into a [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html) and use [`dask-ml`](https://ml.dask.org/) to answer this question\n",
    "\n",
    "> based on characteristics that can be known at the beginning of a trip, what tip will this trip earn (as a % of the total fare)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekofyear\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_week_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "categorical_feat = [\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = \"tip_fraction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Initialize A Dask Cluster\n",
    "\n",
    "This tutorial uses multiple machines to show how to apply more computing resources to machine learning training. This is done with Dask. Saturn Cloud offers managed Dask clusters, which can be provisioned and modified programmatically.\n",
    "\n",
    "The code below creates a Dask cluster using [`dask-saturn`](https://github.com/saturncloud/dask-saturn), the official Dask client for Saturn Cloud. It creates a cluster with the following specs:\n",
    "\n",
    "* `n_workers=3` --> 3 machines in the cluster\n",
    "* `scheduler_size='medium'` --> the Dask scheduler will have 4GB of RAM and 2 CPU cores\n",
    "* `worker_size='large'` --> each worker machine will have 2 CPU cores and 16GB of RAM\n",
    "\n",
    "To see a list of possible sizes, run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_saturn\n",
    "\n",
    "dask_saturn.describe_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask-saturn` code below creates two important objects: a cluster and a client.\n",
    "\n",
    "* `cluster`: knows about and manages the scheduler and workers\n",
    "    - can be used to create, resize, reconfigure, or destroy those resources\n",
    "    - knows how to communicate with the scheduler, and where to find logs and diagnostic dashboards\n",
    "* `client`: tells the cluster to do things\n",
    "    - can send work to the cluster\n",
    "    - can restart all the worker processes\n",
    "    - can send data to the cluster or pull data back from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers,\n",
    "    scheduler_size=\"medium\",\n",
    "    worker_size=\"large\",\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster in the Saturn UI if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering\n",
    "\n",
    "Load a sample from a single month for this exercise. Note we are loading the data with Dask now (`dd.read_csv` vs. `pd.read_csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "taxi = dd.read_csv(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2019-01.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    storage_options={\"anon\": True},\n",
    "    assume_missing=True,\n",
    ").sample(frac=0.01, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num rows: {len(taxi)}, Size: {taxi.memory_usage(deep=True).sum().compute() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df: dd.DataFrame) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate features from a raw taxi dataframe.\n",
    "    \"\"\"\n",
    "    df = df[df.fare_amount > 0]  # avoid divide-by-zero\n",
    "    df[\"tip_fraction\"] = df.tip_amount / df.fare_amount\n",
    "\n",
    "    df[\"pickup_weekday\"] = df.tpep_pickup_datetime.dt.weekday\n",
    "    df[\"pickup_weekofyear\"] = df.tpep_pickup_datetime.dt.isocalendar().week\n",
    "    df[\"pickup_hour\"] = df.tpep_pickup_datetime.dt.hour\n",
    "    df[\"pickup_week_hour\"] = (df.pickup_weekday * 24) + df.pickup_hour\n",
    "    df[\"pickup_minute\"] = df.tpep_pickup_datetime.dt.minute\n",
    "    df = df[features + [y_col]].astype(float).fillna(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "taxi_train = prep_df(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the preprocessing and `GridSearchCV` classes from dask-ml, but still use the scikit-learn `ElasticNet` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from dask_ml.compose import ColumnTransformer\n",
    "from dask_ml.preprocessing import StandardScaler, DummyEncoder, Categorizer\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"categorize\", Categorizer(columns=categorical_feat)),\n",
    "        (\"onehot\", DummyEncoder(columns=categorical_feat)),\n",
    "        (\n",
    "            \"scale\",\n",
    "            ColumnTransformer(\n",
    "                transformers=[(\"num\", StandardScaler(), numeric_feat)],\n",
    "                remainder=\"passthrough\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", ElasticNet(normalize=False, max_iter=100)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"clf__l1_ratio\": np.arange(0, 1.1, 0.1),\n",
    "    \"clf__alpha\": [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=3, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the Dask dashboard after you run the cell below, you'll see the grid search in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(taxi_train[features], taxi_train[y_col])\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "`GridSearchCV` automatically fits the best parameters to the full data and stores in `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "with open(f\"{MODEL_PATH}/elastic_net_dask.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics on test set\n",
    "\n",
    "Use a different month for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = dd.read_csv(\n",
    "    \"s3://nyc-tlc/trip data/yellow_tripdata_2019-02.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    storage_options={\"anon\": True},\n",
    "    assume_missing=True,\n",
    ").sample(frac=0.01, replace=False)\n",
    "\n",
    "taxi_test = prep_df(taxi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = grid_search.predict(taxi_test[features])\n",
    "mean_squared_error(taxi_test[y_col], preds, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to use `dask-ml` to perform feature engineering and evaluate machine learning experiments based on distributed data in Dask DataFrames.\n",
    "\n",
    "Next, try [this Dask + xgboost notebook](./xgboost-dask.ipynb) to see how to perform distributed XGBoost training using Dask.\n",
    "\n",
    "Alternatively, try [this notebook](./dashboard.ipynb) to learn how to use Saturn Cloud to deploy an interactive dashboard that includes a component which uses the models trained above.\n",
    "\n",
    "If you want to try working with larger datasets, change the code in this notebook to use more data or explore how to programmatically resize your Dask cluster by following the tips in [\"Managing Dask Resources for Machine Learning Training\"](https://www.saturncloud.io/docs/tips-and-tricks/training-on-dask).\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
