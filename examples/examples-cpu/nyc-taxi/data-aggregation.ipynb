{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation for Dashboard\n",
    "\n",
    "This notebook contains the data aggregation code to prepare data files for the dashboard. You can run this notebook to see how Dask is used with a Saturn cluster for data processing, but the files generated here will not be used by any of the examples. The dashboard uses pre-aggregated files from Saturn's public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "import hvplot.dask, hvplot.pandas\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Dask cluster\n",
    "\n",
    "We will need to do some data processing that exceeds the capacity of our JupyterLab client. To monitor the status of your cluster, visit the \"Logs\" page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(n_workers=n_workers, scheduler_size='medium', worker_size='large', nthreads=2)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialized your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster from the \"Dask\" page in Saturn if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Setup a function to load files with Dask. Cleanup some column names and parse data types correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance',\n",
    "           'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount',\n",
    "           'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n",
    "\n",
    "def read_taxi_csv(files):\n",
    "    ddf = dd.read_csv(files, \n",
    "                      assume_missing=True,\n",
    "                      parse_dates=[1, 2], \n",
    "                      usecols=usecols, \n",
    "                      storage_options={'anon': True})\n",
    "    # grab the columns we need and rename\n",
    "    ddf = ddf[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID',\n",
    "               'passenger_count', 'trip_distance', 'payment_type', 'tip_amount', 'fare_amount']]\n",
    "    ddf.columns = ['pickup_datetime', 'dropoff_datetime', 'pickup_taxizone_id', 'dropoff_taxizone_id',\n",
    "                   'passenger_count', 'trip_distance', 'payment_type', 'tip_amount', 'fare_amount']\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a listing of files from the public S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f's3://{x}' for x in fs.glob('s3://nyc-tlc/trip data/yellow_tripdata_201*.csv')\n",
    "         if '2017' in x or '2018' in x or '2019' in x]\n",
    "len(files), files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = read_taxi_csv(files[:5])  # only load first 5 months of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We are loading a small sample for this exercise, but if you want to use the full data and replicate the aggregated data hosted on Saturn's bucket, you will need to use a larger cluster. Here is a sample cluster configuration you can use, but you can play around with sizes and see how performance changes!\n",
    "\n",
    "```python\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=10, \n",
    "    scheduler_size='xlarge',\n",
    "    worker_size='8xlarge', \n",
    "    nthreads=32,\n",
    ")\n",
    "```\n",
    "\n",
    "You will have to run `cluster.reset(...)` if the cluster has already been configured. Run the following to see what sizes are available:\n",
    "\n",
    "```python\n",
    "from dask_saturn.core import describe_sizes\n",
    "describe_sizes()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all 3 years of data\n",
    "# ddf = read_taxi_csv(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregated files for Dashboard\n",
    "\n",
    "Create several CSV file to use for visualization in the dashboard. Note that each of these perform some Dask dataframe operations, then call `compute()` to pull down a pandas dataframe, and then write that dataframe to a CSV file.\n",
    "\n",
    "## Augment data\n",
    "\n",
    "We'll distill some features out of the datetime component of the data. This is similar to the feature engineering that is done in other places in this demo, but we'll only create the features that'll be most useful in the visuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"pickup_hour\"] = ddf.pickup_datetime.dt.hour\n",
    "ddf[\"dropoff_hour\"] = ddf.dropoff_datetime.dt.hour\n",
    "ddf[\"pickup_weekday\"] = ddf.pickup_datetime.dt.weekday\n",
    "ddf[\"dropoff_weekday\"] = ddf.dropoff_datetime.dt.weekday\n",
    "ddf[\"percent_tip\"] = (ddf[\"tip_amount\"] / ddf[\"fare_amount\"]).replace([np.inf, -np.inf], np.nan) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take out the extreme high values since they disrupt the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"percent_tip\"] = ddf[\"percent_tip\"].apply(lambda x: np.nan if x > 1000 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all of the above cells execute pretty much instantly. This is because of Dask's [lazy evaluation](https://tutorial.dask.org/01x_lazy.html). Calling `persist()` below tells Dask to run all the operations and keep the results in memory for faster computation. This cell takes some time to run because Dask needs to first parse all the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = ddf.persist()\n",
    "_ = wait(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries datasets\n",
    "\n",
    "We'll resample to an hourly timestep so that we don't have to pass around so much data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_ddf = ddf[[\"pickup_datetime\", \"percent_tip\"]].set_index(\"pickup_datetime\").dropna()\n",
    "tips = tip_ddf.resample('1H').mean().compute()\n",
    "\n",
    "tips.to_csv(f\"{DATA_PATH}/pickup_average_percent_tip_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_ddf = ddf[[\"pickup_datetime\", \"fare_amount\"]].set_index(\"pickup_datetime\").dropna()\n",
    "fare = fare_ddf.resample('1H').mean().compute()\n",
    "\n",
    "fare.to_csv(f\"{DATA_PATH}/pickup_average_fare_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate datasets\n",
    "\n",
    "Since our data is rather large and will mostly be viewed in grouped aggregates, we can do some aggregation now and save it off for use in plots later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in [\"pickup\", \"dropoff\"]:\n",
    "    data = (ddf\n",
    "            .groupby([\n",
    "                f\"{value}_taxizone_id\", \n",
    "                f\"{value}_hour\",  \n",
    "                f\"{value}_weekday\",\n",
    "            ])\n",
    "            .agg({\n",
    "                \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "                \"trip_distance\": [\"mean\"],\n",
    "                \"percent_tip\": [\"mean\"],\n",
    "            })\n",
    "            .compute()\n",
    "           )\n",
    "    data.columns = data.columns.to_flat_index()\n",
    "    data = data.rename({\n",
    "        (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "        (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "        (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "        (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "        (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "        \n",
    "    }, axis=1).reset_index(level=[1, 2])\n",
    "    data.to_csv(f\"{DATA_PATH}/{value}_grouped_by_zone_and_time.csv\")\n",
    "\n",
    "grouped_zone_and_time = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in [\"pickup\", \"dropoff\"]:\n",
    "    data = (ddf\n",
    "            .groupby([\n",
    "                f\"{value}_taxizone_id\", \n",
    "            ])\n",
    "            .agg({\n",
    "                \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "                \"trip_distance\": [\"mean\"],\n",
    "                \"percent_tip\": [\"mean\"],\n",
    "            })\n",
    "            .compute()\n",
    "           )\n",
    "    data.columns = data.columns.to_flat_index()\n",
    "    data = data.rename({\n",
    "        (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "        (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "        (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "        (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "        (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "        \n",
    "    }, axis=1)\n",
    "    data.to_csv(f\"{DATA_PATH}/{value}_grouped_by_zone.csv\")\n",
    "\n",
    "grouped_zone = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = \"pickup\"\n",
    "data = (ddf\n",
    "        .groupby([\n",
    "            f\"{value}_hour\", \n",
    "            f\"{value}_weekday\"\n",
    "        ])\n",
    "        .agg({\n",
    "            \"fare_amount\": [\"mean\", \"count\", \"sum\"],\n",
    "            \"trip_distance\": [\"mean\"],\n",
    "            \"percent_tip\": [\"mean\"],\n",
    "        })\n",
    "        .compute()\n",
    "       )\n",
    "data.columns = data.columns.to_flat_index()\n",
    "data = data.rename({\n",
    "    (\"fare_amount\", \"mean\"): \"average_fare\",\n",
    "    (\"fare_amount\", \"count\"): \"total_rides\",\n",
    "    (\"fare_amount\", \"sum\"): \"total_fare\",\n",
    "    (\"trip_distance\", \"mean\"): \"average_trip_distance\",\n",
    "    (\"percent_tip\", \"mean\"): \"average_percent_tip\",\n",
    "\n",
    "}, axis=1)\n",
    "\n",
    "data.to_csv(f\"{DATA_PATH}/{value}_grouped_by_time.csv\")\n",
    "grouped_time = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get shape files for dashboard\n",
    "\n",
    "The shape files are stored in a zip on the public S3. Here we pull it down, unzip it, then place the files on our S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with fs.open('s3://nyc-tlc/misc/taxi_zones.zip') as f:\n",
    "    with zipfile.ZipFile(f) as zip_ref:\n",
    "        zip_ref.extractall(f'{DATA_PATH}/taxi_zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "To make use of the new datasets we can visualize all the data at once using a grouped heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_zone_and_time.hvplot.heatmap(\n",
    "    x=\"dropoff_weekday\", \n",
    "    y=\"dropoff_hour\", \n",
    "    C=\"average_percent_tip\",\n",
    "    groupby=\"dropoff_taxizone_id\", \n",
    "    responsive=True, min_height=600, cmap=\"viridis\", clim=(0, 20),\n",
    "    colorbar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset that is only grouped by zone can be paired with other information such as geography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "zones = gpd.read_file(f'{DATA_PATH}/taxi_zones/taxi_zones.shp').to_crs('epsg:4326')\n",
    "joined = zones.join(grouped_zone, on=\"LocationID\")\n",
    "\n",
    "joined.hvplot(x=\"longitude\", y=\"latitude\", c=\"average_fare\", \n",
    "              geo=True, tiles=\"CartoLight\", cmap=\"fire\", alpha=0.5,\n",
    "              hover_cols=[\"zone\", \"borough\"], \n",
    "              title=\"Average fare by dropoff location\",\n",
    "              height=600, width=800, clim=(0, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
