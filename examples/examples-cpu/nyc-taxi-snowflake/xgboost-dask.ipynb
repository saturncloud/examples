{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost regression (multi-node with Dask)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/XGBoost_logo.png\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./img/snowflake.png\" width=\"450\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes how to use Dask to scale training of XGBoost models. For more detailed information, see [\"Distributed XGBoost with Dask\"](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html) in the XGBoost documentation and [\"XGBoost Training with Dask\"](https://www.saturncloud.io/docs/tutorials/xgboost/) in Saturn Cloud's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_PATH = 'models'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "numeric_feat = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "categorical_feat = [\n",
    "    'pickup_taxizone_id', \n",
    "    'dropoff_taxizone_id',\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = 'tip_fraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dask cluster\n",
    "\n",
    "The code below uses [`dask-saturn`](https://github.com/saturncloud/dask-saturn) to create a Dask cluster or connect to one that is already running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers, \n",
    "    scheduler_size='medium',\n",
    "    worker_size='large', \n",
    "    nthreads=2,\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dashboard (link above ^) and watch it when you execute some commands, you'll see which tasks are running across the cluster.\n",
    "\n",
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster from the \"Dask\" page in Saturn if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering\n",
    "\n",
    "Load a sample from a single month for this exercise. Note we are loading the data with Dask now by defining a `dask.delayed` function to load partitions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "import snowflake.connector\n",
    "\n",
    "SNOWFLAKE_ACCOUNT = os.environ['SNOWFLAKE_ACCOUNT']\n",
    "SNOWFLAKE_USER = os.environ['SNOWFLAKE_USER']\n",
    "SNOWFLAKE_PASSWORD = os.environ['SNOWFLAKE_PASSWORD']\n",
    "\n",
    "SNOWFLAKE_WAREHOUSE = os.environ['SNOWFLAKE_WAREHOUSE']\n",
    "TAXI_DATABASE = os.environ['TAXI_DATABASE']\n",
    "TAXI_SCHEMA = os.environ['TAXI_SCHEMA']\n",
    "\n",
    "conn_info = {\n",
    "    'account': SNOWFLAKE_ACCOUNT,\n",
    "    'user': SNOWFLAKE_USER,\n",
    "    'password': SNOWFLAKE_PASSWORD,\n",
    "    'warehouse': SNOWFLAKE_WAREHOUSE,\n",
    "    'database': TAXI_DATABASE,\n",
    "    'schema': TAXI_SCHEMA,\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load()` function specifies how each partition of the Dask dataframe should load its chunk of the data. As each partition of a Dask dataframe is a Pandas dataframe, we utilize the `fetch_pandas_all()` method.  We use a [binding for the Snowflake query](https://docs.snowflake.com/en/user-guide/python-connector-example.html#binding-data) so that we can pass different values at execution time. In this case, we pull one day of taxi rides into each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM (\n",
    "    SELECT \n",
    "        pickup_taxizone_id,\n",
    "        dropoff_taxizone_id,\n",
    "        passenger_count,\n",
    "        DIV0(tip_amount, fare_amount) AS tip_fraction,\n",
    "        DAYOFWEEKISO(pickup_datetime) - 1 AS pickup_weekday,\n",
    "        WEEKOFYEAR(pickup_datetime) AS pickup_weekofyear,\n",
    "        HOUR(pickup_datetime) AS pickup_hour,\n",
    "        (pickup_weekday * 24) + pickup_hour AS pickup_week_hour,\n",
    "        MINUTE(pickup_datetime) AS pickup_minute\n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        DATE(pickup_datetime) = %s\n",
    ") SAMPLE (30)\n",
    "\"\"\"\n",
    "\n",
    "@dask.delayed\n",
    "def load(conn_info, query, day):\n",
    "    conn = snowflake.connector.connect(**conn_info)\n",
    "    taxi = conn.cursor().execute(query, str(day)).fetch_pandas_all()\n",
    "    taxi.columns = taxi.columns.str.lower()\n",
    "    return taxi    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query to determine which specific dates we need to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(start, end):\n",
    "    date_query = \"\"\"\n",
    "    SELECT\n",
    "        DISTINCT(DATE(pickup_datetime)) as date \n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        pickup_datetime BETWEEN %s and %s\n",
    "    \"\"\"\n",
    "    dates_df = conn.cursor().execute(date_query, (start, end)).fetch_pandas_all()\n",
    "    return dates_df['DATE'].tolist()\n",
    "\n",
    "dates = get_dates('2019-01-01', '2019-01-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dd.from_delayed` function takes delayed Pandas outputs from a list and converts that to a high-level DataFrame object.\n",
    "\n",
    "Since we're sampling in the Snowflake query, we run `persist()` to tell Dask to load all results into memory across the cluster. This ensures that Dask's lazy evaluation doesn't call the Snowflake query multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = dd.from_delayed([load(conn_info, query, day) for day in dates])\n",
    "taxi = taxi.persist()\n",
    "_ = wait(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num rows: {len(taxi)}, Size: {taxi.memory_usage(deep=True).sum().compute() / 1e6} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train = taxi[features + [y_col]].astype(float).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model\n",
    "\n",
    "This example uses the native Dask integration built into XGBoost. That integration was added in `xgboost` 1.3.0, and should be preferred to [`dask-xgboost`](https://github.com/dask/dask-xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data for `xgboost.dask` needs to be prepared in a special object called `DaskDMatrix`. This is like the XGBoost `DMatrix` that you might be familiar with, but is backed by Dask's distributed collections (Dask DataFrame and Dask Array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.dask.DaskDMatrix(\n",
    "    client=client,\n",
    "    data=taxi_train[features],\n",
    "    label=taxi_train[y_col]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any [xgboost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) to `xgb.dask.train()`. The training process will then start up on all workers that have some of the data in `dtrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = xgb.dask.train(\n",
    "    client=client,\n",
    "    params={\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 5,\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xgb.dask.train()` produces a regular `xgb.core.Booster` object, the same model object produced by non-Dask training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = result[\"booster\"]\n",
    "type(booster)\n",
    "\n",
    "# xgboost.core.Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "with open(f'{MODEL_PATH}/xgboost_dask.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(booster, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics on test set\n",
    "\n",
    "Use a different month for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = get_dates('2019-02-01', '2019-02-28')\n",
    "taxi_test = dd.from_delayed([load(conn_info, query, day) for day in test_dates])\n",
    "taxi_test = taxi_test.persist()\n",
    "_ = wait(taxi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xgboost.dask.predict()` can be used to create predictions on a Dask collection using an XGBoost model object. Because the model object here is just a regular XGBoost model, using `dask-xgboost` for batch scoring doesn't require that you also perform training on Dask.\n",
    "\n",
    "This function returns a Dask Array or Dask Series of predictions, depending on the input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.dask.predict(\n",
    "    client=client,\n",
    "    model=booster,\n",
    "    data=taxi_test[features]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics functions in `dask_ml` can compute metrics on Dask collections like Dask Array and Dask DataFrame, so you never have to hold all of the test data in memory on the client. These functions intentionally mimic the metrics functions in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(taxi_test[y_col].to_dask_array(), preds.to_dask_array(), squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
