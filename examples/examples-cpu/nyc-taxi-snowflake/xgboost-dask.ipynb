{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost regression (multi-node with Dask)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../_img/dask-horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../_img/xgboost.png\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../_img/snowflake.png\" width=\"450\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes a machine learning training workflow using the famous [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). That dataset contains information on taxi trips in New York City.\n",
    "\n",
    "In this exercise, you'll load data from Snowflake into a [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html) and use [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html) to answer this question\n",
    "\n",
    "> based on characteristics that can be known at the beginning of a trip, what tip will this trip earn (as a % of the total fare)?\n",
    "\n",
    "This notebook gives an introductory tutorial on how to use Dask to scale training of XGBoost models. For more detailed information, see [\"Distributed XGBoost with Dask\"](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html) in the XGBoost documentation and [\"XGBoost Training with Dask\"](https://www.saturncloud.io/docs/tutorials/xgboost/) in Saturn Cloud's documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Initialize A Dask Cluster\n",
    "\n",
    "This tutorial uses multiple machines to show how to apply more computing resources to machine learning training. This is done with Dask. Saturn Cloud offers managed Dask clusters, which can be provisioned and modified programmatically.\n",
    "\n",
    "The code below creates a Dask cluster using [`dask-saturn`](https://github.com/saturncloud/dask-saturn), the official Dask client for Saturn Cloud. It creates a cluster with the following specs:\n",
    "\n",
    "* `n_workers=3` --> 3 machines in the cluster\n",
    "* `scheduler_size='medium'` --> the Dask scheduler will have 4GB of RAM and 2 CPU cores\n",
    "* `worker_size='large'` --> each worker machine will have 2 CPU cores and 16GB of RAM\n",
    "\n",
    "To see a list of possible sizes, run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_saturn\n",
    "\n",
    "dask_saturn.describe_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dask-saturn` code below creates two important objects: a cluster and a client.\n",
    "\n",
    "* `cluster`: knows about and manages the scheduler and workers\n",
    "    - can be used to create, resize, reconfigure, or destroy those resources\n",
    "    - knows how to communicate with the scheduler, and where to find logs and diagnostic dashboards\n",
    "* `client`: tells the cluster to do things\n",
    "    - can send work to the cluster\n",
    "    - can restart all the worker processes\n",
    "    - can send data to the cluster or pull data back from the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers,\n",
    "    scheduler_size=\"medium\",\n",
    "    worker_size=\"large\",\n",
    ")\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster in the Saturn UI if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Resource Usage\n",
    "\n",
    "This tutorial aims to teach you how to take advantage of multiple GPUs for data science workflows. To prove to yourself that Dask RAPIDS are utilizing the GPUs, it's important to understand how to monitor that utilization while your code is running. If you already know how to do that, skip to the next section.\n",
    "\n",
    "Print the `cluster` object in a notebook renders a widget that shows the number of workers, available CPU and memory, and a dashboard link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click that dashboard link to view some diagnostic information about the Dask cluster. This can be used to view the current resource utilization of workers in the cluster and lots of information about what they're currently working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Connect to Snowflake\n",
    "\n",
    "This examples uses data stored in a Snowflake data warehouse. The code below expects the following environment variables to be set. If you haven't set these, return to the Saturn project page to set them.\n",
    "\n",
    "* `SNOWFLAKE_ACCOUNT`\n",
    "* `SNOWFLAKE_USER`\n",
    "* `SNOWFLAKE_PASSWORD`\n",
    "* `SNOWFLAKE_WAREHOUSE`\n",
    "* `TAXI_DATABASE`\n",
    "* `TAXI_SCHEMA`\n",
    "\n",
    "For more details on these environment variables, see [\"Connecting to Snowflake\"](https://docs.snowflake.com/en/user-guide/python-connector-example.html#connecting-to-snowflake) in the `snowflake-connector-python` docs.\n",
    "\n",
    "The `SNOWFLAKE_*` variables should be set up as Saturn credentials. The `TAXI_*` variables can be set on your Jupyter server or overwritten below based on the Snowflake warehouse and schema you used when running [load-data.sql](../snowflake/load-data.sql). Note that in order to update environment variables your Jupyter server will need to be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "import snowflake.connector\n",
    "\n",
    "SNOWFLAKE_ACCOUNT = os.environ[\"SNOWFLAKE_ACCOUNT\"]\n",
    "SNOWFLAKE_USER = os.environ[\"SNOWFLAKE_USER\"]\n",
    "SNOWFLAKE_PASSWORD = os.environ[\"SNOWFLAKE_PASSWORD\"]\n",
    "\n",
    "SNOWFLAKE_WAREHOUSE = os.environ[\"SNOWFLAKE_WAREHOUSE\"]\n",
    "TAXI_DATABASE = os.environ[\"TAXI_DATABASE\"]\n",
    "TAXI_SCHEMA = os.environ[\"TAXI_SCHEMA\"]\n",
    "\n",
    "conn_info = {\n",
    "    \"account\": SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": SNOWFLAKE_USER,\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"warehouse\": SNOWFLAKE_WAREHOUSE,\n",
    "    \"database\": TAXI_DATABASE,\n",
    "    \"schema\": TAXI_SCHEMA,\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Load data\n",
    "\n",
    "This example is designed to run quickly with small resources. So let's just load a single month of taxi data for training.\n",
    "\n",
    "This example uses Snowflake to handle the hard work of creating new features, then creates a Dask DataFrame with the result. It uses `dask.delayed()` to load partitions of the query result in parallel on the worker, so that you don't have to load the entire query result in this Jupyter and then send it to the Dask cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load()` function specifies how each partition of the Dask dataframe should load its chunk of the data. As each partition of a Dask dataframe is a Pandas dataframe, we utilize the `fetch_pandas_all()` method.  We use a [binding for the Snowflake query](https://docs.snowflake.com/en/user-guide/python-connector-example.html#binding-data) so that we can pass different values at execution time. In this case, we pull one day of taxi rides into each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM (\n",
    "    SELECT\n",
    "        pickup_taxizone_id,\n",
    "        dropoff_taxizone_id,\n",
    "        passenger_count,\n",
    "        DIV0(tip_amount, fare_amount) AS tip_fraction,\n",
    "        DAYOFWEEKISO(pickup_datetime) - 1 AS pickup_weekday,\n",
    "        WEEKOFYEAR(pickup_datetime) AS pickup_weekofyear,\n",
    "        HOUR(pickup_datetime) AS pickup_hour,\n",
    "        (pickup_weekday * 24) + pickup_hour AS pickup_week_hour,\n",
    "        MINUTE(pickup_datetime) AS pickup_minute\n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        DATE(pickup_datetime) = %s\n",
    ") SAMPLE (30)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def load(conn_info, query, day):\n",
    "    with snowflake.connector.connect(**conn_info) as conn:\n",
    "        taxi = conn.cursor().execute(query, str(day)).fetch_pandas_all()\n",
    "        taxi.columns = taxi.columns.str.lower()\n",
    "        return taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query to determine which specific dates we need to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(start, end):\n",
    "    date_query = \"\"\"\n",
    "    SELECT\n",
    "        DISTINCT(DATE(pickup_datetime)) as date\n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        pickup_datetime BETWEEN %s and %s\n",
    "    \"\"\"\n",
    "    dates_df = conn.cursor().execute(date_query, (start, end)).fetch_pandas_all()\n",
    "    return dates_df[\"DATE\"].tolist()\n",
    "\n",
    "\n",
    "dates = get_dates(\"2019-01-01\", \"2019-01-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dd.from_delayed` function takes delayed Pandas outputs from a list and converts that to a high-level DataFrame object.\n",
    "\n",
    "Since we're sampling in the Snowflake query, we run `persist()` to tell Dask to load all results into memory across the cluster. This ensures that Dask's lazy evaluation doesn't call the Snowflake query multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = dd.from_delayed([load(conn_info, query, day) for day in dates])\n",
    "taxi = taxi.persist()\n",
    "_ = wait(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num rows: {len(taxi)}, Size: {taxi.memory_usage(deep=True).sum().compute() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feat = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekofyear\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_week_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "categorical_feat = [\n",
    "    \"pickup_taxizone_id\",\n",
    "    \"dropoff_taxizone_id\",\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = \"tip_fraction\"\n",
    "\n",
    "taxi_train = taxi[features + [y_col]].astype(float).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train a model\n",
    "\n",
    "This example uses the native Dask integration built into XGBoost. That integration was added in `xgboost` 1.3.0, and should be preferred to [`dask-xgboost`](https://github.com/dask/dask-xgboost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data for `xgboost.dask` needs to be prepared in a special object called `DaskDMatrix`. This is like the XGBoost `DMatrix` that you might be familiar with, but is backed by Dask's distributed collections (Dask DataFrame and Dask Array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = taxi_train[features]\n",
    "label = taxi_train[y_col]\n",
    "\n",
    "dtrain = xgb.dask.DaskDMatrix(client=client, data=data, label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass any [xgboost parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) to `xgb.dask.train()`. The training process will then start up on all workers that have some of the data in `dtrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = xgb.dask.train(\n",
    "    client=client,\n",
    "    params={\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 5,\n",
    "    },\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xgb.dask.train()` produces a regular `xgb.core.Booster` object, the same model object produced by non-Dask training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = result[\"booster\"]\n",
    "type(booster)\n",
    "\n",
    "# xgboost.core.Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Save model\n",
    "\n",
    "`xgb.dask.train()` produces a regular `xgb.core.Booster` object, the same model object produced by non-Dask training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = result[\"booster\"]\n",
    "type(booster)\n",
    "\n",
    "# xgboost.core.Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've trained a model, save it in a file to use later for scoring or for comparison with other models.\n",
    "\n",
    "There are several ways to do this, but `cloudpickle` is likely to give you the best experience. It handles some common drawbacks of the built-in `pickle` library.\n",
    "\n",
    "`cloudpickle` can be used to write a Python object to bytes, and to create a Python object from that binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "with open(f\"{MODEL_PATH}/xgboost_dask.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(booster, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Calculate metrics on test set\n",
    "\n",
    "Use a different month for test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = get_dates(\"2019-02-01\", \"2019-02-28\")\n",
    "taxi_test = dd.from_delayed([load(conn_info, query, day) for day in test_dates])\n",
    "taxi_test = taxi_test.persist()\n",
    "_ = wait(taxi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xgboost.dask.predict()` can be used to create predictions on a Dask collection using an XGBoost model object. Because the model object here is just a regular XGBoost model, using `dask-xgboost` for batch scoring doesn't require that you also perform training on Dask.\n",
    "\n",
    "This function returns a Dask Array or Dask Series of predictions, depending on the input type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xgb.dask.predict(client=client, model=booster, data=taxi_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics functions in `dask_ml` can compute metrics on Dask collections like Dask Array and Dask DataFrame, so you never have to hold all of the test data in memory on the client. These functions intentionally mimic the metrics functions in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(taxi_test[y_col].to_dask_array(), preds.to_dask_array(), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you learned how to use Dask and `xgboost` for distributed model training.\n",
    "\n",
    "If you want to try working with larger datasets, change the code in this notebook to use more data or explore how to programmatically resize your Dask cluster by following the tips in [\"Managing Dask Resources for Machine Learning Training\"](https://www.saturncloud.io/docs/tips-and-tricks/training-on-dask).\n",
    "\n",
    "Alternatively, try [this notebook](./dashboard.ipynb) to learn how to use Saturn Cloud to deploy an interactive dashboard that includes a component which uses the models trained above.\n",
    "\n",
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
