{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "## Dask cluster\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask_horizontal.svg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "MODEL_PATH = 'models'\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "numeric_feat = [\n",
    "    'pickup_weekday', \n",
    "    'pickup_weekofyear', \n",
    "    'pickup_hour', \n",
    "    'pickup_week_hour', \n",
    "    'pickup_minute', \n",
    "    'passenger_count',\n",
    "]\n",
    "categorical_feat = [\n",
    "    'pickup_taxizone_id', \n",
    "    'dropoff_taxizone_id',\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = 'tip_fraction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "import time\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers,\n",
    "    scheduler_size='medium',\n",
    "    worker_size='large', \n",
    "    nthreads=2,\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dashboard (link above ^) and watch it when you execute some commands, you'll see which tasks are running across the cluster.\n",
    "\n",
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster from the \"Dask\" page in Saturn if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(client.scheduler_info()['workers']) < n_workers:\n",
    "    print('Waiting for workers, got', len(client.scheduler_info()['workers']))\n",
    "    time.sleep(30)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering\n",
    "\n",
    "Load a sample from a single month for this exercise. Note we are loading the data with Dask now by defining a `dask.delayed` function to load partitions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import yaml\n",
    "import snowflake.connector\n",
    "\n",
    "creds = yaml.full_load(open('/home/jovyan/snowflake_creds.yml'))\n",
    "\n",
    "conn_info = {\n",
    "    'warehouse': 'COMPUTE_WH',\n",
    "    'database': 'NYC_TAXI',\n",
    "    'schema': 'PUBLIC',\n",
    "    **creds,\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load()` function specifies how each partition of the Dask dataframe should load its chunk of the data. As each partition of a Dask dataframe is a Pandas dataframe, we utilize the `fetch_pandas_all()` method.  We use a [binding for the Snowflake query](https://docs.snowflake.com/en/user-guide/python-connector-example.html#binding-data) so that we can pass different values at execution time. In this case, we pull one day of taxi rides into each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM (\n",
    "    SELECT \n",
    "        pickup_taxizone_id,\n",
    "        dropoff_taxizone_id,\n",
    "        passenger_count,\n",
    "        DIV0(tip_amount, fare_amount) AS tip_fraction,\n",
    "        DAYOFWEEKISO(pickup_datetime) - 1 AS pickup_weekday,\n",
    "        WEEKOFYEAR(pickup_datetime) AS pickup_weekofyear,\n",
    "        HOUR(pickup_datetime) AS pickup_hour,\n",
    "        (pickup_weekday * 24) + pickup_hour AS pickup_week_hour,\n",
    "        MINUTE(pickup_datetime) AS pickup_minute\n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        DATE(pickup_datetime) = %s\n",
    ") SAMPLE (1)\n",
    "\"\"\"\n",
    "\n",
    "@delayed\n",
    "def load(conn_info, query, day):\n",
    "    conn = snowflake.connector.connect(**conn_info)\n",
    "    taxi = conn.cursor().execute(query, str(day)).fetch_pandas_all()\n",
    "    taxi.columns = taxi.columns.str.lower()\n",
    "    return taxi    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query to determine which specific dates we need to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(start, end):\n",
    "    date_query = \"\"\"\n",
    "    SELECT\n",
    "        DISTINCT(DATE(pickup_datetime)) as date \n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        pickup_datetime BETWEEN %s and %s\n",
    "    \"\"\"\n",
    "    dates_df = conn.cursor().execute(date_query, (start, end)).fetch_pandas_all()\n",
    "    return dates_df['DATE'].tolist()\n",
    "\n",
    "dates = get_dates('2019-01-01', '2019-01-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dd.from_delayed` function takes delayed Pandas outputs from a list and converts that to a high-level DataFrame object.\n",
    "\n",
    "Since we're sampling in the Snowflake query, we run `persist()` to tell Dask to load all results into memory across the cluster. This ensures that Dask's lazy evaluation doesn't call the Snowflake query multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = dd.from_delayed([load(conn_info, query, day) for day in dates])\n",
    "taxi = taxi.persist()\n",
    "_ = wait(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num rows: {len(taxi)}, Size: {taxi.memory_usage(deep=True).sum().compute() / 1e6} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train = taxi[features + [y_col]].astype(float).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the preprocessing and `GridSearchCV` classes from dask-ml, but still use the scikit-learn `ElasticNet` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from dask_ml.compose import ColumnTransformer\n",
    "from dask_ml.preprocessing import StandardScaler, DummyEncoder, Categorizer\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('categorize', Categorizer(columns=categorical_feat)),\n",
    "    ('onehot', DummyEncoder(columns=categorical_feat)),\n",
    "    ('scale', ColumnTransformer(transformers=[('num', StandardScaler(), numeric_feat)])),\n",
    "    ('clf', ElasticNet(normalize=False, max_iter=100)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'clf__l1_ratio': np.arange(0, 1.1, 0.1),\n",
    "    'clf__alpha': [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=3, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the Dask dashboard after you run the cell below, you'll see the grid search in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(taxi_train[features], taxi_train[y_col])\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "`GridSearchCV` automatically fits the best paramemters to the full data and stores in `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "\n",
    "with open(f'{MODEL_PATH}/elastic_net_dask.pkl', 'wb') as f:\n",
    "    cloudpickle.dump(grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics on test set\n",
    "\n",
    "Use a different month for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = get_dates('2019-02-01', '2019-02-28')\n",
    "taxi_test = dd.from_delayed([load(conn_info, query, day) for day in test_dates])\n",
    "taxi_test = taxi_test.persist()\n",
    "_ = wait(taxi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = grid_search.predict(taxi_test[features])\n",
    "mean_squared_error(taxi_test[y_col], preds, squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
