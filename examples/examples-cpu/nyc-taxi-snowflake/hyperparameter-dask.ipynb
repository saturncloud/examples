{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "## Dask cluster\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../_img/dask-horizontal.svg\" width=\"300\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../_img/snowflake.png\" width=\"300\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numeric_feat = [\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekofyear\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_week_hour\",\n",
    "    \"pickup_minute\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "categorical_feat = [\n",
    "    \"pickup_taxizone_id\",\n",
    "    \"dropoff_taxizone_id\",\n",
    "]\n",
    "features = numeric_feat + categorical_feat\n",
    "y_col = \"tip_fraction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster\n",
    "\n",
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers,\n",
    "    scheduler_size=\"medium\",\n",
    "    worker_size=\"large\",\n",
    "    nthreads=2,\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dashboard (link above ^) and watch it when you execute some commands, you'll see which tasks are running across the cluster.\n",
    "\n",
    "If you created your cluster here in this notebook, it might take a few minutes for all your nodes to become available. You can run the chunk below to block until all nodes are ready.\n",
    "\n",
    ">**Pro tip**: Create and/or start your cluster from the \"Dask\" page in Saturn if you want to get a head start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.wait_for_workers(n_workers=n_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and feature engineering\n",
    "\n",
    "Load a sample from a single month for this exercise. Note we are loading the data with Dask now by defining a `dask.delayed` function to load partitions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import snowflake.connector\n",
    "\n",
    "SNOWFLAKE_ACCOUNT = os.environ[\"SNOWFLAKE_ACCOUNT\"]\n",
    "SNOWFLAKE_USER = os.environ[\"SNOWFLAKE_USER\"]\n",
    "SNOWFLAKE_PASSWORD = os.environ[\"SNOWFLAKE_PASSWORD\"]\n",
    "\n",
    "SNOWFLAKE_WAREHOUSE = os.environ[\"SNOWFLAKE_WAREHOUSE\"]\n",
    "TAXI_DATABASE = os.environ[\"TAXI_DATABASE\"]\n",
    "TAXI_SCHEMA = os.environ[\"TAXI_SCHEMA\"]\n",
    "\n",
    "conn_info = {\n",
    "    \"account\": SNOWFLAKE_ACCOUNT,\n",
    "    \"user\": SNOWFLAKE_USER,\n",
    "    \"password\": SNOWFLAKE_PASSWORD,\n",
    "    \"warehouse\": SNOWFLAKE_WAREHOUSE,\n",
    "    \"database\": TAXI_DATABASE,\n",
    "    \"schema\": TAXI_SCHEMA,\n",
    "}\n",
    "conn = snowflake.connector.connect(**conn_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load()` function specifies how each partition of the Dask dataframe should load its chunk of the data. As each partition of a Dask dataframe is a Pandas dataframe, we utilize the `fetch_pandas_all()` method.  We use a [binding for the Snowflake query](https://docs.snowflake.com/en/user-guide/python-connector-example.html#binding-data) so that we can pass different values at execution time. In this case, we pull one day of taxi rides into each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM (\n",
    "    SELECT\n",
    "        pickup_taxizone_id,\n",
    "        dropoff_taxizone_id,\n",
    "        passenger_count,\n",
    "        DIV0(tip_amount, fare_amount) AS tip_fraction,\n",
    "        DAYOFWEEKISO(pickup_datetime) - 1 AS pickup_weekday,\n",
    "        WEEKOFYEAR(pickup_datetime) AS pickup_weekofyear,\n",
    "        HOUR(pickup_datetime) AS pickup_hour,\n",
    "        (pickup_weekday * 24) + pickup_hour AS pickup_week_hour,\n",
    "        MINUTE(pickup_datetime) AS pickup_minute\n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        DATE(pickup_datetime) = %s\n",
    ") SAMPLE (1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def load(conn_info, query, day):\n",
    "    with snowflake.connector.connect(**conn_info) as conn:\n",
    "        taxi = conn.cursor().execute(query, str(day)).fetch_pandas_all()\n",
    "        taxi.columns = taxi.columns.str.lower()\n",
    "        return taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query to determine which specific dates we need to pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dates(start, end):\n",
    "    date_query = \"\"\"\n",
    "    SELECT\n",
    "        DISTINCT(DATE(pickup_datetime)) as date\n",
    "    FROM taxi_yellow\n",
    "    WHERE\n",
    "        pickup_datetime BETWEEN %s and %s\n",
    "    \"\"\"\n",
    "    dates_df = conn.cursor().execute(date_query, (start, end)).fetch_pandas_all()\n",
    "    return dates_df[\"DATE\"].tolist()\n",
    "\n",
    "\n",
    "dates = get_dates(\"2019-01-01\", \"2019-01-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dd.from_delayed` function takes delayed Pandas outputs from a list and converts that to a high-level DataFrame object.\n",
    "\n",
    "Since we're sampling in the Snowflake query, we run `persist()` to tell Dask to load all results into memory across the cluster. This ensures that Dask's lazy evaluation doesn't call the Snowflake query multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = dd.from_delayed([load(conn_info, query, day) for day in dates])\n",
    "taxi = taxi.persist()\n",
    "_ = wait(taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num rows: {len(taxi)}, Size: {taxi.memory_usage(deep=True).sum().compute() / 1e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train = taxi[features + [y_col]].astype(float).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the preprocessing and `GridSearchCV` classes from dask-ml, but still use the scikit-learn `ElasticNet` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from dask_ml.compose import ColumnTransformer\n",
    "from dask_ml.preprocessing import StandardScaler, DummyEncoder, Categorizer\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"categorize\", Categorizer(columns=categorical_feat)),\n",
    "        (\"onehot\", DummyEncoder(columns=categorical_feat)),\n",
    "        (\n",
    "            \"scale\",\n",
    "            ColumnTransformer(\n",
    "                transformers=[(\"num\", StandardScaler(), numeric_feat)],\n",
    "                remainder=\"passthrough\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", ElasticNet(normalize=False, max_iter=100)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"clf__l1_ratio\": np.arange(0, 1.1, 0.1),\n",
    "    \"clf__alpha\": [0, 0.5, 1, 2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, params, cv=3, scoring=\"neg_mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up the Dask dashboard after you run the cell below, you'll see the grid search in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = grid_search.fit(taxi_train[features], taxi_train[y_col])\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "\n",
    "`GridSearchCV` automatically fits the best parameters to the full data and stores in `best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import os\n",
    "\n",
    "MODEL_PATH = \"models\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "\n",
    "with open(f\"{MODEL_PATH}/elastic_net_dask.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(grid_search.best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics on test set\n",
    "\n",
    "Use a different month for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = get_dates(\"2019-02-01\", \"2019-02-28\")\n",
    "taxi_test = dd.from_delayed([load(conn_info, query, day) for day in test_dates])\n",
    "taxi_test = taxi_test.persist()\n",
    "_ = wait(taxi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = grid_search.predict(taxi_test[features])\n",
    "mean_squared_error(taxi_test[y_col], preds, squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
