{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce78932",
   "metadata": {},
   "source": [
    "[Tensorflow](https://rapids.ai/) is a popular, powerful framework for deep learning used by data scientists across industries.\n",
    "\n",
    "In this example, you'll train Resnet50 architecture to identify different species of birds. This dataset constitutes 40,000+ birds and has been taken from [kaggle](https://www.kaggle.com/gpiosenka/100-bird-species).\n",
    "\n",
    "\n",
    "First we import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323cd78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4e3b5e",
   "metadata": {},
   "source": [
    "We will be using [Weights & Biases](https://wandb.ai/site) to monitor GPU performance. Users will need to set their own Saturn Cloud env credential for wandb. Check [here](https://saturncloud.io/docs/examples/python/weights-and-biases/qs-wandb/) on more information on creating and connecting W&B account to Saturn Cloud. First, we log into Weights & Biases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448d4326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cd0fd",
   "metadata": {},
   "source": [
    "### Extracting Data\n",
    "The dataset originally had 285 classes. We have taken subset of this data which has 61 classes . The data is stored in AWS S3.The first time you run this job, you'll need to download the training and test data which will be saved at path `dataset/birds/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcedb346-9132-4deb-9e07-2ae904943992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "_ = s3.get(\n",
    "    rpath=\"s3://saturn-public-data/100-bird-species/100-bird-species/*/*/*.jpg\",\n",
    "    lpath=\"dataset/birds/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d38b8",
   "metadata": {},
   "source": [
    " Run the code below to ensure that TensorFlow just absorbs memory as needed, instead of absorbing all the RAM it has access to on your GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1f23d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75217f72",
   "metadata": {},
   "source": [
    "### Training\n",
    "In code below we are constructing Keras data object for training and validation set using `keras.preprocessing.image_dataset_from_directory` . We have chosen Adam optimizer and have set learning rate to 0.02.  We are training our classifier with  ResNet50 architecture, which has 48 Convolution layers along with 1 MaxPool and 1 Average Pool layer. The model is being compiled,trained and saved at path 'model/keras_single/'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c2f95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_fit(n_epochs, base_lr, batchsize, classes):\n",
    "\n",
    "    model = tf.keras.applications.ResNet50(include_top=True, weights=None, classes=classes)\n",
    "\n",
    "    # --------- Start wandb --------- #\n",
    "    wandb.init(config=wbargs, project=\"wandb_saturn_demo\")\n",
    "\n",
    "    # Data\n",
    "    train_ds = (\n",
    "        tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            \"dataset/birds/train\", image_size=(224, 224), batch_size=batchsize\n",
    "        )\n",
    "        .prefetch(2)\n",
    "        .cache()\n",
    "        .shuffle(1000)\n",
    "    )\n",
    "\n",
    "    valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"dataset/birds/valid\", image_size=(224, 224), batch_size=batchsize\n",
    "    ).prefetch(2)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr=base_lr)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    start = time.time()\n",
    "\n",
    "    model.fit(train_ds, epochs=n_epochs, validation_data=valid_ds, callbacks=[WandbCallback()])\n",
    "    end = time.time() - start\n",
    "    print(\"model training time\", end)\n",
    "    wandb.log({\"training_time\": end})\n",
    "\n",
    "    # Close your wandb run\n",
    "    wandb.run.finish()\n",
    "\n",
    "    tf.keras.models.save_model(model, \"model/keras_single/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6631bcc",
   "metadata": {},
   "source": [
    "In code below we are setting up necessary parameters . We are only running 2 epochs, to save time, but once you've got this working you'll have all the information you need to build and run bigger Tensorflow models on Saturn Cloud. A single GPU reviews all our batches every epoch. All the model parameters, as well as some extra elements like Notes and Tags are tracked by Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e596ad2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = {\"n_epochs\": 2, \"base_lr\": 0.02, \"classes\": 285, \"batchsize\": 64}\n",
    "\n",
    "wbargs = {\n",
    "    **model_params,\n",
    "    \"Notes\": \"tf_v100_2x\",\n",
    "    \"Tags\": [\"single\", \"gpu\", \"tensorflow\"],\n",
    "    \"dataset\": \"Birds\",\n",
    "    \"architecture\": \"ResNet50\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b40bf2",
   "metadata": {},
   "source": [
    "Now run the model training process, and save your trained model object to the Jupyter instance memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc494ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tester = train_model_fit(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c8e6a-d729-41d5-909f-8a945b00e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
