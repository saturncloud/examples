{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5468cbd5",
   "metadata": {},
   "source": [
    "# Training using Multiple GPUs\n",
    "[Tensorflow](https://rapids.ai/) is a popular, powerful framework for deep learning used by data scientists across industries.\n",
    "\n",
    "In this example, you'll train a Resnet50 model to identify different species of birds using multiple GPUs. This dataset constitutes 40,000+ birds and has been taken from [kaggle](https://www.kaggle.com/gpiosenka/100-bird-species).\n",
    "\n",
    "\n",
    "First we import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad674cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2fad5",
   "metadata": {},
   "source": [
    "We will be using [Weights & Biases](https://wandb.ai/site) to monitor GPU performance. Users will need to set their own Saturn Cloud environment credential for wandb. Check [here](https://saturncloud.io/docs/examples/python/weights-and-biases/qs-wandb/) on more information on creating and connecting W&B account to Saturn Cloud. First, we log into Weights & Biases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7070eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f301ed9",
   "metadata": {},
   "source": [
    "### Extracting Data\n",
    "The dataset originally had 285 classes. We have taken subset of this data which has 61 classes. The data is stored in AWS S3.The first time you run this job, you'll need to download the training and test data which will be saved in `dataset/birds/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738937d-656e-41dd-9ce8-7ac72cea9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "_ = s3.get(\n",
    "    rpath=\"s3://saturn-public-data/100-bird-species/100-bird-species/*/*/*.jpg\",\n",
    "    lpath=\"dataset/birds/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0fc33",
   "metadata": {},
   "source": [
    "Run the code below to ensure that TensorFlow uses only the memory required for the model, instead of using all the GPU RAM it has access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c537aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8de87",
   "metadata": {},
   "source": [
    "### Training \n",
    "In code below, we construct Keras data objects for the training and validation sets using `keras.preprocessing.image_dataset_from_directory`. We use the Adam optimizer and set the learning rate to 0.02. \n",
    "We will parallelize the learning by using the TensorFlow Mirrored Strategy. The model will split the data in each batch (sometimes called \"global batch\") across the GPUs, making \"worker batches.\" Each GPU has a copy of the model, called a \"replica,\" and while they learn on different parts of the data, they will combine the learned gradients at the end of the step. They stay synchronized this way, and the result at the end of training is one model that has learned on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c44f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_multigpu(\n",
    "    n_epochs, classes, base_lr, batchsize, wbargs, scale_batch=False, scale_lr=False\n",
    "):\n",
    "\n",
    "    wbargs = {**wbargs, \"scale_batch\": scale_batch, \"scale_lr\": scale_lr}\n",
    "\n",
    "    # --------- Start wandb --------- #\n",
    "    wandb.init(config=wbargs, project=\"wandb_saturn_demo\")\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(\"Number of devices: %d\" % strategy.num_replicas_in_sync)\n",
    "\n",
    "    if scale_batch:\n",
    "        batchsize = batchsize * strategy.num_replicas_in_sync\n",
    "\n",
    "    if scale_lr:\n",
    "        base_lr = base_lr * strategy.num_replicas_in_sync\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.applications.ResNet50(include_top=True, weights=None, classes=classes)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=base_lr)\n",
    "        model.compile(\n",
    "            loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    # Data\n",
    "    train_ds = (\n",
    "        tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            \"dataset/birds/train\", image_size=(224, 224), batch_size=batchsize\n",
    "        )\n",
    "        .prefetch(2)\n",
    "        .cache()\n",
    "        .shuffle(1000)\n",
    "    )\n",
    "\n",
    "    valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"dataset/birds/valid\", image_size=(224, 224), batch_size=batchsize\n",
    "    ).prefetch(2)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.fit(train_ds, epochs=n_epochs, validation_data=valid_ds, callbacks=[WandbCallback()])\n",
    "\n",
    "    end = time.time() - start\n",
    "    print(\"model training time\", end)\n",
    "    wandb.log({\"training_time\": end})\n",
    "\n",
    "    # Close your wandb run\n",
    "    wandb.run.finish()\n",
    "\n",
    "    tf.keras.models.save_model(model, \"model/keras_multi/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff37bb2",
   "metadata": {},
   "source": [
    "In the code below, we set up the necessary parameters. We are only running two epochs to save time. But once you've got this working, you'll have all the information you need to build and run bigger TensorFlow models on Saturn Cloud. A single GPU processes all our batches every epoch. All the model parameters, as well as some extra elements like Notes and Tags, are tracked by Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac8f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_epochs\": 2,\n",
    "    \"base_lr\": 0.02,\n",
    "    \"batchsize\": 64,\n",
    "    \"classes\": 285,\n",
    "    \"scale_batch\": True,\n",
    "}\n",
    "\n",
    "wbargs = {\n",
    "    **model_params,\n",
    "    \"Notes\": \"tf_v100_8x\",\n",
    "    \"Tags\": [\"multi\", \"gpu\", \"tensorflow\"],\n",
    "    \"dataset\": \"Birds\",\n",
    "    \"architecture\": \"ResNet50\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c81a1f",
   "metadata": {},
   "source": [
    "Now run the model training process and save your trained model object to the Jupyter instance memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b72b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tester_plain = train_multigpu(wbargs=wbargs, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1361d-5af2-40a4-bca2-affdc0055085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
