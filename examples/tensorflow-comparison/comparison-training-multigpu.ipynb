{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5468cbd5",
   "metadata": {},
   "source": [
    "[Tensorflow](https://rapids.ai/) is a popular, powerful framework for deep learning used by data scientists across industries.\n",
    "\n",
    "In this example, you'll train Resnet50 architecture to identify different species of birds using multiple GPUs. This dataset constitutes 40,000+ birds and has been taken from [kaggle](https://www.kaggle.com/gpiosenka/100-bird-species).\n",
    "\n",
    "\n",
    "First we import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad674cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f2fad5",
   "metadata": {},
   "source": [
    "We will be using [Weights & Biases](https://wandb.ai/site) to monitor GPU performance. Users will need to set their own Saturn Cloud env credential for wandb. Check [here](https://saturncloud.io/docs/examples/python/weights-and-biases/qs-wandb/) on more information on creating and connecting W&B account to Saturn Cloud. First, we log into Weights & Biases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7070eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f301ed9",
   "metadata": {},
   "source": [
    "### Extracting Data\n",
    "The dataset originally had 285 classes. We have taken subset of this data which has 61 classes . The data is stored in AWS S3.The first time you run this job, you'll need to download the training and test data which will be saved at path `dataset/birds/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738937d-656e-41dd-9ce8-7ac72cea9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "_ = s3.get(\n",
    "    rpath=\"s3://saturn-public-data/100-bird-species/100-bird-species/*/*/*.jpg\",\n",
    "    lpath=\"dataset/birds/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0fc33",
   "metadata": {},
   "source": [
    "Run the code below to ensure that TensorFlow just absorbs memory as needed, instead of absorbing all the RAM it has access to on your GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c537aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8de87",
   "metadata": {},
   "source": [
    "### Training \n",
    "In code below we are constructing Keras data object for training and validation set using `keras.preprocessing.image_dataset_from_directory` . We have chosen Adam optimizer and have set learning rate to 0.02.  We are training our classifier with  ResNet50 architecture. \n",
    "We will parallelize the learning by using TensorFlow Mirrored Strategy. The model will split the data in each batch (sometimes called “global batch”) across the GPUs (making “worker batches”). Each GPU has a copy of the model, called a “replica”, and while they learn on different parts of each batch, they will combine the learned gradients at the end of the step. They stay synchronized this way, and the result at the end of training is one model that has learned on all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c44f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_multigpu(\n",
    "    n_epochs, classes, base_lr, batchsize, wbargs, scale_batch=False, scale_lr=False\n",
    "):\n",
    "\n",
    "    wbargs = {**wbargs, \"scale_batch\": scale_batch, \"scale_lr\": scale_lr}\n",
    "\n",
    "    # --------- Start wandb --------- #\n",
    "    wandb.init(config=wbargs, project=\"wandb_saturn_demo\")\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(\"Number of devices: %d\" % strategy.num_replicas_in_sync)\n",
    "\n",
    "    if scale_batch:\n",
    "        batchsize = batchsize * strategy.num_replicas_in_sync\n",
    "\n",
    "    if scale_lr:\n",
    "        base_lr = base_lr * strategy.num_replicas_in_sync\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.applications.ResNet50(include_top=True, weights=None, classes=classes)\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr=base_lr)\n",
    "        model.compile(\n",
    "            loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    # Data\n",
    "    train_ds = (\n",
    "        tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            \"dataset/birds/train\", image_size=(224, 224), batch_size=batchsize\n",
    "        )\n",
    "        .prefetch(2)\n",
    "        .cache()\n",
    "        .shuffle(1000)\n",
    "    )\n",
    "\n",
    "    valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        \"dataset/birds/valid\", image_size=(224, 224), batch_size=batchsize\n",
    "    ).prefetch(2)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    model.fit(train_ds, epochs=n_epochs, validation_data=valid_ds, callbacks=[WandbCallback()])\n",
    "\n",
    "    end = time.time() - start\n",
    "    print(\"model training time\", end)\n",
    "    wandb.log({\"training_time\": end})\n",
    "\n",
    "    # Close your wandb run\n",
    "    wandb.run.finish()\n",
    "\n",
    "    tf.keras.models.save_model(model, \"model/keras_multi/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff37bb2",
   "metadata": {},
   "source": [
    "In code below we are setting up necessary parameters . We are only running 2 epochs, to save time, but once you've got this working you'll have all the information you need to build and run bigger Tensorflow models on Saturn Cloud. A single GPU reviews all our batches every epoch. All the model parameters, as well as some extra elements like Notes and Tags are tracked by Weights & Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac8f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"n_epochs\": 2,\n",
    "    \"base_lr\": 0.02,\n",
    "    \"batchsize\": 64,\n",
    "    \"classes\": 285,\n",
    "    \"scale_batch\": True,\n",
    "}\n",
    "\n",
    "wbargs = {\n",
    "    **model_params,\n",
    "    \"Notes\": \"tf_v100_8x\",\n",
    "    \"Tags\": [\"multi\", \"gpu\", \"tensorflow\"],\n",
    "    \"dataset\": \"Birds\",\n",
    "    \"architecture\": \"ResNet50\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c81a1f",
   "metadata": {},
   "source": [
    "Now run the model training process, and save your trained model object to the Jupyter instance memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b72b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tester_plain = train_multigpu(wbargs=wbargs, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1361d-5af2-40a4-bca2-affdc0055085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
