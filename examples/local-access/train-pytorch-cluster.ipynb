{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Image Classifier with PyTorch on GPU Dask Cluster\n",
    "\n",
    "Before you begin, make sure you have completed the steps in the [setup-external-connection.ipynb](setup-external-connection.ipynb) notebook.\n",
    "\n",
    "\n",
    "In this project, we use the Stanford Dogs dataset, and starting with a pre-trained version of Resnet50, we will use transfer learning to make it perform better at dog image identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import json \n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from dask_pytorch_ddp import data, dispatch\n",
    "import multiprocessing as mp\n",
    "from distributed.worker import logger # required for logging in Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Model Specifications\n",
    "\n",
    "Here you can assign your model hyperparameters, as well as identifying where the training data is housed on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {'n_epochs': 6, \n",
    "    'batch_size': 100,\n",
    "    'base_lr': .01,\n",
    "    'downsample_to':.5, # Value represents percent of training data you want to use\n",
    "    'bucket': \"saturn-public-data\",\n",
    "    'prefix': \"dogs/Images\",\n",
    "    'pretrained_classes':imagenetclasses} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Formatting \n",
    "These utilities ensure the training data labels correspond to the pretrained model's label expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import s3fs\n",
    "\n",
    "##### Load label dataset\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "with s3.open('s3://saturn-public-data/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "    imagenetclasses = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "##### Format labels to match pretrained Resnet\n",
    "def replace_label(dataset_label, model_labels):\n",
    "    label_string = re.search('n[0-9]+-([^/]+)', dataset_label).group(1)\n",
    "    \n",
    "    for i in model_labels:\n",
    "        i = str(i).replace('{', '').replace('}', '')\n",
    "        model_label_str = re.search('''b[\"'][0-9]+: [\"']([^\\/]+)[\"'],[\"']''', str(i))\n",
    "        model_label_idx = re.search('''b[\"']([0-9]+):''', str(i)).group(1)\n",
    "        \n",
    "        if re.search(str(label_string).replace('_', ' '), str(model_label_str).replace('_', ' ')):\n",
    "            return i, model_label_idx\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "This function encompasses the training task. \n",
    "* Load model and wrap it in PyTorch's Distributed Data Parallel function\n",
    "* Set up DataLoader to iterate over training data\n",
    "* Perform training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_train_cluster(bucket, prefix, batch_size, downsample_to, n_epochs, base_lr, pretrained_classes):\n",
    "    worker_rank = int(dist.get_rank())\n",
    "    \n",
    "    # --------- Format params --------- #\n",
    "    device = torch.device(\"cuda\")\n",
    "    net = models.resnet50(pretrained=True) # True means we start with the imagenet version\n",
    "    model = net.to(device)\n",
    "    model = DDP(model)\n",
    "\n",
    "    # --------- Set up eval --------- #\n",
    "    criterion = nn.CrossEntropyLoss().cuda()    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9)\n",
    "\n",
    "    # --------- Retrieve data for training --------- #\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize(256), \n",
    "    transforms.CenterCrop(250), \n",
    "    transforms.ToTensor()])\n",
    "    \n",
    "    # Because we want to load our images directly and lazily from S3,\n",
    "    # we use a custom Dataset class called S3ImageFolder.\n",
    "    whole_dataset = data.S3ImageFolder(\n",
    "        bucket, \n",
    "        prefix, \n",
    "        transform=transform, \n",
    "        anon = True\n",
    "    )\n",
    "\n",
    "    # Format target labels\n",
    "    new_class_to_idx = {x: int(replace_label(x, pretrained_classes)[1]) for x in whole_dataset.classes}\n",
    "    whole_dataset.class_to_idx = new_class_to_idx\n",
    "\n",
    "    # ------ Create dataloader ------- #\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        whole_dataset, \n",
    "        sampler=RandomSampler(\n",
    "            whole_dataset, \n",
    "            replacement = True,\n",
    "            num_samples = math.floor(len(whole_dataset)*downsample_to)), \n",
    "        batch_size=batch_size, \n",
    "        num_workers=40, \n",
    "        multiprocessing_context=mp.get_context('fork'))   \n",
    "\n",
    "    # --------- Start Training ------- #\n",
    "    for epoch in range(n_epochs):\n",
    "        count = 0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            dt = datetime.datetime.now().isoformat()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Run model iteration\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Format results\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            perct = [torch.nn.functional.softmax(el, dim=0)[i].item() for i, el in zip(preds, outputs)]\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "                \n",
    "            # Record the results of this model iteration for later review.\n",
    "            if worker_rank == 0:\n",
    "                logger.info(f\"loss: {loss.item()}, correct: {correct}, epoch: {epoch}, count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model\n",
    "\n",
    "To run the model, we use the `dask-pytorch-ddp` function `dispatch.run()`. This takes our client, our training function, and our dictionary of model parameters. You can monitor the model run on all workers using the Dask dashboard.\n",
    "\n",
    "Model performance statistics will print to the logs for Worker 0 inside the Saturn Cloud UI, but other model performance monitoring such as Weights and Biases or Tensorboard can be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart() # Clears memory on cluster- optional but recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time    \n",
    "futures = dispatch.run(client, simple_train_cluster, **model_params)\n",
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one or more worker jobs errors, this will describe the issue\n",
    "# futures[0].result()"
   ]
  },
  {
   "source": [
    "## Housekeeping\n",
    "\n",
    "Because we are not working inside the UI, we want to make sure that we close down any resources when we are done- otherwise undesired costs can be incurred.\n",
    "\n",
    "To shut down the cluster entirely:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}