{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data From S3 Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![S3 Logo](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/s3-logo.png  \"doc-image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "If you use AWS S3 to store your data, connecting to Saturn Cloud takes just a couple of steps. \n",
    "\n",
    "In this example we use <a href=\"https://github.com/dask/s3fs/\" target='_blank' rel='noopener'>s3fs</a> to connect to data, but you can also use libraries like <a href=\"https://aws.amazon.com/sdk-for-python/\" target='_blank' rel='noopener'>boto3</a> if you prefer.\n",
    "\n",
    "Before starting this, you should create a Jupyter server resource. See our [quickstart](https://saturncloud.io/docs/start_in_ten/) if you don't know how to do this yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To connect to public S3 buckets, you can simply connect using anonymous connections in Jupyter, the way you might with your local laptop. In this case, you can skip to the [Connect to Data Via `s3fs` section](#connect-via-s3fs).\n",
    "\n",
    "If your S3 data storage is not public and requires AWS credentials, please read on! \n",
    "\n",
    "### Create AWS Credentials\n",
    "Credentials for S3 access can be acquired inside your AWS account. Visit <a href=\"https://aws.amazon.com/\" target='_blank' rel='noopener'>https://aws.amazon.com/</a> and sign in to your account.\n",
    "\n",
    "In the top right corner, click the dropdown under your username and select **My Security Credentials**.\n",
    "\n",
    "![Screenshot of AWS site My Security Credentials page](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/aws-secrurity-credentials-arrow.png \"doc-image\")\n",
    "\n",
    "Under \"My Security Credentials\" you'll see section titled \"Access keys for CLI, SDK, & API access\". If you don't yet have an access key listed, create one.\n",
    "\n",
    "![Screenshot of Access Keys section of AWS site My Security Credentials page](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/aws-access-key.png \"doc-image\")\n",
    "\n",
    "Save the key information that this generates, and keep it in a safe place!\n",
    "\n",
    "### Add AWS Credentials to Saturn Cloud\n",
    "Sign in to your Saturn Cloud account and select **Credentials** from the menu on the left.\n",
    "\n",
    "<img src=\"https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/saturn-credentials-arrow.jpeg\" style=\"width:200px;\" alt=\"Saturn Cloud left menu with arrow pointing to Credentials tab\" class=\"doc-image\">\n",
    "\n",
    "This is where you will add your S3 API key information. *This is a secure storage location, and it will not be available to the public or other users without your consent.*\n",
    "\n",
    "At the top right corner of this page, you will find the **New** button. Click here, and you will be taken to the Credentials Creation form. \n",
    "\n",
    "![Screenshot of Saturn Cloud Create Credentials form](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/credentials.jpg \"doc-image\")\n",
    "\n",
    "You will be adding three credentials items: your AWS Access Key, AWS Secret Access Key, and you default region. Complete the form one time for each item. \n",
    "\n",
    "|  Credential | Type  | Name| Variable Name  |\n",
    "|---|---|---|---|\n",
    "| AWS Access Key ID   |  Environment Variable  | `aws-access-key-id` | `AWS_ACCESS_KEY_ID`\n",
    "| AWS Secret Access Key | Environment Variable  | `aws-secret-access-key`  | `AWS_SECRET_ACCESS_KEY`\n",
    "| AWS Default Region  | Environment Variable  | `aws-default-region`  | `AWS_DEFAULT_REGION`\n",
    "\n",
    "Copy the values from your AWS console into the *Value* section of the credential creation form. The credential names are recommendations; feel free to change them as needed for your workflow. You must, however, use the provided *Variable Names* for S3 to connect correctly.\n",
    "\n",
    "With this complete, your S3 credentials will be accessible by Saturn Cloud resources! You will need to restart any Jupyter Server or Dask Clusters for the credentials to populate to those resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='connect-via-s3fs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Data Via `s3fs`\n",
    "#### Set Up the Connection\n",
    "Normally, `s3fs` will automatically seek your AWS credentials from the environment. Since you have followed our instructions above for adding and saving credentials, this will work for you! \n",
    "\n",
    "If you don't have credentials and are accessing a public repository, set `anon=True` in the `s3fs.S3FileSystem()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can reference the `s3` handle and look at the contents of your S3 bucket as if it were a local file system. For examples, you can visit the <a href=\"https://s3fs.readthedocs.io/en/latest/#examples\" target='_blank' rel='noopener'>s3fs documentation</a> where they show multiple ways to interact with files like this.\n",
    "\n",
    "#### Load a CSV Using pandas\n",
    "This approach just uses routine pandas syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = \"nyc-tlc/trip data/yellow_tripdata_2019-01.csv\"\n",
    "with s3.open(file, mode=\"rb\") as f:\n",
    "    df = pd.read_csv(f, parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small files, this approach will work fine. For large or multiple files, we recommend using Dask, as described next.\n",
    "\n",
    "#### Load a CSV Using Dask\n",
    "This syntax is the same as pandas, but produces a distributed data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "file = \"nyc-tlc/trip data/yellow_tripdata_2019-01.csv\"\n",
    "with s3.open(file, mode=\"rb\") as f:\n",
    "    df = dd.read_csv(f, parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Folder of CSVs Using Dask\n",
    "Dask can read and load a whole folder of files if they are formatted the same, using glob syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = s3.glob(\"s3://nyc-tlc/trip data/yellow_tripdata_2019-*.csv\")\n",
    "taxi = dd.read_csv(\n",
    "    files,\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"],\n",
    "    storage_options={\"anon\": False},\n",
    "    assume_missing=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
