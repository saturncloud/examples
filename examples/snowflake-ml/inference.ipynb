{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/saturn.png\" width=\"300\">\n",
    "\n",
    "# Inference with Snowflake and Saturn Cloud\n",
    "\n",
    "This notebook contains steps for loading image files from a Snowflake unstructured table, and running image classification inference. \n",
    "Follow along in [our guide on the Snowflake website](https://quickstarts.snowflake.com/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import requests, io, os, datetime, re  # noqa: E401\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import dask\n",
    "from PIL import Image\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up Snowflake Connection\n",
    "\n",
    "Credentials are stored in the Saturn Cloud credentials tool."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "conn_kwargs = dict(\n",
    "    user=os.environ[\"SNOWFLAKE_USER\"],\n",
    "    password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "    account=\"mf80263.us-east-2.aws\",\n",
    "    warehouse=\"COMPUTE_WH\",\n",
    "    database=\"clothing\",\n",
    "    schema=\"PUBLIC\",\n",
    "    role=\"sysadmin\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up Dask Cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(2)\n",
    "client"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Functions\n",
    "\n",
    "### Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@dask.delayed\n",
    "def preprocess(list_img_attr):\n",
    "    \"\"\"Ingest images directly from S3, apply transformations,\n",
    "    and extract the ground truth and image identifier. Accepts\n",
    "    a filepath.\"\"\"\n",
    "\n",
    "    path, snow_path, filesize, orig_timestamp = (\n",
    "        list_img_attr[4],\n",
    "        list_img_attr[0],\n",
    "        list_img_attr[2],\n",
    "        list_img_attr[3],\n",
    "    )\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(250),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    file1 = requests.get(path).content\n",
    "    img2 = Image.open(io.BytesIO(file1)).convert(\"RGB\")\n",
    "    nvis = transform(img2)\n",
    "\n",
    "    truth = re.search(\n",
    "        \"clothing-dataset-small/test/([a-z-]+)\\/([^\\/]+(\\.jpg))\", path  # noqa: W605\n",
    "    ).group(  # noqa: W605\n",
    "        1\n",
    "    )  # noqa: W605\n",
    "    name = re.search(\n",
    "        \"clothing-dataset-small/test/([a-z-]+)\\/([^\\/]+(\\.jpg))\", path  # noqa: W605\n",
    "    ).group(  # noqa: W605\n",
    "        2\n",
    "    )  # noqa: W605\n",
    "\n",
    "    return [name, nvis, truth, path, snow_path, filesize, orig_timestamp]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@dask.delayed\n",
    "def reformat(batch):\n",
    "    batch_transposed = list(map(list, zip(*batch)))\n",
    "    batch_transposed[1] = torch.stack(batch_transposed[1]).to(device)\n",
    "    return batch_transposed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Human Readable Predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate_pred_batch(batch, gtruth, classes):\n",
    "    \"\"\"Accepts batch of images, returns human readable predictions.\"\"\"\n",
    "\n",
    "    _, indices = torch.sort(batch, descending=True)\n",
    "    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n",
    "    percentage, indices = percentage.cpu(), indices.cpu().numpy()\n",
    "\n",
    "    preds = []\n",
    "    labslist = []\n",
    "    for i in range(len(batch)):\n",
    "        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n",
    "        preds.append(pred)\n",
    "\n",
    "        labs = gtruth[i]\n",
    "        labslist.append(labs)\n",
    "\n",
    "    return (preds, labslist)\n",
    "\n",
    "\n",
    "def is_match(label, pred):\n",
    "    \"\"\"Evaluates human readable prediction against ground truth.\"\"\"\n",
    "    if re.search(label.replace(\"_\", \" \"), str(pred).replace(\"_\", \" \")):\n",
    "        match = True\n",
    "    else:\n",
    "        match = False\n",
    "    return match"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "@dask.delayed\n",
    "def run_batch_to_s3(iteritem):\n",
    "    \"\"\"Accepts iterable result of preprocessing, generates\n",
    "    inferences and evaluates.\"\"\"\n",
    "\n",
    "    names, images, truelabels, paths, snow_paths, filesizes, orig_timestamps = iteritem\n",
    "\n",
    "    indices = list(range(0, 10))\n",
    "    classes = [\n",
    "        \"dress\",\n",
    "        \"hat\",\n",
    "        \"longsleeve\",\n",
    "        \"outwear\",\n",
    "        \"pants\",\n",
    "        \"shirt\",\n",
    "        \"shoes\",\n",
    "        \"shorts\",\n",
    "        \"skirt\",\n",
    "        \"t-shirt\",\n",
    "    ]\n",
    "    classes2 = dict(zip(indices, classes))\n",
    "\n",
    "    # Retrieve, set up model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    resnet = models.resnet50(pretrained=False)\n",
    "    resnet.load_state_dict(torch.load(\"./model/modeltrained.pt\"))\n",
    "    resnet = resnet.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        resnet.eval()\n",
    "        pred_batch = resnet(images)\n",
    "\n",
    "        # Evaluate batch\n",
    "        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes2)\n",
    "\n",
    "        # Organize prediction results\n",
    "        outcomes = []\n",
    "        for j in range(0, len(images)):\n",
    "            match = is_match(labslist[j], preds[j])\n",
    "            outcome = {\n",
    "                \"name\": names[j],\n",
    "                \"ground_truth\": labslist[j],\n",
    "                \"prediction\": preds[j],\n",
    "                \"prediction_text\": preds[j][0][0],\n",
    "                \"prediction_prob\": preds[j][0][1],\n",
    "                \"evaluation\": match,\n",
    "                \"snow_path\": snow_paths[j],\n",
    "                \"filesize\": filesizes[j],\n",
    "                \"orig_timestamp\": orig_timestamps[j],\n",
    "            }\n",
    "            outcomes.append(outcome)\n",
    "\n",
    "        return outcomes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect To Snowflake\n",
    "\n",
    "Query for the image data from the `clothing_data` table.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stage = \"clothing_dataset\"\n",
    "relative_path_col = \"RELATIVE_PATH\"\n",
    "\n",
    "with snowflake.connector.connect(**conn_kwargs) as conn:\n",
    "    df = pd.read_sql(\n",
    "        f\"\"\"select FILE_URL,\n",
    "    RELATIVE_PATH, SIZE, LAST_MODIFIED,\n",
    "    get_presigned_url(@{stage}, {relative_path_col})\n",
    "    as SIGNEDURL from clothing_test\"\"\",\n",
    "        conn,\n",
    "    )\n",
    "    list_paths = df[\"SIGNEDURL\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Delayed Preprocessing Steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = 80  # batch size\n",
    "list_df = [df[i : i + n] for i in range(0, df.shape[0], n)]\n",
    "image_rows = [[x for j, x in y.iterrows()] for y in list_df]\n",
    "image_batches1 = [[preprocess(list(x)) for x in y] for y in image_rows]\n",
    "image_batches = [reformat(result) for result in image_batches1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Model file placed on workers\n",
    "\n",
    "from dask_saturn.plugins import RegisterFiles, sync_files\n",
    "\n",
    "client.register_worker_plugin(RegisterFiles())\n",
    "sync_files(client, \"/home/jovyan/project/examples/model\")\n",
    "client.run(os.listdir, \"./model\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Inference on Cluster"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "\n",
    "futures = client.map(run_batch_to_s3, image_batches)\n",
    "futures_gathered = client.gather(futures)\n",
    "futures_computed = client.compute(futures_gathered, sync=False)\n",
    "\n",
    "import logging\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "for fut in futures_computed:\n",
    "    try:\n",
    "        result = fut.result()\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        logging.error(e)\n",
    "    else:\n",
    "        results.extend(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Errors around inability to recognize or read the image may be a result of expired pre-signed links."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Review Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.DataFrame(results)\n",
    "df2.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check Quality of Inference\n",
    "\n",
    "This calculation just tells you what percent of your model's predictions were correct."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "true_preds = [x[\"evaluation\"] for x in results if x[\"evaluation\"] == True]  # noqa: E712\n",
    "false_preds = [x[\"evaluation\"] for x in results if x[\"evaluation\"] == False]  # noqa: E712\n",
    "len(true_preds) / len(results) * 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize\n",
    "\n",
    "This section will show some samples of predictions and contrast with the ground truth."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample = dask.compute(*image_batches)\n",
    "s5 = list(map(list, zip(*sample)))\n",
    "\n",
    "test_names = [i for sublist in s5[0] for i in sublist]\n",
    "test_tensors = [i for sublist in s5[1] for i in sublist]\n",
    "test_orig = [i for sublist in s5[2] for i in sublist]\n",
    "test_final = list(zip(test_names, test_tensors, test_orig))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "expanded_list = [\n",
    "    (i, j)\n",
    "    for i in results\n",
    "    for j in test_final\n",
    "    if i[\"name\"] in test_names and j[0] in test_names and i[\"name\"] == j[0]\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# noqa: W291\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cpudevice = torch.device(\"cpu\")\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "imglist = expanded_list[325:330]\n",
    "f, ax = plt.subplots(nrows=1, ncols=5, figsize=(16, 6))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    img1 = to_pil(imglist[i][1][1].to(cpudevice))\n",
    "    ax[i].imshow(img1).axes.xaxis.set_visible(False)\n",
    "    ax[i].axes.yaxis.set_visible(False)\n",
    "    textcol = \"green\" if imglist[i][0][\"evaluation\"] == True else \"red\"  # noqa: E712\n",
    "    ax[i].set_title(\n",
    "        f\"\"\"Predicted Class: {imglist[i][0][\"prediction_text\"]} \n",
    "    Actual Class: {imglist[i][0][\"ground_truth\"]} \"\"\",  # noqa: W291\n",
    "        color=textcol,\n",
    "    )\n",
    "\n",
    "title = \"Sample Images\"\n",
    "f.suptitle(title, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Results to Snowflake\n",
    "\n",
    "Populate a temp table, update the permanent table, then remove the temp table."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "make_table = \"\"\"\n",
    "    CREATE OR REPLACE TABLE clothing_temp\n",
    "    (\n",
    "      FILE_URL VARCHAR,\n",
    "      SIZE NUMBER,\n",
    "      LAST_MODIFIED TIMESTAMP_LTZ,\n",
    "      TYPE VARCHAR,\n",
    "      CONFIDENCE FLOAT8,\n",
    "      PRED_TIMESTAMP TIMESTAMP_LTZ\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "check_library = \"show tables in CLOTHING.PUBLIC\"\n",
    "\n",
    "update_query = \"\"\"\n",
    "    update clothing_test\n",
    "      set clothing_test.TYPE = clothing_temp.TYPE,\n",
    "          clothing_test.CONFIDENCE = clothing_temp.CONFIDENCE,\n",
    "          clothing_test.PRED_TIMESTAMP = clothing_temp.PRED_TIMESTAMP\n",
    "      from clothing_temp\n",
    "      where clothing_test.FILE_URL = clothing_temp.FILE_URL\n",
    "      and  clothing_test.SIZE = clothing_temp.SIZE\n",
    "\"\"\"\n",
    "\n",
    "clean_house = \"drop table if exists clothing_temp\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with snowflake.connector.connect(**conn_kwargs) as conn:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(make_table)\n",
    "    print(\"Temp table created.\")\n",
    "    snow_df = df2[\n",
    "        [\"snow_path\", \"filesize\", \"orig_timestamp\", \"prediction_text\", \"prediction_prob\"]\n",
    "    ].copy()\n",
    "    snow_df.rename(\n",
    "        columns={\n",
    "            \"snow_path\": \"FILE_URL\",\n",
    "            \"filesize\": \"SIZE\",\n",
    "            \"orig_timestamp\": \"LAST_MODIFIED\",\n",
    "            \"prediction_text\": \"TYPE\",\n",
    "            \"prediction_prob\": \"CONFIDENCE\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    snow_df[\"PRED_TIMESTAMP\"] = pd.to_datetime(datetime.datetime.now()).tz_localize(\"UTC\")\n",
    "    success, nchunks, nrows, _ = write_pandas(conn, snow_df, \"CLOTHING_TEMP\")\n",
    "    print(f\"Temp results table created: {success}. Rows inserted in table: {nrows}.\")\n",
    "    res = cur.execute(update_query)\n",
    "    print(f\"Updated {res.rowcount} rows in permanent table from temp source.\")\n",
    "    cur.execute(clean_house)\n",
    "    print(\"Temp table removed.\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}